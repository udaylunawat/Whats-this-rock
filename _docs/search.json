[
  {
    "objectID": "b_data_utils.html",
    "href": "b_data_utils.html",
    "title": "Data utils",
    "section": "",
    "text": "# #| export\n\n# def timer_func(func):\n#     \"\"\"Show the execution time of the function object passed.\n\n#     Parameters\n#     ----------\n#     func : _type_\n#         _description_\n#     \"\"\"\n\n#     def wrap_func(*args, **kwargs):\n#         t1 = time()\n#         result = func(*args, **kwargs)\n#         t2 = time()\n#         print(f\"Function {func.__name__!r} executed in {(t2-t1):.4f}s\")\n#         return result\n\n#     return wrap_func\n\n# def find_filepaths(root_folder: str):\n#     \"\"\"Recursively finds all files.\n\n#     Parameters\n#     ----------\n#     root_folder : str\n#         _description_\n\n#     Returns\n#     -------\n#     _type_\n#         _description_\n#     \"\"\"\n#     filepaths = []\n#     for dirname, _, filenames in os.walk(root_folder):\n#         for filename in filenames:\n#             filepaths.append(os.path.join(dirname, filename))\n#     total_images_before_deletion = len(filepaths)\n#     print(f\"Total images before deletion = {total_images_before_deletion}\")\n#     return filepaths\n\n\n# def remove_unsupported_images(root_folder: str):\n#     \"\"\"Remove unsupported images.\n\n#     Parameters\n#     ----------\n#     root_folder : str\n#         Root Folder.\n#     \"\"\"\n#     print(\"\\n\\nRemoving unsupported images...\")\n#     count = 1\n#     filepaths = find_filepaths(root_folder)\n#     for filepath in filepaths:\n#         if filepath.endswith((\"JFIF\", \"webp\", \"jfif\")):\n#             shutil.move(\n#                 filepath,\n#                 os.path.join(\"data\", \"corrupted_images\", os.path.basename(filepath)),\n#             )\n#             count += 1\n#     print(f\"Removed {count} unsupported files.\")\n\n# @timer_func\n# def remove_corrupted_images(\n#     s_dir: str, ext_list: list = [\"jpg\", \"png\", \"jpeg\", \"gif\", \"bmp\", \"JPEG\"]\n# ):\n#     \"\"\"Remove corrupted images.\n\n#     Parameters\n#     ----------\n#     s_dir : str\n#         Source directory.\n#     ext_list : list, optional\n#         Extensions list, by default [\"jpg\", \"png\", \"jpeg\", \"gif\", \"bmp\", \"JPEG\"]\n#     \"\"\"\n#     print(\"\\n\\nRemoving corrupted images...\")\n#     classes = os.listdir(s_dir)\n\n#     def remove_corrupted_from_dir(rock_class):\n#         # remove corrupted images from single directory\n#         bad_images = []\n#         class_path = os.path.join(s_dir, rock_class)\n#         print(f\"Processing class directory {rock_class}...\")\n#         if os.path.isdir(class_path):\n#             file_list = os.listdir(class_path)\n#             for f in file_list:\n#                 f_path = os.path.join(class_path, f)\n#                 tip = imghdr.what(f_path)\n#                 if ext_list.count(tip) == 0:\n#                     bad_images.append(f_path)\n#                 if os.path.isfile(f_path):\n#                     try:\n#                         cv2.imread(f_path)\n#                         # shape = img.shape\n#                     except Exception:\n#                         print(\"file \", f_path, \" is not a valid image file\")\n#                         bad_images.append(f_path)\n#                 else:\n#                     print(\n#                         \"*** fatal error, you a sub directory \",\n#                         f,\n#                         \" in class directory \",\n#                         rock_class,\n#                     )\n#         else:\n#             print(\n#                 \"*** WARNING*** you have files in \",\n#                 s_dir,\n#                 \" it should only contain sub directories\",\n#             )\n\n#         for f_path in bad_images:\n#             shutil.move(\n#                 f_path,\n#                 os.path.join(\"data\", \"corrupted_images\", os.path.basename(f_path)),\n#             )\n#         print(f\"removed {len(bad_images)} bad images from {rock_class}.\")\n\n#     # Multiprocessing\n#     # create all tasks\n#     from multiprocessing import Process\n\n#     processes = [Process(target=remove_corrupted_from_dir, args=(i,)) for i in classes]\n#     # start all processes\n#     for process in processes:\n#         process.start()\n#     # wait for all processes to complete\n#     for process in processes:\n#         process.join()\n#     # report that all tasks are completed\n#     print(\"Removed all corrupted images.\", flush=True)\n\n\n# def get_dims(file: str) -> Optional[tuple]:\n#     \"\"\"Return dimenstions for an RBG image.\n\n#     Parameters\n#     ----------\n#     file : str\n#         file path for image\n\n#     Returns\n#     -------\n#     Optional[tuple, None]\n#         returns a tuple of heights and width of image or None\n#     \"\"\"\n#     im = cv2.imread(file)\n#     if im is not None:\n#         arr = np.array(im)\n#         h, w = arr.shape[0], arr.shape[1]\n#         return h, w\n#     elif im is None:\n#         return None\n\n\n# def get_df(root: str = \"data/2_processed\") -> pd.DataFrame:\n#     \"\"\"Return df with classes, image paths and file names.\n\n#     Parameters\n#     ----------\n#     root : str, optional\n#         directory to scan for image files, by default \"data/2_processed\"\n\n#     Returns\n#     -------\n#     pd.DataFrame\n#         with columns file_name, class and file_path\n#     \"\"\"\n#     classes = os.listdir(root)\n\n#     class_names = []\n#     images_paths = []\n#     file_names = []\n\n#     for class_name in classes:\n#         for dirname, _, filenames in os.walk(os.path.join(root, class_name)):\n#             for file_name in filenames:\n#                 images_paths.append(os.path.join(root, class_name, file_name))\n#                 class_names.append(class_name)\n#                 file_names.append(file_name)\n\n#     df = pd.DataFrame(\n#         list(zip(file_names, class_names, images_paths)),\n#         columns=[\"file_name\", \"class\", \"file_path\"],\n#     )\n\n#     return df\n\n\n# def get_value_counts(dataset_path: str) -> None:\n#     \"\"\"Get class counts of all classes in the dataset.\n\n#     Parameters\n#     ----------\n#     dataset_path : str\n#         directory with subclasses\n#     \"\"\"\n#     data = get_df(dataset_path)\n#     vc = data[\"file_name\"].apply(lambda x: x.split(\".\")[-1]).value_counts()\n#     print(vc)\n\n\n# ####################################### tf.data Utilities ###################################\n\n\n# def scalar(img: Image) -> Image:\n#     \"\"\"Scale pixel between -1 and +1.\n\n#     Parameters\n#     ----------\n#     img : Image\n#         PIL Image\n\n#     Returns\n#     -------\n#     Image\n#         imagew with pixel values scaled between -1 and 1\n#     \"\"\"\n#     return img / 127.5 - 1\n\n\n# def get_preprocess(cfg):\n#     \"\"\"Return preprocess function for particular model.\n\n#     Parameters\n#     ----------\n#     cfg : cfg (omegaconf.DictConfig)\n#         Hydra Configuration\n\n#     Returns\n#     -------\n#     _type_\n#         _description_\n#     \"\"\"\n#     preprocess_dict = {\n#         # \"convnexttiny\":applications.convnext,\n#         \"vgg16\": applications.vgg16,\n#         \"resnet\": applications.resnet,\n#         \"inceptionresnetv2\": applications.inception_resnet_v2,\n#         \"mobilenetv2\": applications.mobilenet_v2,\n#         \"efficientnetv2\": applications.efficientnet_v2,\n#         \"efficientnetv2m\": applications.efficientnet_v2,\n#         \"xception\": applications.xception,\n#     }\n\n#     return preprocess_dict[cfg.backbone].preprocess_input\n\n\n# def prepare(ds, cfg, shuffle=False, augment=False):\n#     \"\"\"Prepare dataset using augment, preprocess, cache, shuffle and prefetch.\n\n#     Parameters\n#     ----------\n#     ds : _type_\n#         _description_\n#     cfg : cfg (omegaconf.DictConfig):\n#         Hydra Configuration\n#     shuffle : bool, optional\n#         _description_, by default False\n#     augment : bool, optional\n#         _description_, by default False\n\n#     Returns\n#     -------\n#     _type_\n#         _description_\n#     \"\"\"\n#     # keras_cv\n#     def to_dict(image, label):\n#         image = tf.image.resize(image, (cfg.image_size, cfg.image_size))\n#         image = tf.cast(image, tf.float32)\n#         # label = tf.one_hot(label, cfg.num_classes)\n#         return {\"images\": image, \"labels\": label}\n\n#     def preprocess_for_model(inputs):\n#         images, labels = inputs[\"images\"], inputs[\"labels\"]\n#         images = tf.cast(images, tf.float32)\n#         return images, labels\n\n#     def cut_mix_and_mix_up(samples):\n#         samples = cut_mix(samples, training=True)\n#         samples = mix_up(samples, training=True)\n#         return samples\n\n#     def apply_rand_augment(inputs):\n#         inputs[\"images\"] = rand_augment(inputs[\"images\"])\n#         return inputs\n\n#     AUTOTUNE = tf.data.AUTOTUNE\n\n#     if cfg.preprocess:\n#         preprocess_input = get_preprocess(cfg)\n#         ds = ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n\n#     if augment:\n#         # normal augmentation\n#         data_augmentation = tf.keras.Sequential(\n#             [\n#                 layers.RandomFlip(\n#                     \"horizontal\",\n#                     input_shape=(cfg.image_size, cfg.image_size, cfg.image_channels),\n#                 ),\n#                 layers.RandomRotation(0.1),\n#                 layers.RandomZoom(0.1),\n#             ]\n#         )\n#         # Use data augmentation only on the training set.\n#         ds = ds.map(\n#             lambda x, y: (data_augmentation(x, training=True), y),\n#             num_parallel_calls=AUTOTUNE,\n#         )\n#     elif augment == \"kerascv\":\n#         # using keras_cv\n#         ds = ds.map(to_dict, num_parallel_calls=AUTOTUNE)\n#         rand_augment = keras_cv.layers.RandAugment(\n#             value_range=(0, 255),\n#             augmentations_per_image=3,\n#             magnitude=0.3,\n#             magnitude_stddev=0.2,\n#             rate=0.5,\n#         )\n#         cut_mix = keras_cv.layers.CutMix()\n#         mix_up = keras_cv.layers.MixUp()\n\n#         ds = ds.map(apply_rand_augment, num_parallel_calls=AUTOTUNE).map(\n#             cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE\n#         )\n\n#         ds = ds.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n\n#     ds = ds.cache()\n#     if shuffle:\n#         ds = ds.shuffle(buffer_size=1000)\n\n#     # # Batch all datasets.\n#     # ds = ds.batch(cfg.batch_size)\n\n#     # Use buffered prefetching on all datasets.\n#     return ds.prefetch(buffer_size=AUTOTUNE)\n\n\n# def get_tfds_from_dir(cfg):\n#     \"\"\"Convert directory of images to tfds dataset.\n\n#     Parameters\n#     ----------\n#     cfg : cfg (omegaconf.DictConfig):\n#         Hydra Configuration\n\n#     Returns\n#     -------\n#     _type_\n#         _description_\n#     \"\"\"\n#     IMAGE_SIZE = (cfg.image_size, cfg.image_size)\n#     train_ds = tf.keras.utils.image_dataset_from_directory(\n#         \"data/4_tfds_dataset/train\",\n#         labels=\"inferred\",\n#         label_mode=\"categorical\",\n#         color_mode=\"rgb\",\n#         batch_size=cfg.batch_size,\n#         image_size=IMAGE_SIZE,\n#         shuffle=True,\n#         seed=cfg.seed,\n#         # subset='training'\n#     )\n\n#     val_ds = tf.keras.utils.image_dataset_from_directory(\n#         \"data/4_tfds_dataset/val\",\n#         labels=\"inferred\",\n#         label_mode=\"categorical\",\n#         color_mode=\"rgb\",\n#         batch_size=cfg.batch_size,\n#         image_size=IMAGE_SIZE,\n#         shuffle=True,\n#         seed=cfg.seed,\n#         # subset='validation'\n#     )\n\n#     test_ds = tf.keras.utils.image_dataset_from_directory(\n#         \"data/4_tfds_dataset/test\",\n#         labels=\"inferred\",\n#         label_mode=\"categorical\",\n#         color_mode=\"rgb\",\n#         batch_size=cfg.batch_size,\n#         image_size=IMAGE_SIZE,\n#         shuffle=False,\n#         seed=cfg.seed,\n#         # subset='validation'\n#     )\n\n#     return train_ds, val_ds, test_ds\n\n\n# def rename_files(source_dir: str = \"data/2_processed/tmp\"):\n#     \"\"\"Rename files in classes and moves to 2_processed.\n\n#     Parameters\n#     ----------\n#     source_dir : str, optional\n#         _description_, by default \"data/2_processed/tmp\"\n#     \"\"\"\n#     class_names = os.listdir(source_dir)\n#     for class_name in class_names:\n#         class_path = os.path.join(source_dir, class_name)\n#         dest_path = os.path.join(\"data/2_processed\", class_name)\n#         count = len(os.listdir(dest_path)) + 1\n#         for filename in os.listdir(class_path):\n#             old_path = os.path.join(source_dir, class_name, filename)\n#             _, extension = os.path.splitext(filename)\n#             new_name = f\"{class_name}_{count}{extension}\"\n#             new_path = os.path.join(dest_path, new_name)\n#             shutil.copy(old_path, new_path)\n#             count += 1\n\n\n# def move_files(src_dir: str, dest_dir: str = \"data/2_processed/tmp\"):\n#     \"\"\"Move files to tmp directory in 2_processed.\n\n#     src_dir: directory of rock subclass with files [Basalt, Marble, Coal, ...]\n\n#     Parameters\n#     ----------\n#     src_dir : str\n#         _description_\n#     dest_dir : str, optional\n#         _description_, by default \"data/2_processed/tmp\"\n#     \"\"\"\n#     if os.path.exists(dest_dir):\n#         shutil.rmtree(dest_dir)\n#     os.makedirs(dest_dir, exist_ok=True)\n\n#     src_dir_name = os.path.basename(src_dir)\n#     dest_dir_path = os.path.join(dest_dir, src_dir_name.capitalize())\n#     os.makedirs(dest_dir_path, exist_ok=True)\n\n#     files = os.listdir(src_dir)\n#     total = len(files)\n#     for index, filename in tqdm(\n#         enumerate(files), desc=f\"Moving {src_dir_name}\", total=total\n#     ):\n#         src_path = os.path.join(src_dir, filename)\n#         dest_path = os.path.join(dest_dir_path, filename)\n#         # print(\"Copying\", src_path, dest_path)\n#         shutil.copy(src_path, dest_path)\n#         # print(f\"Moved {index+1} files from {src_dir} to {dest_dir_path}\")\n\n\n# def move_and_rename(class_dir: str):\n#     \"\"\"Move files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, ...), which contains image files.\n\n#     Parameters\n#     ----------\n#     class_dir : str\n#         _description_\n#     \"\"\"\n#     target_classes = os.listdir(\"data/2_processed/\")\n#     if \"tmp\" in target_classes:\n#         target_classes.remove(\"tmp\")\n#     target_classes_lower = list(map(lambda x: x.lower(), target_classes))\n\n#     for subclass_dir in os.listdir(class_dir):\n#         if subclass_dir.lower() in target_classes_lower:\n#             subclass_dir_path = os.path.join(class_dir, subclass_dir)\n#             move_files(subclass_dir_path)\n#             rename_files()\n#             shutil.rmtree(\"data/2_processed/tmp\")"
  },
  {
    "objectID": "a_datasets.html#downloading-datasets",
    "href": "a_datasets.html#downloading-datasets",
    "title": "Downloading Data",
    "section": "Downloading datasets",
    "text": "Downloading datasets\n\nDataset 1\nkaggle datasets download salmaneunus/rock-classification --path data/0_raw/\nunzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\nmv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n\n\nDataset 2\nkaggle datasets download mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals --path data/0_raw/\nunzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\nmv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n\nrm -rf data/1_extracted/dataset2/minerals\n\n\nDataset 3\nwget --quiet -O data/0_raw/dataset3.zip https://github.com/SmartPracticeschool/llSPS-INT-3797-Rock-identification-using-deep-convolution-neural-network/raw/master/dataset.zip\nunzip -qn data/0_raw/dataset3.zip -d data/1_extracted/\nmv data/1_extracted/dataset data/1_extracted/dataset3\n\n\nDataset 4\nkaggle datasets download neelgajare/rocks-dataset --path data/0_raw/\nunzip -qn data/0_raw/rocks-dataset.zip -d data/1_extracted/\nmkdir -p data/1_extracted/dataset4/rock_classes/\nmv -n data/1_extracted/Rocks/* data/1_extracted/dataset4/\nrm -rf data/1_extracted/Rocks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types."
  },
  {
    "objectID": "index.html#installation-training-steps",
    "href": "index.html#installation-training-steps",
    "title": "Whats-this-rock",
    "section": "Installation & Training Steps",
    "text": "Installation & Training Steps\n\nUse the Telegram Bot\nYou can try the bot here on Telegram.\nType /help to get instructions.\n\n\nDeploy Telegram Bot\npip install -r requirements-prod.txt\npython src/bot.py\n\n\nTrain Model\nPaste your kaggle.json file in the root directory\nRun these commands\npip install -r requirements-dev.txt\nsh src/scripts/setup.sh\npython src/models/train.py\nYou can try different models and parameters by editing config.json.\nBy using Hydra it’s now much more easier to override parameters like this\npython src/models/train.py  wandb.project=Whats-this-rockv \\\n                            dataset_id=[1,2,3,4] \\\n                            epochs=50 \\\n                            backbone=resnet\n\n\n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweeps.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid\nwandb agent udaylunawat/Whats-this-rock/$sweepid"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n\n\n\n\n<style=‘font-size:37px’>Features added\n\n\n<style=‘font-size:37px’>Features planned\n\n\n\n\n\nWandb\nDatasets\n\n4 Datasets\n\nAugmentation\n\nkeras-cv\nRegular Augmentation\n\nSampling\n\nOversampling\nUndersampling\nClass weights\n\nRemove Corrupted Images\nTry Multiple Optimizers (Adam, RMSProp, AdamW, SGD)\nGenerators\n\nTFDS datasets\nImageDataGenerator\n\nModels\n\nConvNextTiny\nBaselineCNN\nEfficientnet\nResnet101\nMobileNetv1\nMobileNetv2\nXception\n\nLRScheduleer, LRDecay\n\nBaseline without scheduler\nStep decay\nCosine annealing\nClassic cosine annealing with bathc steps w/o restart\n\nModel Checkpoint, Resume Training\nEvaluation\n\nConfusion Matrix\nClassification Report\n\nDeploy Telegram Bot\n\nHeroku - Deprecated\nRailway\nShow CM and CL in bot\n\nDocker\nGitHub Actions\n\nDeploy Bot when bot.py is updated.\nLint code using GitHub super-linter\n\nConfiguration Management\n\nml-collections\nHydra\n\nPerformance improvement\n\nConvert to tf.data.Dataset\n\nLinting & Formatting\n\nBlack\nFlake8\nisort\npydocstyle\n\n\n\nDeploy to Huggingface spaces\nAccessing the model through FastAPI (Backend)\nStreamlit (Frontend)\nconvert models.py to Classes and more OOP style\nnbdev\nGroup Runs\nkfold cross validation\n\nWandB Tables\nfind the long tail examples or hard examples,\nfind the classes that the model is performing terribly on,\nAdd Badges\nLinting\nRailway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n├── imgs                              <- Images for skill banner, project banner and other images\n│\n├── configs                           <- Configuration files\n│   ├── configs.yaml                  <- config for single run\n│   └── sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n│\n├── data\n│   ├── corrupted_images              <- corrupted images will be moved to this directory\n│   ├── sample_images                 <- Sample images for inference\n│   ├── 0_raw                         <- The original, immutable data dump.\n│   ├── 1_external                    <- Data from third party sources.\n│   ├── 2_interim                     <- Intermediate data that has been transformed.\n│   └── 3_processed                   <- The final, canonical data sets for modeling.\n│\n├── notebooks                         <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                                        the creator's initials, and a short `-` delimited description, e.g.\n│                                        1.0-jqp-initial-data-exploration`.\n│\n│\n├── src                               <- Source code for use in this project.\n│   │\n│   ├── data                          <- Scripts to download or generate data\n│   │   ├── download.py\n│   │   ├── preprocess.py\n│   │   └── utils.py\n│   │\n│   ├── callbacks                     <- functions that are executed during training at given stages of the training procedure\n│   │   ├── custom_callbacks.py\n│   │   └── callbacks.py\n│   │\n│   ├── models                        <- Scripts to train models and then use trained models to make\n│   │   │                                predictions\n│   │   ├── evaluate.py\n│   │   ├── models.py\n│   │   ├── predict.py\n│   │   ├── train.py\n│   │   └── utils.py\n│   │\n│   └── scripts                       <- Scripts to setup dir structure and download datasets\n│   │   ├── clean_dir.sh\n│   │   ├── dataset1.sh\n│   │   ├── dataset2.sh\n│   │   ├── dataset3.sh\n│   │   ├── dataset4.sh\n│   │   └── setup.sh\n│.  │\n│   └── visualization                 <- Scripts for visualizations\n│\n├── .dockerignore                     <- Docker ignore\n├── .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n├── LICENSE                           <- Your project's license.\n├── Makefile                          <- Makefile with commands like `make data` or `make train`\n├── README.md                         <- The top-level README for developers using this project.\n├── requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n│                                        generated with `pip freeze > requirements.txt`\n└── setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn’t handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you’d like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nLinkFree is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset - by Mahmoud Alforawi"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ⭐️ from you. Don’t forget to leave a star ⭐️\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  }
]