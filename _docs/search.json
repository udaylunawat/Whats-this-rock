[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()"
  },
  {
    "objectID": "sweeps.html",
    "href": "sweeps.html",
    "title": "Hyperparameter Tuning using Wandb Sweeps",
    "section": "",
    "text": "#@title Wandb mode\nimport os\nmode = 'online' #@param {type:\"string\"}\nos.environ['WANDB_MODE'] = mode\n\nif 'WANDB_API_KEY' not in os.environ:\n    if os.environ['WANDB_MODE'] == 'online':\n        from getpass import getpass\n        secret = getpass('Enter WandB API Key: ')\n        os.environ['WANDB_API_KEY'] = secret\n    else:\n        print(\"WandB Offline!\")\n\n\n#@title Setup\n!rm -rf /content/Whats-this-rock/\n!git clone https://github.com/udaylunawat/Whats-this-rock.git\n# !git clone -b hydra https://github.com/udaylunawat/Whats-this-rock.git\n\n\n!nvidia-smi\n!sh src/scripts/setup.sh\n\n\n!git pull\n\n\nmethod: bayes\nmetric:\n  goal: minimize\n  name: val_loss\nparameters:\n  wandb.use:\n    value: True\n  wandb.mode:\n    value: online\n  wandb.project:\n    value: Whats-this-rockv18\n  notes:\n    value: \"\"\n  seed:\n    values: [1]\n  lr:\n    values: [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5]\n  lr_decay_steps:\n    distribution: uniform\n    min: 10\n    max: 10000\n  lr_schedule:\n    values:\n      - cosine_decay_restarts\n  epochs:\n    value: 75\n  data_path:\n    value: data/4_tfds_dataset/\n  dataset_id:\n    values:\n      - [1, 2, 3, 4]\n  augmentation:\n    values: [kerascv]\n  class_weights:\n    values: [True, False]\n  optimizer:\n    values: [adam]\n  loss:\n    values: [categorical_crossentropy]\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  train_split:\n    values:\n      - 0.75\n  image_size:\n    value: 224\n  image_channels:\n    value: 3\n  sampling:\n    values: [None]\n  backbone:\n    values: [resnet]\n  use_pretrained_weights:\n    values: [True]\n  trainable:\n    values: [False, True]\n  last_layers:\n    distribution: int_uniform\n    min: 1\n    max: 50\n  custom_callback:\n    values: [False]\n  preprocess:\n    values: [True, False]\n  dropout_rate:\n    values: [0.3, 0.5]\n  monitor:\n    values: [\"val_loss\"]\n  earlystopping.use:\n    values: [False]\n  earlystopping.patience:\n    values: [10]\n  reduce_lr.use:\n    values: [False]\n  reduce_lr.factor:\n    values: [.9]\n  reduce_lr.patience:\n    values: [1]\n  reduce_lr.min_lr:\n    values: [1e-6]\n  save_model:\n    value: False\n\nprogram: src/models/train.py\ncommand:\n  - ${env}\n  - python\n  - ${program}\n  - ${args_no_hyphens}\n\n\n!wandb sweep \\\n    --project Whats-this-rockv17 \\\n    --entity udaylunawat \\\n    configs/sweep.yaml\n\n\n!wandb agent udaylunawat/Whats-this-rockv17/xrjlfpy0"
  },
  {
    "objectID": "b_data_utils.html",
    "href": "b_data_utils.html",
    "title": "Data utils",
    "section": "",
    "text": "move_and_rename\n\n move_and_rename (class_dir:str)\n\nMove files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, ‚Ä¶), which contains image files.\n\n\n\n\nType\nDetails\n\n\n\n\nclass_dir\nstr\ndescription\n\n\n\n\n\n\nmove_files\n\n move_files (src_dir:str, dest_dir:str='data/2_processed/tmp')\n\nMove files to tmp directory in 2_processed.\nsrc_dir: directory of rock subclass with files [Basalt, Marble, Coal, ‚Ä¶]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\ndescription\n\n\ndest_dir\nstr\ndata/2_processed/tmp\ndescription, by default ‚Äúdata/2_processed/tmp‚Äù\n\n\n\n\n\n\nrename_files\n\n rename_files (source_dir:str='data/2_processed/tmp')\n\nRename files in classes and moves to 2_processed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_dir\nstr\ndata/2_processed/tmp\ndescription, by default ‚Äúdata/2_processed/tmp‚Äù\n\n\n\n\n\n\nget_tfds_from_dir\n\n get_tfds_from_dir (cfg)\n\nConvert directory of images to tfds dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\nprepare\n\n prepare (ds, cfg, shuffle=False, augment=False)\n\nPrepare dataset using augment, preprocess, cache, shuffle and prefetch.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\ntype\n\ndescription\n\n\ncfg\ncfg (omegaconf.DictConfig):\n\nHydra Configuration\n\n\nshuffle\nbool\nFalse\ndescription, by default False\n\n\naugment\nbool\nFalse\ndescription, by default False\n\n\nReturns\ntype\n\ndescription\n\n\n\n\n\n\nget_preprocess\n\n get_preprocess (cfg)\n\nReturn preprocess function for particular model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\nscalar\n\n scalar (img:<module'PIL.Image'from'/opt/anaconda3/envs/rocks/lib/python3.\n         10/site-packages/PIL/Image.py'>)\n\nScale pixel between -1 and +1.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimg\nPIL.Image\nPIL Image\n\n\nReturns\nPIL.Image\nimagew with pixel values scaled between -1 and 1\n\n\n\n\n\n\nget_value_counts\n\n get_value_counts (dataset_path:str)\n\nGet class counts of all classes in the dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ndataset_path\nstr\ndirectory with subclasses\n\n\nReturns\nNone\n\n\n\n\n\n\n\nget_df\n\n get_df (root:str='data/2_processed')\n\nReturn df with classes, image paths and file names.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\ndata/2_processed\ndirectory to scan for image files, by default ‚Äúdata/2_processed‚Äù\n\n\nReturns\nDataFrame\n\nwith columns file_name, class and file_path\n\n\n\n\n\n\nget_dims\n\n get_dims (file:str)\n\nReturn dimenstions for an RBG image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nstr\nfile path for image\n\n\nReturns\nOptional\nreturns a tuple of heights and width of image or None\n\n\n\n\n\n\ntimer_func..wrap_func\n\n timer_func.<locals>.wrap_func (*args, **kwargs)\n\n\n\n\nremove_unsupported_images\n\n remove_unsupported_images (root_folder:str)\n\nRemove unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\nRoot Folder.\n\n\n\n\n\n\nfind_filepaths\n\n find_filepaths (root_folder:str)\n\nRecursively finds all files.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\ntimer_func\n\n timer_func (func)\n\nShow the execution time of the function object passed.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\ntype\ndescription"
  },
  {
    "objectID": "detailed_trainer.html",
    "href": "detailed_trainer.html",
    "title": "Trainer",
    "section": "",
    "text": "Setup\n\n!rm -rf /content/Whats-this-rock/\n!git clone -b hydra https://github.com/udaylunawat/Whats-this-rock.git\n\n/content\nCloning into 'Whats-this-rock'...\nremote: Enumerating objects: 2093, done.\nremote: Counting objects: 100% (333/333), done.\nremote: Compressing objects: 100% (246/246), done.\nremote: Total 2093 (delta 152), reused 188 (delta 86), pack-reused 1760\nReceiving objects: 100% (2093/2093), 5.84 MiB | 2.85 MiB/s, done.\nResolving deltas: 100% (1355/1355), done.\n/content/Whats-this-rock\n\n\n\nimport os\nos.environ['WANDB_MODE'] = 'offline' # offline\n\nif 'WANDB_API_KEY' not in os.environ:\n    if os.environ['WANDB_MODE'] == 'online':\n        from getpass import getpass\n        secret = getpass('Enter WandB API Key: ')\n        os.environ['WANDB_API_KEY'] = secret\n    else:\n        print(\"WandB Offline!\")\n\nWandB Offline!\n\n\n\n!sh src/scripts/setup.sh\n\nThe following package was automatically installed and is no longer required:\n  libnvidia-common-460\nUse 'apt autoremove' to remove it.\nThe following packages will be REMOVED:\n  libcudnn8-dev\nThe following held packages will be changed:\n  libcudnn8\nThe following packages will be upgraded:\n  libcudnn8\n1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\nNeed to get 430 MB of archives.\nAfter this operation, 3,139 MB disk space will be freed.\n(Reading database ... 155569 files and directories currently installed.)\nRemoving libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n(Reading database ... 155547 files and directories currently installed.)\nPreparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\nUnpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\nSetting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.8 MB 28.4 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151 kB 74.1 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158 kB 59.3 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181 kB 72.3 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 1.7 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 72.6 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 72.9 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 75.3 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 75.1 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 84.0 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 64.6 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 78.5 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 156 kB 73.5 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79 kB 8.3 MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117 kB 76.0 MB/s \n  Building wheel for antlr4-python3-runtime (setup.py) ... done\n  Building wheel for pathtools (setup.py) ... done\nrenamed 'kaggle.json' -> '/root/.kaggle/kaggle.json'\n\n\n\n!sh src/scripts/clean_dir.sh\n\n\n#!/usr/bin/env python\n\"\"\"\nTrains a model on rocks dataset\n\"\"\"\n\nimport os\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\nimport gc\nimport subprocess\nimport random\nimport click\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import classification_report\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n# speed improvements\nfrom tensorflow.keras import mixed_precision\n\nmixed_precision.set_global_policy('mixed_float16')\n\nimport wandb\nfrom wandb.keras import WandbCallback\n\nfrom absl import app\n\nimport hydra\nfrom omegaconf import DictConfig\n\nfrom src.data.preprocess import process_data\nfrom src.models.models import get_model\nfrom src.data.utils import get_tfds_from_dir, prepare\nfrom src.models.utils import get_optimizer, get_model_weights_ds\nfrom src.data.download import get_data\nfrom src.callbacks.callbacks import get_earlystopper, get_reduce_lr_on_plateau\nfrom src.visualization import plot\n\n\ndef seed_everything(seed):\n    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nModuleNotFoundError: ignored\n\n\n\n# Get configs from the config file.\nconfig = get_config()\n\nconfig.seed = 42\nconfig.dataset_config.train_dataset = [1,2,3,4]\nconfig.dataset_config.sampling = None\n\nconfig.train_config.lr = 0.001\nconfig.train_config.epochs = 50\nconfig.train_config.loss = \"categorical_crossentropy\"\n\nconfig.model_config.preprocess = True\nconfig.model_config.backbone = 'resnet'\nconfig.model_config.regularize_more = False\n\nconfig.callback_config.save_model = False\nconfig.callback_config.rlrp_factor = 0.8\nconfig.callback_config.rlrp_patience = 1\nconfig.callback_config.early_patience = 10\n\n\nprint(config)\n\n\nfrom src.train import train\nfrom src.train import evaluate\n\nseed_everything(config.seed)\n\nrun = wandb.init(\n    project=config.wandb_config.project,\n    config=config.to_dict(),\n    allow_val_change=True,\n)\n\nartifact = wandb.Artifact('rocks', type='files')\nartifact.add_dir('src/')\nwandb.log_artifact(artifact)\n\nprint(f\"\\nDatasets used for Training:- {config.dataset_config.train_dataset}\")\n\nfor dataset_id in config.dataset_config.train_dataset:\n    get_data(dataset_id)\n\nif not os.path.exists('data/4_tfds_dataset/train'):\n    process_data(config)\n\ntrain_dataset, val_dataset, test_dataset = get_tfds_from_dir(config)\n\nlabels = [\n    \"Basalt\",\n    \"Coal\",\n    \"Granite\",\n    \"Limestone\",\n    \"Marble\",\n    \"Quartzite\",\n    \"Sandstone\",\n]\n## Update the `num_classes` and update wandb config\nconfig.dataset_config.num_classes = 7\nif wandb.run is not None:\n    wandb.config.update(\n        {\"dataset_config.num_classes\": config.dataset_config.num_classes})\n    \nmodel, history = train(config, train_dataset, val_dataset, labels)\nevaluate(config, model, history, test_dataset, labels)\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntest_ds = prepare(test_dataset, config)\n# Scores\nscores = model.evaluate(test_dataset, return_dict=True)\nprint('Scores: ', scores)\n\ny_true = tf.concat([y for x, y in test_ds], axis=0)\ntrue_categories = tf.argmax(y_true, axis=1)\n\n# Predict\ny_pred = model.predict(test_dataset, verbose=1)\npredicted_categories = tf.argmax(y_pred, axis=1)\n\n# Confusion Matrix\ncm = plot.plot_confusion_matrix(labels, true_categories, predicted_categories)\n\n# Classification Report\ncl_report = classification_report(true_categories,\n                                  predicted_categories,\n                                  labels=[0, 1, 2, 3, 4, 5, 6],\n                                  target_names=labels,\n                                  output_dict=True)\nprint(cl_report)\n\ncr = sns.heatmap(pd.DataFrame(cl_report).iloc[:-1, :].T, annot=True)\nplt.savefig('imgs/cr.png', dpi=400)\n\n6/6 [==============================] - 2s 302ms/step - loss: 4.8600 - accuracy: 0.3476 - f1_score: 0.3195\nScores:  {'loss': 4.859963417053223, 'accuracy': 0.34756097197532654, 'f1_score': 0.3195052146911621}\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n{'Basalt': {'precision': 0.05555555555555555, 'recall': 0.045454545454545456, 'f1-score': 0.049999999999999996, 'support': 22}, 'Coal': {'precision': 0.4727272727272727, 'recall': 0.4406779661016949, 'f1-score': 0.45614035087719296, 'support': 59}, 'Granite': {'precision': 0.3333333333333333, 'recall': 0.4, 'f1-score': 0.3636363636363636, 'support': 25}, 'Limestone': {'precision': 0.3888888888888889, 'recall': 0.375, 'f1-score': 0.3818181818181819, 'support': 56}, 'Marble': {'precision': 0.34210526315789475, 'recall': 0.24074074074074073, 'f1-score': 0.2826086956521739, 'support': 54}, 'Quartzite': {'precision': 0.3291139240506329, 'recall': 0.4, 'f1-score': 0.3611111111111111, 'support': 65}, 'Sandstone': {'precision': 0.3148148148148148, 'recall': 0.3617021276595745, 'f1-score': 0.33663366336633666, 'support': 47}, 'accuracy': 0.3475609756097561, 'macro avg': {'precision': 0.31950557893262754, 'recall': 0.32336791142236515, 'f1-score': 0.31884976663733716, 'support': 328}, 'weighted avg': {'precision': 0.34721532925108595, 'recall': 0.3475609756097561, 'f1-score': 0.34463378640286313, 'support': 328}}\n\n\n\n\n\n\ntest_ds = prepare(test_dataset, config)\n# Scores\nscores = model.evaluate(test_dataset, return_dict=True)\nprint(\"Scores: \", scores)\n\ny_true = tf.concat([y for x, y in test_ds], axis=0)\ntrue_categories = tf.argmax(y_true, axis=1)\n\n# Predict\ny_pred = model.predict(test_ds)\npredicted_categories = tf.argmax(y_pred, axis=1)\n\n# Confusion Matrix\ncm = plot.plot_confusion_matrix(labels, true_categories,\n                            predicted_categories)\n\n# Classification Report\ncl_report = classification_report(\n    true_categories,\n    predicted_categories,\n    labels=[0, 1, 2, 3, 4, 5, 6],\n    target_names=labels,\n    output_dict=True,\n)\nprint(cl_report)\n\ncr = sns.heatmap(pd.DataFrame(cl_report).iloc[:-1, :].T, annot=True)\nplt.savefig(\"cr.png\", dpi=400)\n\nwandb.log({\"Test Accuracy\": scores[\"accuracy\"]})\nwandb.log({\"Test F1 Score\": scores[\"f1_score\"]})\n\n# average of val and test f1 score\nwandb.log({\n    \"Avg VT F1 Score\":\n    (scores[\"f1_score\"] + max(history.history[\"val_f1_score\"])) / 2\n})\nwandb.log({\"Confusion Matrix\": cm})\nwandb.log({\n    \"Classification Report Image:\":\n    wandb.Image(\"cr.png\", caption=\"Classification Report\")\n})\n\nrun.finish()\n\n6/6 [==============================] - 2s 336ms/step - loss: 4.8600 - accuracy: 0.3476 - f1_score: 0.3195\nScores:  {'loss': 4.859963417053223, 'accuracy': 0.34756097197532654, 'f1_score': 0.3195052146911621}\n6/6 [==============================] - 1s 219ms/step\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n{'Basalt': {'precision': 0.05263157894736842, 'recall': 0.045454545454545456, 'f1-score': 0.04878048780487805, 'support': 22}, 'Coal': {'precision': 0.5370370370370371, 'recall': 0.4915254237288136, 'f1-score': 0.5132743362831859, 'support': 59}, 'Granite': {'precision': 0.36363636363636365, 'recall': 0.32, 'f1-score': 0.3404255319148936, 'support': 25}, 'Limestone': {'precision': 0.45614035087719296, 'recall': 0.4642857142857143, 'f1-score': 0.46017699115044247, 'support': 56}, 'Marble': {'precision': 0.4, 'recall': 0.37037037037037035, 'f1-score': 0.3846153846153846, 'support': 54}, 'Quartzite': {'precision': 0.3013698630136986, 'recall': 0.3384615384615385, 'f1-score': 0.31884057971014496, 'support': 65}, 'Sandstone': {'precision': 0.3584905660377358, 'recall': 0.40425531914893614, 'f1-score': 0.38, 'support': 47}, 'accuracy': 0.38109756097560976, 'macro avg': {'precision': 0.3527579656499138, 'recall': 0.3477647016357026, 'f1-score': 0.3494447587827042, 'support': 328}, 'weighted avg': {'precision': 0.38267056817598527, 'recall': 0.38109756097560976, 'f1-score': 0.381069435442386, 'support': 328}}\n\n\nError: ignored"
  },
  {
    "objectID": "training.html#login-to-wandb",
    "href": "training.html#login-to-wandb",
    "title": "Training Notebook",
    "section": "ü™Ñ Login to WandB",
    "text": "ü™Ñ Login to WandB\n\nimport os\nos.environ['WANDB_MODE'] = 'offline' # offline\n\nif 'WANDB_API_KEY' not in os.environ:\n    if os.environ['WANDB_MODE'] == 'online':\n        from getpass import getpass\n        secret = getpass('Enter WandB API Key: ')\n        os.environ['WANDB_API_KEY'] = secret\n    else:\n        print(\"WandB Offline!\")\n\nWandB Offline!"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Notebook",
    "section": "",
    "text": "Code\ndf = get_df(\"data/2_processed\")\ndf['dimensions'] = df['file_path'].apply(lambda x: get_dims(x))\ndf['image_width'] = df['dimensions'].apply(lambda x: x[0] if x is not None else None)\ndf['image_height'] = df['dimensions'].apply(lambda x: x[1] if x is not None else None)\ndf['pixels'] = df['image_width'] * df['image_height']\n# df['corrupt_status'] = df['file_path'].apply(lambda x: check_corrupted(x))\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      file_name\n      class\n      file_path\n      dimensions\n      image_width\n      image_height\n      pixels\n    \n  \n  \n    \n      0\n      Marble_175.jpg\n      Marble\n      data/2_processed/Marble/Marble_175.jpg\n      (408, 612)\n      408\n      612\n      249696\n    \n    \n      1\n      Marble_443.png\n      Marble\n      data/2_processed/Marble/Marble_443.png\n      (632, 474)\n      632\n      474\n      299568\n    \n    \n      2\n      Marble_123.jpg\n      Marble\n      data/2_processed/Marble/Marble_123.jpg\n      (219, 230)\n      219\n      230\n      50370\n    \n    \n      3\n      Marble_287.jpg\n      Marble\n      data/2_processed/Marble/Marble_287.jpg\n      (183, 275)\n      183\n      275\n      50325\n    \n    \n      4\n      Marble_166.jpg\n      Marble\n      data/2_processed/Marble/Marble_166.jpg\n      (160, 316)\n      160\n      316\n      50560\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3379\n      Limestone_462.jpg\n      Limestone\n      data/2_processed/Limestone/Limestone_462.jpg\n      (168, 300)\n      168\n      300\n      50400\n    \n    \n      3380\n      Limestone_416.jpg\n      Limestone\n      data/2_processed/Limestone/Limestone_416.jpg\n      (232, 217)\n      232\n      217\n      50344\n    \n    \n      3381\n      Limestone_191.jpg\n      Limestone\n      data/2_processed/Limestone/Limestone_191.jpg\n      (409, 626)\n      409\n      626\n      256034\n    \n    \n      3382\n      Limestone_504.jpg\n      Limestone\n      data/2_processed/Limestone/Limestone_504.jpg\n      (189, 267)\n      189\n      267\n      50463\n    \n    \n      3383\n      Limestone_134.jpg\n      Limestone\n      data/2_processed/Limestone/Limestone_134.jpg\n      (408, 612)\n      408\n      612\n      249696\n    \n  \n\n3384 rows √ó 7 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\nCode\ndf['file_name'].apply(lambda x: x.split('.')[-1]).value_counts()\n\n\njpg     2959\npng      398\njpeg      25\nJPEG       2\nName: file_name, dtype: int64\n\n\n\n\n\n\n\nCode\ndf['corrupt_status'] = df['file_path'].apply(lambda x: check_corrupted(x))\ndf.corrupt_status.value_counts()\n\n\nTrue    382\nName: corrupt_status, dtype: int64\n\n\n\n\n\n\n\nCode\ndf[df['corrupt_status']==True]\n\n\nNameError: name 'df' is not defined\n\n\n\n\nCode\nimport seaborn as sns\n\nclass_names = df['class'].value_counts().keys()\ncounts = df['class'].value_counts().values\n\ncount_df = pd.DataFrame(list(zip(class_names, counts)), columns=['class', 'count'])\n\nsns.set_theme(style=\"darkgrid\")\nax = sns.barplot(y='class', x='count', data=count_df)\n\n\n/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\nNameError: name 'df' is not defined\n\n\n\n\nCode\nwidth_list = df.image_width\nheight_list = df.image_height\naverage_width = sum(width_list)/len(width_list)\naverage_height = sum(height_list)/len(height_list)\n\nprint('average width: {} and height: {}'.format(average_width, average_height))\n\nfig, ax =plt.subplots(1,2, figsize=(15, 8))\n\nsns.distplot(width_list, ax=ax[0])\nax[0].set_title('Image width')\nsns.distplot(height_list, ax=ax[1])\nax[1].set_title('Image height')\nfig.show()\n\n\naverage width: nan and height: nan\n\n\n/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\nCode\n# plot histograms to show the distribution of width and height values\nfig, axs = plt.subplots(1,2, figsize=(15,7))\naxs[0].hist(df.image_width.values, bins=20, color = '#91bd3a')\naxs[0].set_title('Width distribution')\n# axs[0].set_xlim(1000, 3000)\n\naxs[1].hist(df.image_height.values, bins=20, color = '#91bd3a')\naxs[1].set_title('Height distribution')\n# axs[1].set_xlim(1000, 3000)\n\nplt.suptitle('Image Dimensions')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nget_df(\"data/4_tfds_dataset/train\")['class'].value_counts()\n\n\nQuartzite    492\nLimestone    438\nMarble       431\nCoal         385\nSandstone    370\nGranite      254\nBasalt       211\nName: class, dtype: int64\n\n\n\n\n\n\n\nCode\nget_df(\"data/4_tfds_dataset/val\")['class'].value_counts()\n\n\nQuartzite    82\nLimestone    73\nMarble       71\nCoal         64\nSandstone    61\nGranite      42\nBasalt       35\nName: class, dtype: int64\n\n\n\n\n\n\n\nCode\nget_df(\"data/4_tfds_dataset/test\")['class'].value_counts()\n\n\nQuartzite    82\nMarble       73\nLimestone    73\nCoal         65\nSandstone    63\nGranite      43\nBasalt       36\nName: class, dtype: int64"
  },
  {
    "objectID": "eda.html#sample-images",
    "href": "eda.html#sample-images",
    "title": "Exploratory Notebook",
    "section": "Sample Images",
    "text": "Sample Images\n\n\nCode\nds = builder.as_dataset(split='train', shuffle_files=True)\ntfds.show_examples(ds, builder.info)\n\n\nINFO:absl:Constructing tf.data.Dataset image_folder for split train, from data/4_tfds_dataset\n\n\n\n\n\n\n\n\n\nSamples before Augmentation\n\n\nCode\ntrain_dataset = load_dataset()\nvisualize_dataset(train_dataset, title=\"Before Augmentation\")\n\n\n\n\n\n\n\nSamples after RandAugment\n\n\nCode\ntrain_dataset = load_dataset().map(apply_rand_augment, num_parallel_calls=AUTOTUNE)\nvisualize_dataset(train_dataset, title=\"After RandAugment\")\n\n\n\n\n\n\n\nSamples after cutmix and mixup augmentation\n\n\nCode\ntrain_dataset = load_dataset().map(cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE)\nvisualize_dataset(train_dataset, title=\"After cut_mix and mix_up\")"
  },
  {
    "objectID": "a_datasets.html#downloading-datasets",
    "href": "a_datasets.html#downloading-datasets",
    "title": "Downloading Data",
    "section": "Downloading datasets",
    "text": "Downloading datasets\n\nDataset 1\nkaggle datasets download salmaneunus/rock-classification --path data/0_raw/\nunzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\nmv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n\n\nDataset 2\nkaggle datasets download mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals --path data/0_raw/\nunzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\nmv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n\nrm -rf data/1_extracted/dataset2/minerals\n\n\nDataset 3\nwget --quiet -O data/0_raw/dataset3.zip https://github.com/SmartPracticeschool/llSPS-INT-3797-Rock-identification-using-deep-convolution-neural-network/raw/master/dataset.zip\nunzip -qn data/0_raw/dataset3.zip -d data/1_extracted/\nmv data/1_extracted/dataset data/1_extracted/dataset3\n\n\nDataset 4\nkaggle datasets download neelgajare/rocks-dataset --path data/0_raw/\nunzip -qn data/0_raw/rocks-dataset.zip -d data/1_extracted/\nmkdir -p data/1_extracted/dataset4/rock_classes/\nmv -n data/1_extracted/Rocks/* data/1_extracted/dataset4/\nrm -rf data/1_extracted/Rocks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types."
  },
  {
    "objectID": "index.html#installation-training-steps",
    "href": "index.html#installation-training-steps",
    "title": "Whats-this-rock",
    "section": "Installation & Training Steps",
    "text": "Installation & Training Steps\n\nUse the Telegram Bot\nYou can try the bot here on Telegram.\nType /help to get instructions.\n\n\nDeploy Telegram Bot\npip install -r requirements-prod.txt\npython src/bot.py\n\n\nTrain Model\nPaste your kaggle.json file in the root directory\nRun these commands\npip install -r requirements-dev.txt\nsh src/scripts/setup.sh\npython src/models/train.py\nYou can try different models and parameters by editing config.json.\nBy using Hydra it‚Äôs now much more easier to override parameters like this\npython src/models/train.py  wandb.project=Whats-this-rockv \\\n                            dataset_id=[1,2,3,4] \\\n                            epochs=50 \\\n                            backbone=resnet\n\n\n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweeps.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid\nwandb agent udaylunawat/Whats-this-rock/$sweepid"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n\n\n\n\n<style=‚Äòfont-size:37px‚Äô>Features added\n\n\n<style=‚Äòfont-size:37px‚Äô>Features planned\n\n\n\n\n\nWandb\nDatasets\n\n4 Datasets\n\nAugmentation\n\nkeras-cv\nRegular Augmentation\n\nSampling\n\nOversampling\nUndersampling\nClass weights\n\nRemove Corrupted Images\nTry Multiple Optimizers (Adam, RMSProp, AdamW, SGD)\nGenerators\n\nTFDS datasets\nImageDataGenerator\n\nModels\n\nConvNextTiny\nBaselineCNN\nEfficientnet\nResnet101\nMobileNetv1\nMobileNetv2\nXception\n\nLRScheduleer, LRDecay\n\nBaseline without scheduler\nStep decay\nCosine annealing\nClassic cosine annealing with bathc steps w/o restart\n\nModel Checkpoint, Resume Training\nEvaluation\n\nConfusion Matrix\nClassification Report\n\nDeploy Telegram Bot\n\nHeroku - Deprecated\nRailway\nShow CM and CL in bot\n\nDocker\nGitHub Actions\n\nDeploy Bot when bot.py is updated.\nLint code using GitHub super-linter\n\nConfiguration Management\n\nml-collections\nHydra\n\nPerformance improvement\n\nConvert to tf.data.Dataset\n\nLinting & Formatting\n\nBlack\nFlake8\nisort\npydocstyle\n\n\n\nDeploy to Huggingface spaces\nAccessing the model through FastAPI (Backend)\nStreamlit (Frontend)\nconvert models.py to Classes and more OOP style\nnbdev\nGroup Runs\nkfold cross validation\n\nWandB Tables\nfind the long tail examples or hard examples,\nfind the classes that the model is performing terribly on,\nAdd Badges\nLinting\nRailway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n‚îú‚îÄ‚îÄ imgs                              <- Images for skill banner, project banner and other images\n‚îÇ\n‚îú‚îÄ‚îÄ configs                           <- Configuration files\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ configs.yaml                  <- config for single run\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n‚îÇ\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ corrupted_images              <- corrupted images will be moved to this directory\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ sample_images                 <- Sample images for inference\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 0_raw                         <- The original, immutable data dump.\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 1_external                    <- Data from third party sources.\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2_interim                     <- Intermediate data that has been transformed.\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 3_processed                   <- The final, canonical data sets for modeling.\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks                         <- Jupyter notebooks. Naming convention is a number (for ordering),\n‚îÇ                                        the creator's initials, and a short `-` delimited description, e.g.\n‚îÇ                                        1.0-jqp-initial-data-exploration`.\n‚îÇ\n‚îÇ\n‚îú‚îÄ‚îÄ src                               <- Source code for use in this project.\n‚îÇ   ‚îÇ\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data                          <- Scripts to download or generate data\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ download.py\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ preprocess.py\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ utils.py\n‚îÇ   ‚îÇ\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ callbacks                     <- functions that are executed during training at given stages of the training procedure\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ custom_callbacks.py\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ callbacks.py\n‚îÇ   ‚îÇ\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models                        <- Scripts to train models and then use trained models to make\n‚îÇ   ‚îÇ   ‚îÇ                                predictions\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ evaluate.py\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models.py\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict.py\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ train.py\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ utils.py\n‚îÇ   ‚îÇ\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ scripts                       <- Scripts to setup dir structure and download datasets\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ clean_dir.sh\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dataset1.sh\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dataset2.sh\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dataset3.sh\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dataset4.sh\n‚îÇ¬†¬† ‚îÇ   ‚îî‚îÄ‚îÄ setup.sh\n‚îÇ.  ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ visualization                 <- Scripts for visualizations\n‚îÇ\n‚îú‚îÄ‚îÄ .dockerignore                     <- Docker ignore\n‚îú‚îÄ‚îÄ .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n‚îú‚îÄ‚îÄ LICENSE                           <- Your project's license.\n‚îú‚îÄ‚îÄ Makefile                          <- Makefile with commands like `make data` or `make train`\n‚îú‚îÄ‚îÄ README.md                         <- The top-level README for developers using this project.\n‚îú‚îÄ‚îÄ requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n‚îÇ                                        generated with `pip freeze > requirements.txt`\n‚îî‚îÄ‚îÄ setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn‚Äôt handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you‚Äôd like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nLinkFree is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset - by Mahmoud Alforawi"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ‚≠êÔ∏è from you. Don‚Äôt forget to leave a star ‚≠êÔ∏è\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  }
]