[
  {
    "objectID": "03_d_train.html",
    "href": "03_d_train.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "main\n\n main ()\n\nRun Main function.\n\n\n\nevaluate\n\n evaluate (cfg:omegaconf.dictconfig.DictConfig,\n           model:keras.engine.training.Model, history:dict,\n           test_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n           labels:list)\n\nEvaluate the trained model on Test Dataset, log confusion matrix and classification report.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration.\n\n\nmodel\nModel\nTensorflow model.\n\n\nhistory\ndict\nHistory object.\n\n\ntest_dataset\nDatasetV2\nTest Dataset.\n\n\nlabels\nlist\nList of Labels.\n\n\n\n\n\n\ntrain\n\n train (cfg:omegaconf.dictconfig.DictConfig,\n        train_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        val_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        class_weights:dict)\n\nTrain the model and returns model and history.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration\n\n\ntrain_dataset\nDatasetV2\nTrain Dataset\n\n\nval_dataset\nDatasetV2\nValidation Dataset\n\n\nclass_weights\ndict\nClass weights dictionary\n\n\nReturns\nmodel and history\nmodel and history"
  },
  {
    "objectID": "03_b_train_models.html",
    "href": "03_b_train_models.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "get_model\n\n get_model (cfg)\n\nGet an image classifier with a CNN based backbone.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntensorflow.keras.Model\nModel object\n\n\n\n/Users/uday/miniforge3/envs/rocks/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n\n\n\nget_backbone\n\n get_backbone (cfg)\n\nGet backbone for the model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nkeras.api._v2.keras.models\nTensroflow Model"
  },
  {
    "objectID": "b_download.html",
    "href": "b_download.html",
    "title": "1) Download dataset",
    "section": "",
    "text": "Create directory structure, and if data already exists, clean it!\nrocks_clean_data\n\n\nDownload and verify the data\n$ rocks_clean_data\nDownloads Dataset1 and moves the extracted files to data/1_extracted/dataset1.\nDownloads Dataset2 and moves the extracted files to data/1_extracted/dataset2.\n\n\nCode\nclass download_and_move:\n    \"\"\"Downloads datasets(zip files), extracts them to the correct folders, and rearranges them.\n    \"\"\"\n    data_dict = {\n        1: {\"url\": \"https://huggingface.co/datasets/udayl/rocks/resolve/main/rock-classification.zip\",\n            \"file_name\": \"rock-classification.zip\",\n            \"folder_name\": \"Dataset\",\n            \"filecount\": 2083},\n        2: {\"url\": \"https://huggingface.co/datasets/udayl/rocks/resolve/main/igneous-metamorphic-sedimentary-rocks-and-minerals.zip\", \n            \"file_name\": \"igneous-metamorphic-sedimentary-rocks-and-minerals.zip\",\n            \"folder_name\": \"Rock_Dataset\",\n            \"filecount\": 546},\n    }\n    classes = ['Coal', 'Basalt', 'Granite', 'Marble', 'Quartzite', 'Limestone', 'Sandstone']\n        \n    @timer_func  # | hide_line\n    def run_scripts(self):\n        \"\"\"\n        Download the datasets using scripts.\n\n        Uses `find_filepaths` to recursively find paths for all files in a directory.\n        \"\"\"\n        clean_data_dir()\n        for dataset_id in self.data_dict:\n            if self.archives_exist(dataset_id) and self.files_exists(dataset_id):\n                # if both zip files exist and are extracted\n                print(f\"Dataset{dataset_id} already exists.\")\n                count = self.verify_files(dataset_id)\n                print(f\"Total Files in dataset{dataset_id}:- {count}.\\n\")\n            if not self.archives_exist(dataset_id):\n                # if zip files do not exist\n                print(f\"Extracting dataset {dataset_id}...\")\n                self.download_file(dataset_id)\n            if not self.files_exists(dataset_id):\n                # if zip files exists but they're not extracted\n                self.extract_archive(f'data/0_raw/dataset{dataset_id}.zip')\n                os.rename(f'data/1_extracted/{self.data_dict[dataset_id][\"folder_name\"]}', \n                          f'data/1_extracted/dataset{dataset_id}')\n            self.move_subclasses_to_root_dir(dataset_id)\n            \n                \n    def download_file(self, dataset_id, dest_dir='data/0_raw/'):\n        \"\"\"Download and write file to destination directory.\"\"\"\n        r = requests.get(self.data_dict[dataset_id]['url'], allow_redirects=True)\n        open(os.path.join(dest_dir, f'dataset{dataset_id}.zip'), 'wb').write(r.content)\n        \n    def extract_archive(self, file_path, dest_dir='data/1_extracted'):\n        \"\"\"Extract zip file to dest_dir.\"\"\"\n        shutil.unpack_archive(file_path, dest_dir, 'zip')\n    \n    def archives_exist(self, dataset_id):\n        if os.path.exists(os.path.join('data/0_raw', f'dataset{dataset_id}.zip')):\n            return True\n        \n    def files_exists(self, dataset_id):\n        \"\"\"check whether extracted files exist.\"\"\"\n        if os.path.exists(\n                os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\")):\n            count = self.verify_files(dataset_id)\n            return count\n        else:\n            return False\n    \n    def move_subclasses_to_root_dir(self, dataset_id):\n        \"\"\"Move subclasses to data/2_processed\"\"\"\n        root_path = f'data/1_extracted/dataset{dataset_id}'\n        for class_name in os.listdir(f'data/1_extracted/dataset{dataset_id}'):\n            class_path = os.path.join(root_path, class_name)\n            for subclass in os.listdir(class_path):\n                if subclass.capitalize() in self.classes:\n                    folder_path = os.path.join(class_path, subclass)\n                    shutil.move(folder_path, f'data/1_extracted/dataset{dataset_id}/')\n                    shutil.move(f'{root_path}/{subclass}', f'{root_path}/{subclass.capitalize()}')\n            shutil.rmtree(class_path)\n            \n    \n    def verify_files(self, dataset_id):\n        \"\"\"verifies the image counts\"\"\"\n        _, count = find_filepaths(\n            os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\"))\n        assert count == self.data_dict[dataset_id][\"filecount\"]\n        return count\n\n\n\n\ndownload_and_move\n\n download_and_move ()\n\nDownloads datasets(zip files), extracts them to the correct folders, and rearranges them.\n\n\n\ndownload_and_move_datasets\n\n download_and_move_datasets ()\n\nRun the download and move datasets script."
  },
  {
    "objectID": "05_bot.html",
    "href": "05_bot.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "handle_photo\n\n handle_photo (update, context)\n\n\n\n\nhandle_message\n\n handle_message (update, context)\n\n\n\n\nmodel_details\n\n model_details (update, context)\n\n\n\n\nhelp\n\n help (update, context)\n\n\n\n\nstart\n\n start (update, context)\n\n\n\n\ndeploy_bot\n\n deploy_bot ()"
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "plot_confusion_matrix\n\n plot_confusion_matrix (labels, y, _y)"
  },
  {
    "objectID": "03_a_train_utils.html",
    "href": "03_a_train_utils.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "get_lr_scheduler\n\n get_lr_scheduler (cfg, lr)\n\nReturn A LearningRateSchedule.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\n\n\n\n\nReturns\nkeras.api._v2.keras.optimizers.schedules\nA LearningRateSchedule\n\n\n\n\n\n\nget_model_weights\n\n get_model_weights\n                    (train_ds:tensorflow.python.data.ops.dataset_ops.Datas\n                    etV2)\n\nReturn model weights dict.\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_ds\nDatasetV2\nTensorflow Dataset.\n\n\nReturns\ndict\nDictionary of class weights.\n\n\n\n/Users/uday/miniforge3/envs/rocks/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n\n\n\nget_optimizer\n\n get_optimizer (cfg, lr:str)\n\nGet optimizer set with an learning rate.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\nstr\nlearning rate\n\n\nReturns\nkeras.api._v2.keras.optimizers\nTensorflow optimizer"
  },
  {
    "objectID": "03_e_predict.html",
    "href": "03_e_predict.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "main\n\n main ()\n\n\n\n\nget_prediction\n\n get_prediction (file)\n\nGet prediction for image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nFile\nImage file\n\n\nReturns\nstr\nPrediction with class name and confidence %.\n\n\n\n\n\n\nload_model\n\n load_model ()\n\n\n\n\npreprocess_image\n\n preprocess_image (file, image_size)\n\nDecode and resize image.\n\n\n\n\nType\nDetails\n\n\n\n\nfile\ntype\ndescription\n\n\nimage_size\ntype\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\nget_run_data\n\n get_run_data ()\n\nGet data for a wandb sweep."
  },
  {
    "objectID": "preprocess_data.html",
    "href": "preprocess_data.html",
    "title": "3) Preprocess Data",
    "section": "",
    "text": "This is the description & steps of the preprocess_data function which combines the following functions\n\n\nRename and move files to data/2_processed.\nList files other than jpg and png, to remove unsupported files.\nList file by types before cleaning.\nRemove\n\nBad Images\nDuplicate Images\nMisclassified Images\nUnsupported Images\nCorrupted Images\n\nList file by types after cleaning.\nGet count of files by class types.\nHandle Imbalance using Undersampling, Oversampling.\n\n\n\n\n\n\n\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\n\n\n\nprint(\"\\n\\nFiles other than jpg and png.\\n\")\nfiles, _ = find_filepaths('data/2_processed/')\nprint('\\n'.join(list(filter(lambda x: not x.endswith('jpg') and not x.endswith('png'), files))))\n\n\n\nFiles other than jpg and png.\n\ndata/2_processed/.DS_Store\ndata/2_processed/Coal/dataset1_Coal_025_12.jpeg\ndata/2_processed/Coal/dataset1_Coal_070_162.jpeg\ndata/2_processed/Coal/dataset1_Coal_071_163.jpeg\ndata/2_processed/Coal/dataset1_Coal_072_164.jpeg\ndata/2_processed/Coal/dataset1_Coal_073_165.jpeg\ndata/2_processed/Coal/dataset1_Coal_074_166.jpeg\ndata/2_processed/Coal/dataset1_Coal_075_167.jpeg\ndata/2_processed/Coal/dataset1_Coal_076_168.jpeg\ndata/2_processed/Coal/dataset1_Coal_077_169.jpeg\ndata/2_processed/Coal/dataset1_Coal_079_170.jpeg\ndata/2_processed/Coal/dataset1_Coal_080_171.jpeg\ndata/2_processed/Coal/dataset1_Coal_081_172.jpeg\ndata/2_processed/Coal/dataset1_Coal_082_173.jpeg\ndata/2_processed/Coal/dataset1_Coal_083_174.jpeg\ndata/2_processed/Coal/dataset1_Coal_084_175.jpeg\ndata/2_processed/Coal/dataset1_Coal_085_176.jpeg\ndata/2_processed/Coal/dataset1_Coal_086_177.jpeg\ndata/2_processed/Coal/dataset1_Coal_087_178.jpeg\ndata/2_processed/Coal/dataset1_Coal_088_179.jpeg\ndata/2_processed/Coal/dataset1_Coal_090_180.jpeg\ndata/2_processed/Coal/dataset1_Coal_091_181.jpeg\ndata/2_processed/Granite/dataset1_Granite_017_23.jpeg\ndata/2_processed/Granite/dataset1_Granite_021_27.jpeg\ndata/2_processed/Granite/dataset1_Granite_029_34.JPEG\ndata/2_processed/Granite/dataset1_Granite_031_36.JPEG\ndata/2_processed/Granite/dataset1_Granite_036_40.JPEG\ndata/2_processed/Granite/dataset1_Granite_062_64.JPEG\ndata/2_processed/Granite/dataset1_Granite_072_73.JPEG\ndata/2_processed/Granite/dataset1_Granite_073_74.JPEG\ndata/2_processed/Granite/dataset1_Granite_074_75.JPEG\ndata/2_processed/Granite/dataset1_Granite_075_76.JPEG\ndata/2_processed/Granite/dataset1_Granite_076_77.JPEG\ndata/2_processed/Granite/dataset1_Granite_077_78.JPEG\ndata/2_processed/Granite/dataset1_Granite_078_79.JPEG\ndata/2_processed/Granite/dataset1_Granite_080_80.JPEG\ndata/2_processed/Granite/dataset1_Granite_081_81.JPEG\ndata/2_processed/Granite/dataset1_Granite_082_82.JPEG\ndata/2_processed/Granite/dataset1_Granite_083_83.JPEG\ndata/2_processed/Granite/dataset1_Granite_084_84.JPEG\ndata/2_processed/Granite/dataset1_Granite_085_85.JPEG\ndata/2_processed/Granite/dataset1_Granite_086_86.JPEG\ndata/2_processed/Granite/dataset1_Granite_092_91.JPEG\ndata/2_processed/Granite/dataset1_Granite_099_98.JPEG\ndata/2_processed/Limestone/dataset1_Limestone_004_100.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_005_101.webp\ndata/2_processed/Limestone/dataset1_Limestone_006_102.jfif\ndata/2_processed/Limestone/dataset1_Limestone_007_103.jfif\ndata/2_processed/Limestone/dataset1_Limestone_008_104.jfif\ndata/2_processed/Limestone/dataset1_Limestone_125_21.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_156_238.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_203_280.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_215_291.jfif\ndata/2_processed/Limestone/dataset1_Limestone_224_3.webp\ndata/2_processed/Limestone/dataset1_Limestone_271_38.jfif\ndata/2_processed/Limestone/dataset1_Limestone_306_7.jpeg\ndata/2_processed/Marble/dataset1_Marble_277_Marmo_z17.jfif\ndata/2_processed/Marble/dataset1_Marble_282_images.jfif\ndata/2_processed/Marble/dataset1_Marble_380_mineral-stone-marble-nonfoliated-metamorphic-260nw-349915676.webp\ndata/2_processed/Marble/dataset1_Marble_387_u3tqorgn31mp65iypkwy_7e2e86ad-6a3f-410e-9584-caa5b075ccd7_825x700.webp\ndata/2_processed/Quartzite/dataset1_Quartzite_471_quartzite-crystal-mineral-sample-studio-shot-with-black-background-972333846-5c7e6525c9e77c0001d19dda.webp\ndata/2_processed/Sandstone/dataset1_Sandstone_058_15.webp\ndata/2_processed/Sandstone/dataset1_Sandstone_248_320.webp"
  },
  {
    "objectID": "preprocess_data.html#functions",
    "href": "preprocess_data.html#functions",
    "title": "3) Preprocess Data",
    "section": "Functions",
    "text": "Functions\n\n\nprocess_data\n\n process_data ()\n\nDownload dataset, removes unsupported and corrupted images, and splits data into train, val and test."
  },
  {
    "objectID": "03_c_callbacks.html",
    "href": "03_c_callbacks.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "get_callbacks\n\n get_callbacks (cfg)\n\nReturn a Callback List.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nList\nCallbacks List\n\n\n\n\n\n\nCustomEarlyStopping\n\n CustomEarlyStopping (monitor='val_accuracy', value=0.6, min_epoch=15,\n                      verbose=0)\n\nStops training when epoch > min_epoch and monitor value < value.\n\n\n\nLRLogger\n\n LRLogger ()\n\nlog lr at the end of every epoch.\n\n\n\nget_reduce_lr_on_plateau\n\n get_reduce_lr_on_plateau (cfg)\n\nReturn tf.keras.callbacks.ReduceLROnPlateau.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntf.keras.callbacks.ReduceLROnPlateau\nReduce learning rate when a metric has stopped improving.\n\n\n\n\n\n\nget_earlystopper\n\n get_earlystopper (cfg)\n\nReturn tf.keras.callbacks.EarlyStopping.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntensorflow.keras.callbacks\nStop training when a monitored metric has stopped improving."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types."
  },
  {
    "objectID": "index.html#installation-training-steps",
    "href": "index.html#installation-training-steps",
    "title": "Whats-this-rock",
    "section": "Installation & Training Steps",
    "text": "Installation & Training Steps\n\nUse the Telegram Bot\nYou can try the bot here on Telegram.\nType /help to get instructions.\n\n\nDeploy Telegram Bot\npip install -r requirements-prod.txt\npython rock_classifier/bot.py\n\n\nTrain Model\nPaste your kaggle.json file in the root directory\nRun these commands\npip install -r requirements-dev.txt\nsh src/scripts/setup.sh\npython src/models/train.py\nYou can try different models and parameters by editing config.json.\nBy using Hydra it’s now much more easier to override parameters like this\npython src/models/train.py  wandb.project=Whats-this-rockv \\\n                            dataset_id=[1,2,3,4] \\\n                            epochs=50 \\\n                            backbone=resnet\n\n\n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweeps.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid\nwandb agent udaylunawat/Whats-this-rock/$sweepid"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n\n\n\n\n<style=‘font-size:37px’>Features added\n\n\n<style=‘font-size:37px’>Features planned\n\n\n\n\n\nWandb\nDatasets\n\n4 Datasets\n\nAugmentation\n\nkeras-cv\nRegular Augmentation\n\nSampling\n\nOversampling\nUndersampling\nClass weights\n\nRemove Corrupted Images\nTry Multiple Optimizers (Adam, RMSProp, AdamW, SGD)\nGenerators\n\nTFDS datasets\nImageDataGenerator\n\nModels\n\nConvNextTiny\nBaselineCNN\nEfficientnet\nResnet101\nMobileNetv1\nMobileNetv2\nXception\n\nLRScheduleer, LRDecay\n\nBaseline without scheduler\nStep decay\nCosine annealing\nClassic cosine annealing with bathc steps w/o restart\n\nModel Checkpoint, Resume Training\nEvaluation\n\nConfusion Matrix\nClassification Report\n\nDeploy Telegram Bot\n\nHeroku - Deprecated\nRailway\nShow CM and CL in bot\n\nDocker\nGitHub Actions\n\nDeploy Bot when bot.py is updated.\nLint code using GitHub super-linter\n\nConfiguration Management\n\nml-collections\nHydra\n\nPerformance improvement\n\nConvert to tf.data.Dataset\n\nLinting & Formatting\n\nBlack\nFlake8\nisort\npydocstyle\n\n\n\nDeploy to Huggingface spaces\nAccessing the model through FastAPI (Backend)\nStreamlit (Frontend)\nconvert models.py to Classes and more OOP style\nnbdev\nGroup Runs\nkfold cross validation\n\nWandB Tables\nfind the long tail examples or hard examples,\nfind the classes that the model is performing terribly on,\nAdd Badges\nLinting\nRailway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n├── imgs                              <- Images for skill banner, project banner and other images\n│\n├── configs                           <- Configuration files\n│   ├── configs.yaml                  <- config for single run\n│   └── sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n│\n├── data\n│   ├── corrupted_images              <- corrupted images will be moved to this directory\n│   ├── sample_images                 <- Sample images for inference\n│   ├── 0_raw                         <- The original, immutable data dump.\n│   ├── 1_external                    <- Data from third party sources.\n│   ├── 2_interim                     <- Intermediate data that has been transformed.\n│   └── 3_processed                   <- The final, canonical data sets for modeling.\n│\n├── notebooks                         <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                                        the creator's initials, and a short `-` delimited description, e.g.\n│                                        1.0-jqp-initial-data-exploration`.\n│\n│\n├── src                               <- Source code for use in this project.\n│   │\n│   ├── data                          <- Scripts to download or generate data\n│   │   ├── download.py\n│   │   ├── preprocess.py\n│   │   └── utils.py\n│   │\n│   ├── callbacks                     <- functions that are executed during training at given stages of the training procedure\n│   │   ├── custom_callbacks.py\n│   │   └── callbacks.py\n│   │\n│   ├── models                        <- Scripts to train models and then use trained models to make\n│   │   │                                predictions\n│   │   ├── evaluate.py\n│   │   ├── models.py\n│   │   ├── predict.py\n│   │   ├── train.py\n│   │   └── utils.py\n│   │\n│   └── scripts                       <- Scripts to setup dir structure and download datasets\n│   │   ├── clean_dir.sh\n│   │   ├── dataset1.sh\n│   │   ├── dataset2.sh\n│   │   ├── dataset3.sh\n│   │   ├── dataset4.sh\n│   │   └── setup.sh\n│.  │\n│   └── visualization                 <- Scripts for visualizations\n│\n├── .dockerignore                     <- Docker ignore\n├── .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n├── LICENSE                           <- Your project's license.\n├── Makefile                          <- Makefile with commands like `make data` or `make train`\n├── README.md                         <- The top-level README for developers using this project.\n├── requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n│                                        generated with `pip freeze > requirements.txt`\n└── setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn’t handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you’d like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nLinkFree is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset - by Mahmoud Alforawi"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ⭐️ from you. Don’t forget to leave a star ⭐️\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  },
  {
    "objectID": "a_download_utils.html",
    "href": "a_download_utils.html",
    "title": "2) Download utilities",
    "section": "",
    "text": "move_and_rename\n\n move_and_rename (class_dir:str)\n\nMove files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, …), which contains image files.\n\n\n\n\nType\nDetails\n\n\n\n\nclass_dir\nstr\ndescription\n\n\n\n\n\n\nmove_files\n\n move_files (src_dir:str, dest_dir:str='data/2_processed/tmp')\n\nMove files to tmp directory in 2_processed.\nsrc_dir: directory of rock subclass with files [Basalt, Marble, Coal, …]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\ndescription\n\n\ndest_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\n\n\nrename_files\n\n rename_files (source_dir:str='data/2_processed/tmp')\n\nRename files in classes and moves to 2_processed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\n\n\nget_tfds_from_dir\n\n get_tfds_from_dir (cfg)\n\nConvert directory of images to tfds dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\nprepare\n\n prepare (ds, cfg, shuffle=False, augment=False)\n\nPrepare dataset using augment, preprocess, cache, shuffle and prefetch.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\ntype\n\ndescription\n\n\ncfg\ncfg (omegaconf.DictConfig):\n\nHydra Configuration\n\n\nshuffle\nbool\nFalse\ndescription, by default False\n\n\naugment\nbool\nFalse\ndescription, by default False\n\n\nReturns\ntype\n\ndescription\n\n\n\n\n\n\nget_preprocess\n\n get_preprocess (cfg)\n\nReturn preprocess function for particular model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\nscalar\n\n scalar (img:<module'PIL.Image'from'/Users/uday/miniforge3/envs/rocks/lib/\n         python3.10/site-packages/PIL/Image.py'>)\n\nScale pixel between -1 and +1.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimg\nPIL.Image\nPIL Image\n\n\nReturns\nPIL.Image\nimagew with pixel values scaled between -1 and 1\n\n\n\n\n\n\nget_value_counts\n\n get_value_counts (dataset_path:str, column:str='file_type')\n\nGet value counts of passed column.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\nstr\n\ndirectory with subclasses\n\n\ncolumn\nstr\nfile_type\ncolumn name\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\nget_df\n\n get_df (root:str='data/2_processed')\n\nReturn df with classes, image paths and file names.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\ndata/2_processed\ndirectory to scan for image files, by default “data/2_processed”\n\n\nReturns\nDataFrame\n\nwith columns file_name, class and file_path\n\n\n\n\n\n\nget_dims\n\n get_dims (file:str)\n\nReturn dimenstions for an RBG image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nstr\nfile path for image\n\n\nReturns\nOptional\nreturns a tuple of heights and width of image or None\n\n\n\n\n\n\ntimer_func..wrap_func\n\n timer_func.<locals>.wrap_func (*args, **kwargs)\n\n\n\n\nremove_unsupported_images\n\n remove_unsupported_images (root_folder:str)\n\nRemove unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\nRoot Folder.\n\n\n\n\n\n\nfind_filepaths\n\n find_filepaths (root_folder:str)\n\nRecursively finds all files.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\ntimer_func\n\n timer_func (func)\n\nShow the execution time of the function object passed.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\ntype\ndescription\n\n\n\n\n\n\nsampling\n\n sampling (cfg)\n\nOversamples/Undersample/No Sampling data into train, val, test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\n\n\nmove_bad_files\n\n move_bad_files (txt_file, dest, text)\n\nMoves files in txt_file to dest.\n\n\n\n\nType\nDetails\n\n\n\n\ntxt_file\nfile\ntext file with path of bad images\n\n\ndest\ntype\ntarget destination\n\n\ntext\n\n\n\n\n\n\n\n\nclean_images\n\n clean_images (cfg)\n\nRemoves bad, misclassified, duplicate, corrupted and unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\n\n\nmove_to_processed\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\n\n\nget_new_name\n\n get_new_name (dir_list:list)\n\nReturn dict with old name and new name of files in multiple directories.\n{‘data/1_extracted/dataset1/Basalt/14.jpg’: ‘data/2_processed/Basalt/dataset1_01_Basalt_14.jpg’}\n\n\n\n\nType\nDetails\n\n\n\n\ndir_list\nlist\nlist of dir paths\n\n\nReturns\ndict\n{old_name: new_name}\n\n\n\n\n\n\nclean_data_dir\n\n clean_data_dir ()\n\nClean all data directories except 0_raw."
  }
]