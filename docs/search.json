[
  {
    "objectID": "d_train.html",
    "href": "d_train.html",
    "title": "Training",
    "section": "",
    "text": "source\n\nmain\n\n main ()\n\nRun Main function.\n\nsource\n\n\nevaluate\n\n evaluate (cfg:omegaconf.dictconfig.DictConfig,\n           model:keras.engine.training.Model, history:dict,\n           test_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n           labels:list)\n\nEvaluate the trained model on Test Dataset, log confusion matrix and classification report.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration.\n\n\nmodel\nModel\nTensorflow model.\n\n\nhistory\ndict\nHistory object.\n\n\ntest_dataset\nDatasetV2\nTest Dataset.\n\n\nlabels\nlist\nList of Labels.\n\n\n\n\nsource\n\n\ntrain\n\n train (cfg:omegaconf.dictconfig.DictConfig,\n        train_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        val_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        class_weights:dict)\n\nTrain the model and returns model and history.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration\n\n\ntrain_dataset\nDatasetV2\nTrain Dataset\n\n\nval_dataset\nDatasetV2\nValidation Dataset\n\n\nclass_weights\ndict\nClass weights dictionary\n\n\nReturns\nmodel and history\nmodel and history"
  },
  {
    "objectID": "b_train_models.html",
    "href": "b_train_models.html",
    "title": "Training models",
    "section": "",
    "text": "source\n\nget_model\n\n get_model (cfg)\n\nGet an image classifier with a CNN based backbone.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntensorflow.keras.Model\nModel object\n\n\n\n\nsource\n\n\nget_backbone\n\n get_backbone (cfg)\n\nGet backbone for the model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nkeras.api._v2.keras.models\nTensroflow Model"
  },
  {
    "objectID": "b_download.html",
    "href": "b_download.html",
    "title": "Download dataset",
    "section": "",
    "text": "Create directory structure\n\nand if data already exists, clean it!\n\n$ rocks_clean_data\n\n\nDownload and verify the data\n$ rocks_download_data\nDownloads Dataset1 and moves the extracted files to data/1_extracted/dataset1.\nDownloads Dataset2 and moves the extracted files to data/1_extracted/dataset2.\n\nsource\n\ndownload_and_move_datasets\n\n download_and_move_datasets ()\n\nRun the download and move datasets script."
  },
  {
    "objectID": "bot.html",
    "href": "bot.html",
    "title": "Telegram bot deployment",
    "section": "",
    "text": "source\n\nhandle_photo\n\n handle_photo (update, context)\n\n\nsource\n\n\nhandle_message\n\n handle_message (update, context)\n\n\nsource\n\n\nmodel_details\n\n model_details (update, context)\n\n\nsource\n\n\nhelp\n\n help (update, context)\n\n\nsource\n\n\nstart\n\n start (update, context)\n\n\nsource\n\n\ndeploy_bot\n\n deploy_bot ()"
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "source\n\nplot_confusion_matrix\n\n plot_confusion_matrix (labels, y, _y)"
  },
  {
    "objectID": "a_train_utils.html",
    "href": "a_train_utils.html",
    "title": "Training utilities",
    "section": "",
    "text": "source\n\nget_lr_scheduler\n\n get_lr_scheduler (cfg, lr)\n\nReturn A LearningRateSchedule.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\n\n\n\n\nReturns\nkeras.api._v2.keras.optimizers.schedules\nA LearningRateSchedule\n\n\n\n\nsource\n\n\nget_model_weights\n\n get_model_weights\n                    (train_ds:tensorflow.python.data.ops.dataset_ops.Datas\n                    etV2)\n\nReturn model weights dict.\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_ds\nDatasetV2\nTensorflow Dataset.\n\n\nReturns\ndict\nDictionary of class weights.\n\n\n\n/Users/uday/miniforge3/envs/rocks/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n\nsource\n\n\nget_optimizer\n\n get_optimizer (cfg, lr:str)\n\nGet optimizer set with an learning rate.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\nstr\nlearning rate\n\n\nReturns\nkeras.api._v2.keras.optimizers\nTensorflow optimizer"
  },
  {
    "objectID": "03_e_predict.html",
    "href": "03_e_predict.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "source\n\nmain\n\n main ()\n\n\nsource\n\n\nget_prediction\n\n get_prediction (file)\n\nGet prediction for image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nFile\nImage file\n\n\nReturns\nstr\nPrediction with class name and confidence %.\n\n\n\n\nsource\n\n\nload_model\n\n load_model ()\n\n\nsource\n\n\npreprocess_image\n\n preprocess_image (file, image_size)\n\nDecode and resize image.\n\n\n\n\nType\nDetails\n\n\n\n\nfile\ntype\ndescription\n\n\nimage_size\ntype\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nget_run_data\n\n get_run_data ()\n\nGet data for a wandb sweep."
  },
  {
    "objectID": "preprocess_data.html",
    "href": "preprocess_data.html",
    "title": "Preprocess Data",
    "section": "",
    "text": "This is the description & steps of the preprocess_data function which combines the following functions\n\n\nClear data directory, Download and move datasets.\nRename and move files to data/2_processed.\nList files other than jpg and png, to remove unsupported files.\nList file by types before cleaning.\nRemove\n\nBad Images\nDuplicate Images\nMisclassified Images\nUnsupported Images\nCorrupted Images\n\nList file by types after cleaning.\nGet count of files by class types.\nHandle Imbalance using Undersampling, Oversampling.\n\n\n\n\nAlso clears the data directory if files already exists.\n\n\nsource\n\n\n\n download_and_move_datasets ()\n\nRun the download and move datasets script.\n\n\nFunction 'run_scripts' executed in 4.3805s\n\n\n\n\n\n\n\nsource\n\n\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\n\nMoving files from Basalt and Basalt to data/2_processed...\nMoving files from Coal and Coal to data/2_processed...\nMoving files from Granite and Granite to data/2_processed...\nMoving files from Limestone and Limestone to data/2_processed...\nMoving files from Marble and Marble to data/2_processed...\nMoving files from Quartzite and Quartzite to data/2_processed...\nMoving files from Sandstone and Sandstone to data/2_processed...\n\n\n\n\n\n\n\n\nCode\nprint(\"\\nFiles other than jpg and png.\\n\")\nfiles, _ = find_filepaths('data/2_processed/')\nprint('\\n'.join(list(filter(lambda x: not x.endswith('jpg') and not x.endswith('png'), files))))\n\n\n\nFiles other than jpg and png.\n\ndata/2_processed/Coal/dataset1_Coal_025_12.jpeg\ndata/2_processed/Coal/dataset1_Coal_070_162.jpeg\ndata/2_processed/Coal/dataset1_Coal_071_163.jpeg\ndata/2_processed/Coal/dataset1_Coal_072_164.jpeg\ndata/2_processed/Coal/dataset1_Coal_073_165.jpeg\ndata/2_processed/Coal/dataset1_Coal_074_166.jpeg\ndata/2_processed/Coal/dataset1_Coal_075_167.jpeg\ndata/2_processed/Coal/dataset1_Coal_076_168.jpeg\ndata/2_processed/Coal/dataset1_Coal_077_169.jpeg\ndata/2_processed/Coal/dataset1_Coal_079_170.jpeg\ndata/2_processed/Coal/dataset1_Coal_080_171.jpeg\ndata/2_processed/Coal/dataset1_Coal_081_172.jpeg\ndata/2_processed/Coal/dataset1_Coal_082_173.jpeg\ndata/2_processed/Coal/dataset1_Coal_083_174.jpeg\ndata/2_processed/Coal/dataset1_Coal_084_175.jpeg\ndata/2_processed/Coal/dataset1_Coal_085_176.jpeg\ndata/2_processed/Coal/dataset1_Coal_086_177.jpeg\ndata/2_processed/Coal/dataset1_Coal_087_178.jpeg\ndata/2_processed/Coal/dataset1_Coal_088_179.jpeg\ndata/2_processed/Coal/dataset1_Coal_090_180.jpeg\ndata/2_processed/Coal/dataset1_Coal_091_181.jpeg\ndata/2_processed/Granite/dataset1_Granite_017_23.jpeg\ndata/2_processed/Granite/dataset1_Granite_021_27.jpeg\ndata/2_processed/Granite/dataset1_Granite_029_34.JPEG\ndata/2_processed/Granite/dataset1_Granite_031_36.JPEG\ndata/2_processed/Granite/dataset1_Granite_036_40.JPEG\ndata/2_processed/Granite/dataset1_Granite_062_64.JPEG\ndata/2_processed/Granite/dataset1_Granite_072_73.JPEG\ndata/2_processed/Granite/dataset1_Granite_073_74.JPEG\ndata/2_processed/Granite/dataset1_Granite_074_75.JPEG\ndata/2_processed/Granite/dataset1_Granite_075_76.JPEG\ndata/2_processed/Granite/dataset1_Granite_076_77.JPEG\ndata/2_processed/Granite/dataset1_Granite_077_78.JPEG\ndata/2_processed/Granite/dataset1_Granite_078_79.JPEG\ndata/2_processed/Granite/dataset1_Granite_080_80.JPEG\ndata/2_processed/Granite/dataset1_Granite_081_81.JPEG\ndata/2_processed/Granite/dataset1_Granite_082_82.JPEG\ndata/2_processed/Granite/dataset1_Granite_083_83.JPEG\ndata/2_processed/Granite/dataset1_Granite_084_84.JPEG\ndata/2_processed/Granite/dataset1_Granite_085_85.JPEG\ndata/2_processed/Granite/dataset1_Granite_086_86.JPEG\ndata/2_processed/Granite/dataset1_Granite_092_91.JPEG\ndata/2_processed/Granite/dataset1_Granite_099_98.JPEG\ndata/2_processed/Limestone/dataset1_Limestone_004_100.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_005_101.webp\ndata/2_processed/Limestone/dataset1_Limestone_006_102.jfif\ndata/2_processed/Limestone/dataset1_Limestone_007_103.jfif\ndata/2_processed/Limestone/dataset1_Limestone_008_104.jfif\ndata/2_processed/Limestone/dataset1_Limestone_125_21.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_156_238.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_203_280.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_215_291.jfif\ndata/2_processed/Limestone/dataset1_Limestone_224_3.webp\ndata/2_processed/Limestone/dataset1_Limestone_271_38.jfif\ndata/2_processed/Limestone/dataset1_Limestone_306_7.jpeg\ndata/2_processed/Marble/dataset1_Marble_277_Marmo_z17.jfif\ndata/2_processed/Marble/dataset1_Marble_282_images.jfif\ndata/2_processed/Marble/dataset1_Marble_380_mineral-stone-marble-nonfoliated-metamorphic-260nw-349915676.webp\ndata/2_processed/Marble/dataset1_Marble_387_u3tqorgn31mp65iypkwy_7e2e86ad-6a3f-410e-9584-caa5b075ccd7_825x700.webp\ndata/2_processed/Quartzite/dataset1_Quartzite_471_quartzite-crystal-mineral-sample-studio-shot-with-black-background-972333846-5c7e6525c9e77c0001d19dda.webp\ndata/2_processed/Sandstone/dataset1_Sandstone_058_15.webp\ndata/2_processed/Sandstone/dataset1_Sandstone_248_320.webp\n\n\n\n\n\n\n\n\nFile types before cleaning:\n.jpg     2550\n.jpeg      28\n.JPEG      20\n.png       17\n.jfif       7\n.webp       7\nName: file_type, dtype: int64\n\n\n\n\n\n- Bad Images\n- Duplicate Images\n- Misclassified Images\n- Unsupported Images\n- Corrupted Images\n\nsource\n\n\n\n clean_images (cfg)\n\nRemoves bad, misclassified, duplicate, corrupted and unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\n\nMoving bad images...\n\nMoved 658 images to data/bad_images.\nMoving misclassified images...\n\nMoved 3 images to data/misclassified_images.\nMoving duplicate images...\n\nMoved 89 images to data/duplicate_images.\n\nRemoving unsupported images...\nRemoved 15 unsupported files.\n\nRemoving corrupted images...\nRemoved 4 bad images from Limestone.\nRemoved 6 bad images from Basalt.\nRemoved 0 bad images from Marble.\n\n\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\nRemoved 16 bad images from Granite.\nRemoved 1 bad images from Quartzite.\nRemoved 1 bad images from Sandstone.\nRemoved 5 bad images from Coal.\nFunction 'remove_corrupted_images' executed in 9.1468s\n\n\n\n\n\n\n\n\n\nFile types after cleaning:\n.jpg     1797\n.jpeg      23\n.png       10\n.JPEG       2\nName: file_type, dtype: int64\n\n\n\n\n\n\n\n\nCounts of classes:\n\nCoal         414\nQuartzite    373\nLimestone    326\nSandstone    299\nMarble       204\nGranite      121\nBasalt        95\nName: class, dtype: int64\n\n\n\n\n\nUsing Undersampling, Oversampling and No Sampling.\n\nsource\n\n\n\n sampling (cfg)\n\nOversamples/Undersample/No Sampling data into train, val, test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\n\n\nSplitting files in Train, Validation and Test and saving to data/3_tfds_dataset/\nData Split:- Training 0.70, Validation 0.15, Test 0.15\nSampling type:- No Sampling.\n\n\nCopying files: 1832 files [00:00, 2089.81 files/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprocess_data wraps all the above functions.\n\nsource\n\n\n\n process_data ()\n\nDownload dataset, removes unsupported and corrupted images, and splits data into train, val and test."
  },
  {
    "objectID": "c_callbacks.html",
    "href": "c_callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "source\n\nget_callbacks\n\n get_callbacks (cfg)\n\nReturn a Callback List.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nList\nCallbacks List\n\n\n\n\nsource\n\n\nCustomEarlyStopping\n\n CustomEarlyStopping (monitor='val_accuracy', value=0.6, min_epoch=15,\n                      verbose=0)\n\nStops training when epoch > min_epoch and monitor value < value.\n\nsource\n\n\nLRLogger\n\n LRLogger ()\n\nlog lr at the end of every epoch.\n\nsource\n\n\nget_reduce_lr_on_plateau\n\n get_reduce_lr_on_plateau (cfg)\n\nReturn tf.keras.callbacks.ReduceLROnPlateau.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntf.keras.callbacks.ReduceLROnPlateau\nReduce learning rate when a metric has stopped improving.\n\n\n\n\nsource\n\n\nget_earlystopper\n\n get_earlystopper (cfg)\n\nReturn tf.keras.callbacks.EarlyStopping.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntensorflow.keras.callbacks\nStop training when a monitored metric has stopped improving."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types."
  },
  {
    "objectID": "index.html#installation-training-steps",
    "href": "index.html#installation-training-steps",
    "title": "Whats-this-rock",
    "section": "Installation & Training Steps",
    "text": "Installation & Training Steps\n\nUse the Telegram Bot\nYou can try the bot here on Telegram.\nType /help to get instructions.\n\n\nDeploy Telegram Bot\npip install -r requirements-prod.txt\npython rock_classifier/bot.py\n\n\nTrain Model\nPaste your kaggle.json file in the root directory\nRun these commands\npip install -r requirements-dev.txt\nsh src/scripts/setup.sh\npython src/models/train.py\nYou can try different models and parameters by editing config.json.\nBy using Hydra it’s now much more easier to override parameters like this\npython src/models/train.py  wandb.project=Whats-this-rockv \\\n                            dataset_id=[1,2,3,4] \\\n                            epochs=50 \\\n                            backbone=resnet\n\n\n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweeps.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid\nwandb agent udaylunawat/Whats-this-rock/$sweepid"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n\n\n\n\n<style=‘font-size:37px’>Features added\n\n\n<style=‘font-size:37px’>Features planned\n\n\n\n\n\nWandb\nDatasets\n\n4 Datasets\n\nAugmentation\n\nkeras-cv\nRegular Augmentation\n\nSampling\n\nOversampling\nUndersampling\nClass weights\n\nRemove Corrupted Images\nTry Multiple Optimizers (Adam, RMSProp, AdamW, SGD)\nGenerators\n\nTFDS datasets\nImageDataGenerator\n\nModels\n\nConvNextTiny\nBaselineCNN\nEfficientnet\nResnet101\nMobileNetv1\nMobileNetv2\nXception\n\nLRScheduleer, LRDecay\n\nBaseline without scheduler\nStep decay\nCosine annealing\nClassic cosine annealing with bathc steps w/o restart\n\nModel Checkpoint, Resume Training\nEvaluation\n\nConfusion Matrix\nClassification Report\n\nDeploy Telegram Bot\n\nHeroku - Deprecated\nRailway\nShow CM and CL in bot\n\nDocker\nGitHub Actions\n\nDeploy Bot when bot.py is updated.\nLint code using GitHub super-linter\n\nConfiguration Management\n\nml-collections\nHydra\n\nPerformance improvement\n\nConvert to tf.data.Dataset\n\nLinting & Formatting\n\nBlack\nFlake8\nisort\npydocstyle\n\n\n\nDeploy to Huggingface spaces\nAccessing the model through FastAPI (Backend)\nStreamlit (Frontend)\nconvert models.py to Classes and more OOP style\nnbdev\nGroup Runs\nkfold cross validation\n\nWandB Tables\nfind the long tail examples or hard examples,\nfind the classes that the model is performing terribly on,\nAdd Badges\nLinting\nRailway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n├── imgs                              <- Images for skill banner, project banner and other images\n│\n├── configs                           <- Configuration files\n│   ├── configs.yaml                  <- config for single run\n│   └── sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n│\n├── data\n│   ├── corrupted_images              <- corrupted images will be moved to this directory\n│   ├── sample_images                 <- Sample images for inference\n│   ├── 0_raw                         <- The original, immutable data dump.\n│   ├── 1_external                    <- Data from third party sources.\n│   ├── 2_interim                     <- Intermediate data that has been transformed.\n│   └── 3_processed                   <- The final, canonical data sets for modeling.\n│\n├── notebooks                         <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                                        the creator's initials, and a short `-` delimited description, e.g.\n│                                        1.0-jqp-initial-data-exploration`.\n│\n│\n├── src                               <- Source code for use in this project.\n│   │\n│   ├── data                          <- Scripts to download or generate data\n│   │   ├── download.py\n│   │   ├── preprocess.py\n│   │   └── utils.py\n│   │\n│   ├── callbacks                     <- functions that are executed during training at given stages of the training procedure\n│   │   ├── custom_callbacks.py\n│   │   └── callbacks.py\n│   │\n│   ├── models                        <- Scripts to train models and then use trained models to make\n│   │   │                                predictions\n│   │   ├── evaluate.py\n│   │   ├── models.py\n│   │   ├── predict.py\n│   │   ├── train.py\n│   │   └── utils.py\n│   │\n│   └── scripts                       <- Scripts to setup dir structure and download datasets\n│   │   ├── clean_dir.sh\n│   │   ├── dataset1.sh\n│   │   ├── dataset2.sh\n│   │   ├── dataset3.sh\n│   │   ├── dataset4.sh\n│   │   └── setup.sh\n│.  │\n│   └── visualization                 <- Scripts for visualizations\n│\n├── .dockerignore                     <- Docker ignore\n├── .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n├── LICENSE                           <- Your project's license.\n├── Makefile                          <- Makefile with commands like `make data` or `make train`\n├── README.md                         <- The top-level README for developers using this project.\n├── requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n│                                        generated with `pip freeze > requirements.txt`\n└── setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn’t handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you’d like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nLinkFree is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset - by Mahmoud Alforawi"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ⭐️ from you. Don’t forget to leave a star ⭐️\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  },
  {
    "objectID": "a_download_utils.html",
    "href": "a_download_utils.html",
    "title": "Download utilities",
    "section": "",
    "text": "source\n\nclean_data_dir\n\n clean_data_dir ()\n\nClean all data directories except 0_raw.\n\nclean_data_dir()\n\n\nsource\n\n\nmove_and_rename\n\n move_and_rename (class_dir:str)\n\nMove files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, …), which contains image files.\n\n\n\n\nType\nDetails\n\n\n\n\nclass_dir\nstr\ndescription\n\n\n\n\nsource\n\n\nmove_files\n\n move_files (src_dir:str, dest_dir:str='data/2_processed/tmp')\n\nMove files to tmp directory in 2_processed.\nsrc_dir: directory of rock subclass with files [Basalt, Marble, Coal, …]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\ndescription\n\n\ndest_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\nsource\n\n\nrename_files\n\n rename_files (source_dir:str='data/2_processed/tmp')\n\nRename files in classes and moves to 2_processed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\nsource\n\n\nget_tfds_from_dir\n\n get_tfds_from_dir (cfg)\n\nConvert directory of images to tfds dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nprepare\n\n prepare (ds, cfg, shuffle=False, augment=False)\n\nPrepare dataset using augment, preprocess, cache, shuffle and prefetch.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\ntype\n\ndescription\n\n\ncfg\ncfg (omegaconf.DictConfig):\n\nHydra Configuration\n\n\nshuffle\nbool\nFalse\ndescription, by default False\n\n\naugment\nbool\nFalse\ndescription, by default False\n\n\nReturns\ntype\n\ndescription\n\n\n\n\nsource\n\n\nget_preprocess\n\n get_preprocess (cfg)\n\nReturn preprocess function for particular model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nscalar\n\n scalar (img:<module'PIL.Image'from'/Users/uday/miniforge3/envs/rocks/lib/\n         python3.10/site-packages/PIL/Image.py'>)\n\nScale pixel between -1 and +1.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimg\nPIL.Image\nPIL Image\n\n\nReturns\nPIL.Image\nimagew with pixel values scaled between -1 and 1\n\n\n\n\nsource\n\n\nget_value_counts\n\n get_value_counts (dataset_path:str, column:str='file_type')\n\nGet value counts of passed column.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\nstr\n\ndirectory with subclasses\n\n\ncolumn\nstr\nfile_type\ncolumn name\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nget_df\n\n get_df (root:str='data/2_processed')\n\nReturn df with classes, image paths and file names.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\ndata/2_processed\ndirectory to scan for image files, by default “data/2_processed”\n\n\nReturns\nDataFrame\n\nwith columns file_name, class and file_path\n\n\n\n\nsource\n\n\nget_dims\n\n get_dims (file:str)\n\nReturn dimenstions for an RBG image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nstr\nfile path for image\n\n\nReturns\nOptional\nreturns a tuple of heights and width of image or None\n\n\n\n\n\n\ntimer_func..wrap_func\n\n timer_func.<locals>.wrap_func (*args, **kwargs)\n\n\nsource\n\n\nremove_unsupported_images\n\n remove_unsupported_images (root_folder:str)\n\nRemove unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\nRoot Folder.\n\n\n\n\nsource\n\n\nfind_filepaths\n\n find_filepaths (root_folder:str)\n\nRecursively finds all files.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\ntimer_func\n\n timer_func (func)\n\nShow the execution time of the function object passed.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\ntype\ndescription\n\n\n\n\nsource\n\n\nsampling\n\n sampling (cfg)\n\nOversamples/Undersample/No Sampling data into train, val, test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\nsource\n\n\nmove_bad_files\n\n move_bad_files (txt_file, dest, text)\n\nMoves files in txt_file to dest.\n\n\n\n\nType\nDetails\n\n\n\n\ntxt_file\nfile\ntext file with path of bad images\n\n\ndest\ntype\ntarget destination\n\n\ntext\n\n\n\n\n\n\nsource\n\n\nclean_images\n\n clean_images (cfg)\n\nRemoves bad, misclassified, duplicate, corrupted and unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\nsource\n\n\nmove_to_processed\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\nsource\n\n\nget_new_name\n\n get_new_name (dir_list:list)\n\nReturn dict with old name and new name of files in multiple directories.\n{‘data/1_extracted/dataset1/Basalt/14.jpg’: ‘data/2_processed/Basalt/dataset1_01_Basalt_14.jpg’}\n\n\n\n\nType\nDetails\n\n\n\n\ndir_list\nlist\nlist of dir paths\n\n\nReturns\ndict\n{old_name: new_name}"
  }
]