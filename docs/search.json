[
  {
    "objectID": "d_train.html",
    "href": "d_train.html",
    "title": "Training",
    "section": "",
    "text": "source\n\ntrain\n\n train (cfg:omegaconf.dictconfig.DictConfig,\n        train_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        val_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        class_weights:dict)\n\nTrain the model and returns model and history.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration\n\n\ntrain_dataset\nDatasetV2\nTrain Dataset\n\n\nval_dataset\nDatasetV2\nValidation Dataset\n\n\nclass_weights\ndict\nClass weights dictionary\n\n\nReturns\nmodel and history\nmodel and history\n\n\n\n\nsource\n\n\nevaluate\n\n evaluate (cfg:omegaconf.dictconfig.DictConfig,\n           model:keras.engine.training.Model, history:dict,\n           test_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n           labels:list)\n\nEvaluate the trained model on Test Dataset, log confusion matrix and classification report.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration.\n\n\nmodel\nModel\nTensorflow model.\n\n\nhistory\ndict\nHistory object.\n\n\ntest_dataset\nDatasetV2\nTest Dataset.\n\n\nlabels\nlist\nList of Labels.\n\n\n\n\nsource\n\n\nmain\n\n main (cfg)\n\nRun Main function.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration\n\n\nReturns\nNone"
  },
  {
    "objectID": "b_train_models.html",
    "href": "b_train_models.html",
    "title": "Training models",
    "section": "",
    "text": "source\n\nget_model\n\n get_model (cfg)\n\nGet an image classifier with a CNN based backbone.\nCalls get_backbone and adds a top_model layer to it.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntensorflow.keras.Model\nModel object\n\n\n\n/Users/uday/miniforge3/envs/rocks/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n\nsource\n\n\nget_backbone\n\n get_backbone (cfg)\n\nGet backbone for the model.\nList of supported models.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nkeras.api._v2.keras.models\nTensroflow Model"
  },
  {
    "objectID": "b_download.html",
    "href": "b_download.html",
    "title": "Download dataset",
    "section": "",
    "text": "Create directory structure\n\nand if data already exists, clean it!\n\n$ rocks_clean_data\n\n\nDownload and verify the data\n$ rocks_download_data\nDownloads Dataset1 and moves the extracted files to data/1_extracted/dataset1.\nDownloads Dataset2 and moves the extracted files to data/1_extracted/dataset2.\n\nsource\n\ndownload_and_move_datasets\n\n download_and_move_datasets ()\n\nRun the download and move datasets script."
  },
  {
    "objectID": "bot.html",
    "href": "bot.html",
    "title": "Telegram bot deployment",
    "section": "",
    "text": "source\n\nhandle_photo\n\n handle_photo (update, context)\n\n\nsource\n\n\nhandle_message\n\n handle_message (update, context)\n\n\nsource\n\n\nmodel_details\n\n model_details (update, context)\n\n\nsource\n\n\nhelp\n\n help (update, context)\n\n\nsource\n\n\nstart\n\n start (update, context)\n\n\nsource\n\n\ndeploy_bot\n\n deploy_bot ()"
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "Whats this rock!",
    "section": "",
    "text": "source\n\nplotly_confusion_matrix\n\n plotly_confusion_matrix (labels, y, _y)\n\n\nsource\n\n\nget_classification_report\n\n get_classification_report (true_categories, predicted_categories, labels)\n\nPrint Classification Report, and return formatted report to log to wandb\n\n\n\n\nType\nDetails\n\n\n\n\ntrue_categories\nList\nactuual categories\n\n\npredicted_categories\nList\npredicted categories\n\n\nlabels\nList\nlabels\n\n\nReturns\ndict\nClassification report dict\n\n\n\n\nsource\n\n\nget_confusion_matrix\n\n get_confusion_matrix (true_categories, predicted_categories, labels)\n\nCreate, plot and save confusion matrix.\n\n\n\n\nType\nDetails\n\n\n\n\ntrue_categories\nList\nActual categories\n\n\npredicted_categories\nList\nPredicted categories\n\n\nlabels\nList\nlabels\n\n\nReturns\narray\nConfusion matrix array"
  },
  {
    "objectID": "a_train_utils.html",
    "href": "a_train_utils.html",
    "title": "Training utilities",
    "section": "",
    "text": "source\n\nget_lr_scheduler\n\n get_lr_scheduler (cfg, lr)\n\nReturn A LearningRateSchedule.\nSupports cosine_decay, exponentialdecay and cosine_decay_restarts.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\nstr\nlearning rate\n\n\nReturns\nkeras.api._v2.keras.optimizers.schedules\nA LearningRateSchedule\n\n\n\n\nsource\n\n\nget_model_weights\n\n get_model_weights\n                    (train_ds:tensorflow.python.data.ops.dataset_ops.Datas\n                    etV2)\n\nReturn model weights dict.\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_ds\nDatasetV2\nTensorflow Dataset.\n\n\nReturns\ndict\nDictionary of class weights.\n\n\n\n/Users/uday/miniforge3/envs/rocks/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n\nsource\n\n\nget_optimizer\n\n get_optimizer (cfg, lr:str)\n\nGet optimizer set with an learning rate.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\nstr\nlearning rate\n\n\nReturns\nkeras.api._v2.keras.optimizers\nTensorflow optimizer"
  },
  {
    "objectID": "a_preprocess_data.html#rename-and-move-files-to-data2_processed.",
    "href": "a_preprocess_data.html#rename-and-move-files-to-data2_processed.",
    "title": "Preprocess Data",
    "section": "1. Rename and move files to data/2_processed.",
    "text": "1. Rename and move files to data/2_processed.\n\nsource\n\nmove_to_processed\n\n move_to_processed ()\n\nCombine files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\n\nMoving files from dataset1/Basalt and dataset2/Basalt to data/2_processed/Basalt ...\nMoving files from dataset1/Coal and dataset2/Coal to data/2_processed/Coal ...\nMoving files from dataset1/Granite and dataset2/Granite to data/2_processed/Granite ...\nMoving files from dataset1/Limestone and dataset2/Limestone to data/2_processed/Limestone ...\nMoving files from dataset1/Marble and dataset2/Marble to data/2_processed/Marble ...\nMoving files from dataset1/Quartzite and dataset2/Quartzite to data/2_processed/Quartzite ...\nMoving files from dataset1/Sandstone and dataset2/Sandstone to data/2_processed/Sandstone ..."
  },
  {
    "objectID": "a_preprocess_data.html#list-files-other-than-jpg-and-png-to-remove-unsupported-files.",
    "href": "a_preprocess_data.html#list-files-other-than-jpg-and-png-to-remove-unsupported-files.",
    "title": "Preprocess Data",
    "section": "2. List files other than jpg and png, to remove unsupported files.",
    "text": "2. List files other than jpg and png, to remove unsupported files.\n\n\nCode\nprint(\"\\nFiles other than jpg and png.\\n\")\nfiles, _ = find_filepaths(\"data/2_processed/\")\nprint(\n    \"\\n\".join(\n        list(filter(lambda x: not x.endswith(\"jpg\") and not x.endswith(\"png\"), files))\n    )\n)\n\n\n\nFiles other than jpg and png.\n\ndata/2_processed/Coal/dataset1_Coal_025_12.jpeg\ndata/2_processed/Coal/dataset1_Coal_070_162.jpeg\ndata/2_processed/Coal/dataset1_Coal_071_163.jpeg\ndata/2_processed/Coal/dataset1_Coal_072_164.jpeg\ndata/2_processed/Coal/dataset1_Coal_073_165.jpeg\ndata/2_processed/Coal/dataset1_Coal_074_166.jpeg\ndata/2_processed/Coal/dataset1_Coal_075_167.jpeg\ndata/2_processed/Coal/dataset1_Coal_076_168.jpeg\ndata/2_processed/Coal/dataset1_Coal_077_169.jpeg\ndata/2_processed/Coal/dataset1_Coal_079_170.jpeg\ndata/2_processed/Coal/dataset1_Coal_080_171.jpeg\ndata/2_processed/Coal/dataset1_Coal_081_172.jpeg\ndata/2_processed/Coal/dataset1_Coal_082_173.jpeg\ndata/2_processed/Coal/dataset1_Coal_083_174.jpeg\ndata/2_processed/Coal/dataset1_Coal_084_175.jpeg\ndata/2_processed/Coal/dataset1_Coal_085_176.jpeg\ndata/2_processed/Coal/dataset1_Coal_086_177.jpeg\ndata/2_processed/Coal/dataset1_Coal_087_178.jpeg\ndata/2_processed/Coal/dataset1_Coal_088_179.jpeg\ndata/2_processed/Coal/dataset1_Coal_090_180.jpeg\ndata/2_processed/Coal/dataset1_Coal_091_181.jpeg\ndata/2_processed/Granite/dataset1_Granite_017_23.jpeg\ndata/2_processed/Granite/dataset1_Granite_021_27.jpeg\ndata/2_processed/Granite/dataset1_Granite_029_34.JPEG\ndata/2_processed/Granite/dataset1_Granite_031_36.JPEG\ndata/2_processed/Granite/dataset1_Granite_036_40.JPEG\ndata/2_processed/Granite/dataset1_Granite_062_64.JPEG\ndata/2_processed/Granite/dataset1_Granite_072_73.JPEG\ndata/2_processed/Granite/dataset1_Granite_073_74.JPEG\ndata/2_processed/Granite/dataset1_Granite_074_75.JPEG\ndata/2_processed/Granite/dataset1_Granite_075_76.JPEG\ndata/2_processed/Granite/dataset1_Granite_076_77.JPEG\ndata/2_processed/Granite/dataset1_Granite_077_78.JPEG\ndata/2_processed/Granite/dataset1_Granite_078_79.JPEG\ndata/2_processed/Granite/dataset1_Granite_080_80.JPEG\ndata/2_processed/Granite/dataset1_Granite_081_81.JPEG\ndata/2_processed/Granite/dataset1_Granite_082_82.JPEG\ndata/2_processed/Granite/dataset1_Granite_083_83.JPEG\ndata/2_processed/Granite/dataset1_Granite_084_84.JPEG\ndata/2_processed/Granite/dataset1_Granite_085_85.JPEG\ndata/2_processed/Granite/dataset1_Granite_086_86.JPEG\ndata/2_processed/Granite/dataset1_Granite_092_91.JPEG\ndata/2_processed/Granite/dataset1_Granite_099_98.JPEG\ndata/2_processed/Limestone/dataset1_Limestone_004_100.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_005_101.webp\ndata/2_processed/Limestone/dataset1_Limestone_006_102.jfif\ndata/2_processed/Limestone/dataset1_Limestone_007_103.jfif\ndata/2_processed/Limestone/dataset1_Limestone_008_104.jfif\ndata/2_processed/Limestone/dataset1_Limestone_125_21.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_156_238.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_203_280.jpeg\ndata/2_processed/Limestone/dataset1_Limestone_215_291.jfif\ndata/2_processed/Limestone/dataset1_Limestone_224_3.webp\ndata/2_processed/Limestone/dataset1_Limestone_271_38.jfif\ndata/2_processed/Limestone/dataset1_Limestone_306_7.jpeg\ndata/2_processed/Marble/dataset1_Marble_277_Marmo_z17.jfif\ndata/2_processed/Marble/dataset1_Marble_282_images.jfif\ndata/2_processed/Marble/dataset1_Marble_380_mineral-stone-marble-nonfoliated-metamorphic-260nw-349915676.webp\ndata/2_processed/Marble/dataset1_Marble_387_u3tqorgn31mp65iypkwy_7e2e86ad-6a3f-410e-9584-caa5b075ccd7_825x700.webp\ndata/2_processed/Quartzite/dataset1_Quartzite_471_quartzite-crystal-mineral-sample-studio-shot-with-black-background-972333846-5c7e6525c9e77c0001d19dda.webp\ndata/2_processed/Sandstone/dataset1_Sandstone_058_15.webp\ndata/2_processed/Sandstone/dataset1_Sandstone_248_320.webp"
  },
  {
    "objectID": "a_preprocess_data.html#list-file-by-types-before-cleaning.",
    "href": "a_preprocess_data.html#list-file-by-types-before-cleaning.",
    "title": "Preprocess Data",
    "section": "3. List file by types before cleaning.",
    "text": "3. List file by types before cleaning.\n\n\n\nFile types before cleaning:\n.jpg     2550\n.jpeg      28\n.JPEG      20\n.png       17\n.jfif       7\n.webp       7\nName: file_type, dtype: int64"
  },
  {
    "objectID": "a_preprocess_data.html#remove",
    "href": "a_preprocess_data.html#remove",
    "title": "Preprocess Data",
    "section": "4. Remove",
    "text": "4. Remove\n- Bad Images\n- Duplicate Images\n- Misclassified Images\n- Unsupported Images\n- Corrupted Images\n\nsource\n\nclean_images\n\n clean_images (cfg)\n\nRemove bad, misclassified, duplicate, corrupted and unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration"
  },
  {
    "objectID": "a_preprocess_data.html#list-file-by-types-after-cleaning.",
    "href": "a_preprocess_data.html#list-file-by-types-after-cleaning.",
    "title": "Preprocess Data",
    "section": "5. List file by types after cleaning.",
    "text": "5. List file by types after cleaning.\n\n\n\nFile types after cleaning:\n.jpg     2550\n.jpeg      28\n.JPEG      20\n.png       17\n.jfif       7\n.webp       7\nName: file_type, dtype: int64"
  },
  {
    "objectID": "a_preprocess_data.html#get-count-of-files-by-class-types.",
    "href": "a_preprocess_data.html#get-count-of-files-by-class-types.",
    "title": "Preprocess Data",
    "section": "6. Get count of files by class types.",
    "text": "6. Get count of files by class types.\n\n\n\nCounts of classes:\n\nQuartzite    517\nCoal         469\nLimestone    452\nMarble       427\nSandstone    370\nGranite      214\nBasalt       180\nName: class, dtype: int64"
  },
  {
    "objectID": "a_preprocess_data.html#handle-imbalance",
    "href": "a_preprocess_data.html#handle-imbalance",
    "title": "Preprocess Data",
    "section": "7. Handle Imbalance",
    "text": "7. Handle Imbalance\nUsing Undersampling, Oversampling and No Sampling.\n\nsource\n\nsampling\n\n sampling (cfg)\n\nOversamples/Undersample/No Sampling data into train, val, test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration"
  },
  {
    "objectID": "a_preprocess_data.html#putting-it-all-together",
    "href": "a_preprocess_data.html#putting-it-all-together",
    "title": "Preprocess Data",
    "section": "Putting it all together",
    "text": "Putting it all together\nprocess_data wraps all the above functions.\n\nsource\n\nprocess_data\n\n process_data (cfg)\n\nDownload dataset, removes unsupported and corrupted images, and splits data into train, val and test.\nSteps -> download_and_move_datasets -> move_to_processed -> ‘find_filepaths’ -> clean_images -> sampling\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration"
  },
  {
    "objectID": "b_eda.html",
    "href": "b_eda.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Show code\ndf = get_df(\"data/2_processed\")\ndf[\"dimensions\"] = df[\"file_path\"].apply(lambda x: get_dims(x))\ndf[\"image_width\"] = df[\"dimensions\"].apply(lambda x: x[0] if x is not None else None)\ndf[\"image_height\"] = df[\"dimensions\"].apply(lambda x: x[1] if x is not None else None)\ndf[\"pixels\"] = df[\"image_width\"] * df[\"image_height\"]\ndf[\"corrupt_status\"] = df[\"file_path\"].apply(lambda x: check_corrupted(x))\ndf.head(5)\n\n\n\n\n\n\n  \n    \n      \n      file_name\n      class\n      file_path\n      file_type\n      dimensions\n      image_width\n      image_height\n      pixels\n      corrupt_status\n    \n  \n  \n    \n      0\n      dataset1_Limestone_147_23.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (285, 380)\n      285.0\n      380.0\n      108300.0\n      False\n    \n    \n      1\n      dataset2_Limestone_418_Limestone521.jpg\n      Limestone\n      data/2_processed/Limestone/dataset2_Limestone_...\n      .jpg\n      (225, 225)\n      225.0\n      225.0\n      50625.0\n      False\n    \n    \n      2\n      dataset1_Limestone_315_78.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (408, 612)\n      408.0\n      612.0\n      249696.0\n      False\n    \n    \n      3\n      dataset1_Limestone_078_168.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (408, 612)\n      408.0\n      612.0\n      249696.0\n      False\n    \n    \n      4\n      dataset1_Limestone_305_69.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (408, 612)\n      408.0\n      612.0\n      249696.0\n      False\n    \n  \n\n\n\n\n\n\n\n\nCode\ndf[\"file_name\"].apply(lambda x: x.split(\".\")[-1]).value_counts()\n\n\njpg     1797\njpeg      23\npng        8\nJPEG       2\nName: file_name, dtype: int64\n\n\n\n\n\n\n\nCode\ndf.corrupt_status.value_counts()\n\n\nFalse    1753\nTrue       77\nName: corrupt_status, dtype: int64\n\n\nCorrupted file list\n\n\nCode\ndf[df[\"corrupt_status\"] == True].head(5)\n\n\n\n\n\n\n  \n    \n      \n      file_name\n      class\n      file_path\n      file_type\n      dimensions\n      image_width\n      image_height\n      pixels\n      corrupt_status\n    \n  \n  \n    \n      8\n      dataset1_Limestone_225_30.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (450, 900)\n      450.0\n      900.0\n      405000.0\n      True\n    \n    \n      10\n      dataset1_Limestone_265_336.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (600, 600)\n      600.0\n      600.0\n      360000.0\n      True\n    \n    \n      21\n      dataset1_Limestone_257_329.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (1182, 1587)\n      1182.0\n      1587.0\n      1875834.0\n      True\n    \n    \n      28\n      dataset1_Limestone_258_33.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (533, 800)\n      533.0\n      800.0\n      426400.0\n      True\n    \n    \n      31\n      dataset1_Limestone_198_276.jpg\n      Limestone\n      data/2_processed/Limestone/dataset1_Limestone_...\n      .jpg\n      (533, 800)\n      533.0\n      800.0\n      426400.0\n      True\n    \n  \n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\n\nclass_names = df[\"class\"].value_counts().keys()\ncounts = df[\"class\"].value_counts().values\n\ncount_df = pd.DataFrame(list(zip(class_names, counts)), columns=[\"class\", \"count\"])\n\nsns.set_theme(style=\"darkgrid\")\nax = sns.barplot(y=\"class\", x=\"count\", data=count_df)\n\n\n\n\n\nCount of rock types.\n\n\n\n\n\n\n\n\n\n\nCode\nwidth_list = df.image_width\nheight_list = df.image_height\naverage_width = sum(width_list) / len(width_list)\naverage_height = sum(height_list) / len(height_list)\n\n# print('average width: {} and height: {}'.format(average_width, average_height))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8))\n\nsns.histplot(width_list, ax=ax[0])\nax[0].set_title(\"Image width\")\nsns.histplot(height_list, ax=ax[1])\nax[1].set_title(\"Image height\");\n\n\n\n\n\n\n\nCode\n# plot histograms to show the distribution of width and height values\nfig, axs = plt.subplots(1, 2, figsize=(15, 7))\naxs[0].hist(df.image_width.values, bins=20, color=\"#91bd3a\")\naxs[0].set_title(\"Width distribution\")\n# axs[0].set_xlim(1000, 3000)\n\naxs[1].hist(df.image_height.values, bins=20, color=\"#91bd3a\")\naxs[1].set_title(\"Height distribution\")\n# axs[1].set_xlim(1000, 3000)\n\nplt.suptitle(\"Image Dimensions\")\nplt.show()\n\n\n\n\n\nWidth and Height distribution.\n\n\n\n\n\n\n\n\n\nSampling type:- None.\n\n\n\n\n\n\nCode\nget_df(\"data/3_tfds_dataset/train\")[\"class\"].value_counts()\n\n\nCoal         289\nQuartzite    261\nLimestone    228\nSandstone    209\nMarble       142\nGranite       84\nBasalt        66\nName: class, dtype: int64\n\n\n\n\n\n\n\nCode\nget_df(\"data/3_tfds_dataset/val\")[\"class\"].value_counts()\n\n\nCoal         62\nQuartzite    55\nLimestone    48\nSandstone    44\nMarble       30\nGranite      18\nBasalt       14\nName: class, dtype: int64\n\n\n\n\n\n\n\nCode\nget_df(\"data/3_tfds_dataset/test\")[\"class\"].value_counts()\n\n\nCoal         63\nQuartzite    57\nLimestone    50\nSandstone    46\nMarble       32\nGranite      19\nBasalt       15\nName: class, dtype: int64"
  },
  {
    "objectID": "b_eda.html#sample-images",
    "href": "b_eda.html#sample-images",
    "title": "Exploratory Analysis",
    "section": "Sample Images",
    "text": "Sample Images\n\n\nCode\nds = builder.as_dataset(split=\"train\", shuffle_files=True)\n# tfds.show_examples(ds, builder.info)\n\n\n\nSamples before Augmentation\n\n\nCode\ntrain_dataset = load_dataset()\nvisualize_dataset(train_dataset, title=\"Before Augmentation\");\n\n\n\n\n\n\n\nSamples after RandAugment\n\n\nCode\ntrain_dataset = load_dataset().map(apply_rand_augment, num_parallel_calls=AUTOTUNE)\nvisualize_dataset(train_dataset, title=\"After RandAugment\")\n\n\n\n\n\n\n\nSamples after cutmix and mixup augmentation\n\n\nCode\ntrain_dataset = load_dataset().map(cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE)\nvisualize_dataset(train_dataset, title=\"After cut_mix and mix_up\")"
  },
  {
    "objectID": "b_eda.html#data-augmentation-using-fastai",
    "href": "b_eda.html#data-augmentation-using-fastai",
    "title": "Exploratory Analysis",
    "section": "Data Augmentation using FastAI",
    "text": "Data Augmentation using FastAI\n\nMixUp\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      00:00\n    \n  \n\n\n\n\n\n\n\n\nCutMix\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      00:00"
  },
  {
    "objectID": "03_e_predict.html",
    "href": "03_e_predict.html",
    "title": "Whats this rock!",
    "section": "",
    "text": "source\n\nmain\n\n main ()\n\n\nsource\n\n\nget_prediction\n\n get_prediction (file, cfg, class_names)\n\nGet prediction for image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nFile\nImage file\n\n\ncfg\n\n\n\n\nclass_names\n\n\n\n\nReturns\nstr\nPrediction with class name and confidence %.\n\n\n\n\nsource\n\n\nload_model\n\n load_model (cfg)\n\n\nsource\n\n\npreprocess_image\n\n preprocess_image (file, image_size)\n\nDecode and resize image.\n\n\n\n\nType\nDetails\n\n\n\n\nfile\ntype\ndescription\n\n\nimage_size\ntype\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nget_run_data\n\n get_run_data ()\n\nGet data for a wandb sweep."
  },
  {
    "objectID": "c_callbacks.html",
    "href": "c_callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "source\n\nget_callbacks\n\n get_callbacks (cfg)\n\nReturn a Callback List.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nList\nCallbacks List\n\n\n\n\nsource\n\n\nCustomEarlyStopping\n\n CustomEarlyStopping (monitor='val_accuracy', value=0.6, min_epoch=15,\n                      verbose=0)\n\nStops training when epoch > min_epoch and monitor value < default value.\n\nsource\n\n\nLRLogger\n\n LRLogger ()\n\nlog lr at the end of every epoch.\n\nsource\n\n\nget_reduce_lr_on_plateau\n\n get_reduce_lr_on_plateau (cfg)\n\nReturn tf.keras.callbacks.ReduceLROnPlateau.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntf.keras.callbacks.ReduceLROnPlateau\nReduce learning rate when a metric has stopped improving.\n\n\n\n\nsource\n\n\nget_earlystopper\n\n get_earlystopper (cfg)\n\nReturn tf.keras.callbacks.EarlyStopping.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntensorflow.keras.callbacks\nStop training when a monitored metric has stopped improving."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types.\nThis package uses tensorflow to accelerate deep learning experimentation.\nMLOps workflow like\nwas all done using Weights & Biases\nAdditionally, nbdev was used to"
  },
  {
    "objectID": "index.html#inspiration",
    "href": "index.html#inspiration",
    "title": "Whats-this-rock",
    "section": "Inspiration",
    "text": "Inspiration\n\nThe common complaint that you need massive amounts of data to do deep learning  can be a very long way from the truth!\n\n\nYou very often don’t need much data at all, a lot of people are looking for ways to share data and aggregate data, but that’s unnecessary.They assume they need more data than they do, cause they’re not familiar with the basics of transfer learning which is this critical technique for needing orders of magnitudes less data.\n\n\nJeremy Howards"
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Whats-this-rock",
    "section": "Documentation",
    "text": "Documentation\nDocumentation for the project has been created using nbdev, and is available at udaylunawat.github.io/Whats-this-rock.\nnbdev is a notebook-driven development platform. Simply write notebooks with lightweight markup and get high-quality documentation, tests, continuous integration, and packaging for free!\nOnce I discovered nbdev, I couldn’t help myself but redo the whole project from scratch.\nIt’s just makes me 10x more productive and makes the whole process streamlined and more enjoyable."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Whats-this-rock",
    "section": "Installation",
    "text": "Installation\nYou can directly install using pip\npip install rocks_classifier\nInstall - Directly from Github (latest beta version)\npip install git+https://github.com/udaylunawat/Whats-this-rock.git\n\nDownload and process data\n%%bash\nrocks_process_data  --config-dir configs \\\n                    remove_bad= True \\\n                    remove_misclassified= True \\\n                    remove_duplicates= True \\\n                    remove_corrupted= True \\\n                    remove_unsupported= True \\\n                    sampling=None \\\n                    train_split=0.8 \\\n\n\nTrain Model\nTrain model using default parameters in configs/config.yaml.\nrocks_train_model   --config-dir configs\nYou can try different models and parameters by editing configs/config.yaml, or you can override it by passing arguments like this:-\nBy using Hydra it’s now much more easier to override parameters like this:-\nrocks_train_model   --config-dir configs \\\n                    wandb.project=Whats-this-rock \\\n                    wandb.mode=offline \\\n                    wandb.use=False \\\n                    dataset_id=[1,2] \\\n                    epochs=30 \\\n                    lr=0.005 \\\n                    augmentation=None \\\n                    monitor=val_loss \\\n                    loss=categorical_crossentropy \\\n                    backbone=resnet \\\n                    lr_schedule=cosine_decay_restarts \\\n                    lr_decay_steps=300 \\\n                    trainable=False \\\n\n  \n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweep.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid, run it to start running sweeps!\nwandb agent udaylunawat/Whats-this-rock/$sweepid\n\n\nTelegram Bot\nYou can try the bot here on Telegram.\n\nType /help to get instructions in chat.\n\nOr deploy it yourself\nrocks_deploy_bot"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nColab\nGitHub\nDownload\n\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n& Things I’ve Experimented with\n\n\n\nFeature\n\nFeature\n\n\n\n\n\nWandb\n- Experiment Tracking- System Tracking- Model Tracking- Hyperparameter Tuning\nDatasets\n- Dataset 1- Dataset 2\n\n\nAugmentation\n- Keras-CV- MixUp- CutMix- Normal\nModels\n- ConvNextTiny- Efficientnet- Resnet101- MobileNetv1- MobileNetv2- Xception\n\n\nOptimisers\n- Adam- Adamax- SGD- RMSProp\nLR Scheduler\n- CosineDecay- ExponentialDecay- CosineDecayRestarts\n\n\nRemove Images\n- Duplicate Images- Corrupt Images- Bad Images- Misclassified\nConfiguration Management\n- hydra- ml-collections- argparse-google-fire\n\n\nGenerators\n- tf.data.DataSet- ImageDataGenerator\nDeployment\n- Heroku- Railway\n\n\nEvaluation\n- Classification Report- Confusion Matrix\nGitHub Actions (CICD)\n- GitHub Super Linter- Deploy to Telegram- Deploy to Railway- nbdev tests CI- GitHub Pages(Documentation)\n\n\nLinting\n- Flake8- Pydocstyle\nTelegram Bot\n- Greet- Info- Predict Image\n\n\nFormatting\n- Black- yapf\nDocumentation\n- Code Description- Code comments- Source link- Doclinks\n\n\nBadges\n- Build- Issues- Lint Codebase\nDocker\n\n\n\nPublishing\n- PyPi"
  },
  {
    "objectID": "index.html#planned-features",
    "href": "index.html#planned-features",
    "title": "Whats-this-rock",
    "section": "Planned Features",
    "text": "Planned Features\n\n\n\nFeature\n\nFeature\n\n\n\n\n\nDeploy\n- HuggingFaces\nBackend\n- FastAPI\n\n\nCoding Style\n- Object Oriented\nFrontend\n- Streamlit\n\n\nWandB\n- Group Runs- WandB Tables\nBadges\n- Railway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n├── imgs                              <- Images for skill banner, project banner and other images\n│\n├── configs                           <- Configuration files\n│   ├── configs.yaml                  <- config for single run\n│   └── sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n│\n├── data\n│   ├── corrupted_images              <- corrupted images will be moved to this directory\n│   ├── misclassified_images          <- misclassified images will be moved to this directory\n│   ├── bad_images                    <- Bad images will be moved to this directory\n│   ├── duplicate_images              <- Duplicate images will be moved to this directory\n│   ├── sample_images                 <- Sample images for inference\n│   ├── 0_raw                         <- The original, immutable data dump.\n│   ├── 1_extracted                   <- Extracted data.\n│   ├── 2_processed                   <- Intermediate data that has been transformed.\n│   └── 3_tfds_dataset                <- The final, canonical data sets for modeling.\n│\n├── notebooks                         <- Jupyter notebooks. Used to create the source code.\n│\n├── rocks_classifier                  <- Source code used in this project.\n│   │\n│   ├── data                          <- Scripts to download or generate data\n│   │   ├── download.py\n│   │   ├── preprocess.py\n│   │   └── utils.py\n│   │\n│   ├── callbacks                     <- functions that are executed during training at given stages of the training procedure\n│   │   └── callbacks.py\n│   │\n│   ├── models                        <- Scripts to train models and then use trained models to make\n│   │   │                                predictions\n│   │   ├── evaluate.py\n│   │   ├── models.py\n│   │   ├── predict.py\n│   │   ├── train.py\n│   │   └── utils.py\n│   │\n│   └── visualization                 <- Scripts for visualizations\n│\n├── .dockerignore                     <- Docker ignore\n├── .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n├── LICENSE                           <- Your project's license.\n├── README.md                         <- The top-level README for developers using this project.\n├── CHANGELOG.md                      <- Release changes.\n├── CODE_OF_CONDUCT.md                <- Code of conduct.\n├── CONTRIBUTING.md                   <- Contributing Guidelines.\n├── settings.ini                      <- configuration.\n├── README.md                         <- The top-level README for developers using this project.\n├── requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n│                                        generated with `pip freeze > requirements.txt`\n└── setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn’t handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you’d like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nWhats-this-rock! is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset 1 - by Mahmoud Alforawi\nDataset 2 - by salmaneunus\nnbdev inspiration - tmabraham"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ⭐️ from you. Don’t forget to leave a star ⭐️\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "0) Template",
    "section": "",
    "text": "from nbdev import nbdev_export\n\nnbdev_export()"
  },
  {
    "objectID": "a_download_utils.html",
    "href": "a_download_utils.html",
    "title": "Download utilities",
    "section": "",
    "text": "source\n\nclean_data_dir\n\n clean_data_dir ()\n\nClean all data directories except 0_raw.\n\nsource\n\n\ncopy_configs_tocwd\n\n copy_configs_tocwd ()\n\nCopy configs directory from package to current directory.\n\nsource\n\n\nfind_filepaths\n\n find_filepaths (root_folder:str)\n\nRecursively finds all files.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\ndirectory\n\n\nReturns\nTuple\nsorted filepaths and length of filepaths\n\n\n\n\nsource\n\n\nget_new_name\n\n get_new_name (dir_list:list)\n\nReturn dict with old name and new name of files in multiple directories.\n{‘data/1_extracted/dataset1/Basalt/14.jpg’: ‘data/2_processed/Basalt/dataset1_01_Basalt_14.jpg’}\n\n\n\n\nType\nDetails\n\n\n\n\ndir_list\nlist\nlist of dir paths\n\n\nReturns\ndict\n{old_name: new_name}\n\n\n\n\nsource\n\n\nmove_to_processed\n\n move_to_processed ()\n\nCombine files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\nsource\n\n\nmove_bad_files\n\n move_bad_files (txt_file, dest, text)\n\nMove files in txt_file to dest.\n\n\n\n\nType\nDetails\n\n\n\n\ntxt_file\nfile\ntext file with path of bad images\n\n\ndest\ntype\ntarget destination\n\n\ntext\n\n\n\n\n\n\nsource\n\n\ntimer_func\n\n timer_func (func)\n\nShow the execution time of the function object passed.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\nfunction\nfunction\n\n\n\n\n\n\ntimer_func..wrap_func\n\n timer_func.<locals>.wrap_func (*args, **kwargs)\n\n\nsource\n\n\nremove_unsupported_images\n\n remove_unsupported_images (root_folder:str)\n\nRemove unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\nRoot Folder.\n\n\n\n\nsource\n\n\nclean_images\n\n clean_images (cfg)\n\nRemove bad, misclassified, duplicate, corrupted and unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\nsource\n\n\nmove_and_rename\n\n move_and_rename (class_dir:str)\n\nMove files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, …), which contains image files.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nclass_dir\nstr\nClass directory that contains folders of classes containing images\n\n\n\n\nsource\n\n\nmove_files\n\n move_files (src_dir:str, dest_dir:str='data/2_processed/tmp')\n\nMove files to tmp directory in 2_processed.\nsrc_dir: directory of rock subclass with files [Basalt, Marble, Coal, …]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\nSource Directory path\n\n\ndest_dir\nstr\ndata/2_processed/tmp\nDestination Directory path, by default “data/2_processed/tmp”\n\n\n\n\nsource\n\n\nrename_files\n\n rename_files (source_dir:str='data/2_processed/tmp')\n\nRename files in classes and moves to 2_processed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_dir\nstr\ndata/2_processed/tmp\nDirectory, by default “data/2_processed/tmp”\n\n\n\n\nsource\n\n\nget_tfds_from_dir\n\n get_tfds_from_dir (cfg)\n\nConvert directory of images to tfds dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nTuple\nTuple containing 3 Tensorflow Datasets\n\n\n\n\nsource\n\n\nprepare\n\n prepare (ds, cfg, shuffle=False, augment=False)\n\nPrepare dataset using augment, preprocess, cache, shuffle and prefetch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\nDataset\n\nTensorflow Dataset\n\n\ncfg\ncfg (omegaconf.DictConfig):\n\nHydra Configuration\n\n\nshuffle\nbool\nFalse\nshuffle parameter, by default False\n\n\naugment\nbool\nFalse\naugment parameter, by default False\n\n\nReturns\nDataset\n\nTensorflow Dataset preprocessed, shuffled, augmented and batched\n\n\n\n\nsource\n\n\nget_preprocess\n\n get_preprocess (cfg)\n\nReturn preprocess function for particular model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nget_value_counts\n\n get_value_counts (dataset_path:str, column:str='file_type')\n\nGet value counts of passed column.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\nstr\n\ndirectory with subclasses\n\n\ncolumn\nstr\nfile_type\ncolumn name\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nget_df\n\n get_df (root:str='data/2_processed')\n\nReturn df with classes, image paths and file names.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\ndata/2_processed\ndirectory to scan for image files, by default “data/2_processed”\n\n\nReturns\nDataFrame\n\nwith columns file_name, class and file_path\n\n\n\n\nsource\n\n\nsampling\n\n sampling (cfg)\n\nOversamples/Undersample/No Sampling data into train, val, test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration"
  }
]