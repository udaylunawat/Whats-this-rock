[
  {
    "objectID": "sweeps.html",
    "href": "sweeps.html",
    "title": "Hyperparameter Tuning using Wandb Sweeps",
    "section": "",
    "text": "#@title Wandb mode\nimport os\nmode = 'online' #@param {type:\"string\"}\nos.environ['WANDB_MODE'] = mode\n\nif 'WANDB_API_KEY' not in os.environ:\n    if os.environ['WANDB_MODE'] == 'online':\n        from getpass import getpass\n        secret = getpass('Enter WandB API Key: ')\n        os.environ['WANDB_API_KEY'] = secret\n    else:\n        print(\"WandB Offline!\")\n\n\n#@title Setup\n!rm -rf /content/Whats-this-rock/\n!git clone https://github.com/udaylunawat/Whats-this-rock.git\n# !git clone -b hydra https://github.com/udaylunawat/Whats-this-rock.git\n\n\n!nvidia-smi\n!sh src/scripts/setup.sh\n\n\n!git pull\n\n\nmethod: bayes\nmetric:\n  goal: minimize\n  name: val_loss\nparameters:\n  wandb.use:\n    value: True\n  wandb.mode:\n    value: online\n  wandb.project:\n    value: Whats-this-rockv18\n  notes:\n    value: \"\"\n  seed:\n    values: [1]\n  lr:\n    values: [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5]\n  lr_decay_steps:\n    distribution: uniform\n    min: 10\n    max: 10000\n  lr_schedule:\n    values:\n      - cosine_decay_restarts\n  epochs:\n    value: 75\n  data_path:\n    value: data/3_tfds_dataset/\n  dataset_id:\n    values:\n      - [1, 2, 3, 4]\n  augmentation:\n    values: [kerascv]\n  class_weights:\n    values: [True, False]\n  optimizer:\n    values: [adam]\n  loss:\n    values: [categorical_crossentropy]\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  train_split:\n    values:\n      - 0.75\n  image_size:\n    value: 224\n  image_channels:\n    value: 3\n  sampling:\n    values: [None]\n  backbone:\n    values: [resnet]\n  use_pretrained_weights:\n    values: [True]\n  trainable:\n    values: [False, True]\n  last_layers:\n    distribution: int_uniform\n    min: 1\n    max: 50\n  custom_callback:\n    values: [False]\n  preprocess:\n    values: [True, False]\n  dropout_rate:\n    values: [0.3, 0.5]\n  monitor:\n    values: [\"val_loss\"]\n  earlystopping.use:\n    values: [False]\n  earlystopping.patience:\n    values: [10]\n  reduce_lr.use:\n    values: [False]\n  reduce_lr.factor:\n    values: [.9]\n  reduce_lr.patience:\n    values: [1]\n  reduce_lr.min_lr:\n    values: [1e-6]\n  save_model:\n    value: False\n\nprogram: src/models/train.py\ncommand:\n  - ${env}\n  - python\n  - ${program}\n  - ${args_no_hyphens}\n\n\n!wandb sweep \\\n    --project Whats-this-rockv17 \\\n    --entity udaylunawat \\\n    configs/sweep.yaml\n\n\n!wandb agent udaylunawat/Whats-this-rockv17/xrjlfpy0"
  },
  {
    "objectID": "test_fastai.html",
    "href": "test_fastai.html",
    "title": "FastAI",
    "section": "",
    "text": "!rm -rf /content/Whats-this-rock/\n# !git clone https://github.com/udaylunawat/Whats-this-rock.git\n!git clone -b nbdev https://github.com/udaylunawat/Whats-this-rock.git"
  },
  {
    "objectID": "test_fastai.html#downloading-data-and-moving-bad-corrupted-and-duplicate-images",
    "href": "test_fastai.html#downloading-data-and-moving-bad-corrupted-and-duplicate-images",
    "title": "FastAI",
    "section": "Downloading data and moving bad, corrupted and duplicate images",
    "text": "Downloading data and moving bad, corrupted and duplicate images\n\nDownloading data\n\n!rm -rf data/bad_images/\n\n\ndel get_data, process_data\n\n\nimport logging\n\n# STEP 1\n# create a logger object instance\nlogger = logging.getLogger()\n\n# STEP 2\n# specifies the lowest severity for logging\nlogger.setLevel(logging.NOTSET)\n\n# STEP 3\n# set a destination for your logs or a \"handler\"\n# here, we choose to print on console (a consoler handler)\nconsole_handler = logging.StreamHandler()\n\n# STEP 4\n# set the logging format for your handler\nlog_format = '%(asctime)s | %(levelname)s: %(message)s'\nconsole_handler.setFormatter(logging.Formatter(log_format))\n\n# finally, we add the handler to the logger\nlogger.addHandler(console_handler)\n\n# the second handler is a file handler\nfile_handler = logging.FileHandler('sample.log')\nfile_handler.setLevel(logging.INFO)\nfile_handler_format = '%(asctime)s | %(levelname)s | %(lineno)d: %(message)s'\nfile_handler.setFormatter(logging.Formatter(file_handler_format))\nlogger.addHandler(file_handler)\n\n\nimport omegaconf\nimport subprocess\nfrom src.data.download import get_data\nfrom src.data.preprocess import process_data\n\ncfg = omegaconf.OmegaConf.load('configs/config.yaml')\ncfg.dataset_id = [1,2,3,4]\n\nsubprocess.run([\"sh\", \"src/scripts/clean_dir.sh\"], stdout=subprocess.PIPE).stdout.decode(\"utf-8\")\nget_data()\nprocess_data(cfg)\n\n\n\nRemove duplicate images\n\n# https://github.com/saifjamsheer/google-dataset-creator/tree/c458ec1f2e676f474527cbf3ee2da3937d260d0c\nfrom imutils import paths\nfrom pathlib import Path\nimport os\nimport cv2\nimport argparse\nfrom src.data.utils import timer_func, find_filepaths\n\n\n\"\"\"\nScript that deletes duplicate images from a dataset. This\ncan be used in conjunction with 'create.py' to delete\nany duplicate images before manual pruning is done. \n\"\"\"\ndef dhash(image, hashSize=8):\n    # Convert the image to grayscale and resize it\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    resized = cv2.resize(gray, (hashSize + 1, hashSize))\n    # Compute the horizontal gradient between adjacent pixels\n    diff = resized[:, 1:] > resized[:, :-1]\n    # Converts the image to a hash and returns it\n    hash = sum([2**i for (i,j) in enumerate(diff.flatten()) if j])\n    return hash\n\n\n@timer_func\ndef remove_duplicates(dir):\n    # Path of the dataset of images\n    output_path = dir\n    hashes = dict()\n\n    # List of paths of the images in the datasets\n    image_paths = list(paths.list_images(output_path))\n\n    for image_path in image_paths:\n        # Load input image\n        \n        image = cv2.imread(image_path)\n        # Compute the hash of the image\n        try:\n            h = dhash(image)\n        except:\n            print(f'cv2.imread {image_path} Returns None!')\n        # Store all images in a dictionary of hashes\n        path = hashes.get(h, [])\n        path.append(image_path)\n        hashes[h] = path\n\n    # Iterate through the hashes\n    for h, path_value in hashes.items():\n        # Checking if a duplicate exists\n        if len(path_value) > 1:\n            # Deleting all duplicates\n            for path in path_value[1:]:\n                print(\"[INFO] Deleting: {}\".format(path))\n                # os.remove(path)\n    return list({k: v for (k, v) in hashes.items() if len(v) > 1}.values())\n\n\nduplicates = remove_duplicates('data/2_processed')\n\n\n!pip install imagededup\n\n\nfrom imagededup.methods import PHash\nimport random\n\ndef find_duplicates(path):\n    phasher = PHash()\n    encodings = phasher.encode_images(image_dir=path)\n    duplicates = phasher.find_duplicates(encoding_map=encodings)\n    duplicates = {k: v for (k, v) in duplicates.items() if len(v) > 0}\n    \n    from imagededup.utils import plot_duplicates\n    plot_duplicates(image_dir=path,\n                    duplicate_map=duplicates,\n                    filename=random.choice(list(duplicates.values()))[0]);\n\nfind_duplicates('data/2_processed/Coal')\n\n\n\nRemoving bad images\n\n# Python program to store list to file using pickle module\nimport pickle\n\n# write list to binary file\ndef write_list(names, filename='delfile'):\n    # store list in binary file so 'wb' mode\n    with open(filename, 'wb') as fp:\n        pickle.dump(names, fp)\n        print('Done writing list into a binary file')\n\n# Read list to memory\ndef read_list(filename='delfile'):\n    # for reading also binary mode is important\n    with open(filename, 'rb') as fp:\n        n_list = pickle.load(fp)\n        return n_list\n\n# del_list = read_list()\n# print('List is', del_list)\n\n\nbad_images = read_list('updated_delete_pickle')\n\n\nimport os\nimport shutil\nos.makedirs('data/bad_images', exist_ok=True)\n\nfor path in bad_images:\n    if os.path.exists(path):\n        shutil.move(str(path), 'data/bad_images')\n\n\n# assert len(os.listdir('data/bad_images')) == len(bad_images)"
  },
  {
    "objectID": "test_fastai.html#loading-data",
    "href": "test_fastai.html#loading-data",
    "title": "FastAI",
    "section": "Loading data",
    "text": "Loading data\n\nfrom fastai.vision.all import *\n\n\ndls = ImageDataLoaders.from_folder(path='data/2_processed/', valid_pct=0.2,\n                                   item_tfms=Resize(224))\ndls.valid_ds.items[:3]\n\n\nprint(f'Training set samples:- {len(dls.train_ds)} images.\\nValidation set samples:- {len(dls.valid_ds)} images.')\n\n\ndls.show_batch()\n\n\nlearn = vision_learner(dls, resnet34, metrics=[error_rate, F1Score(average='macro')])\n\n\nlearn.fine_tune(5, cbs=ShowGraphCallback())\n\nLowest error rate:- 0.2460, val_loss:- 0.93\n\n# save model\nlearn.export(file='model.pkl')\n\nlearn = load_learner('model.pkl')\n\n\nlearn.predict('data/4_tfds_dataset/test/Basalt/Basalt_104.jpg')\n\n\ndef get_results(learn):\n    print(f'\\nSample Results:')\n    learn.show_results()\n    plt.show()\n\n    interp = Interpretation.from_learner(learn)\n    print(f'\\nTop losses:-')\n    interp.plot_top_losses(9, figsize=(15,10))\n    plt.show()\n\n    interp = ClassificationInterpretation.from_learner(learn)\n    losses,idxs = interp.top_losses()\n    len(dls.valid_ds)==len(losses)==len(idxs)\n\n    print(f'\\nConfusion Matrix:-')\n    interp.plot_confusion_matrix(figsize=(7,7))\n    plt.show()\n\n    print(f'\\nNormalized Confusion Matrix:-')\n    interp.plot_confusion_matrix(figsize=(7,7), normalize=True)\n    plt.show()\n\n\nget_results(learn)"
  },
  {
    "objectID": "test_fastai.html#data-cleaning",
    "href": "test_fastai.html#data-cleaning",
    "title": "FastAI",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nRemove bad images\n\nfiles_to_delete = []\n\n\nfrom fastai.vision.widgets import *\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\nfor idx in cleaner.delete(): files_to_delete.append(cleaner.fns[idx])\n\n\ndef tfds_to_processed(filepath):\n    base = '/'.join(str(filepath).split('/')[-2:]).replace('\\')','')\n    processed = os.path.join('data/2_processed', base)\n    return Path(set(processed))\n\nnew_paths = list(map(lambda x:tfds_to_processed(x), files_to_delete))\n\n\n# update delfile\npaths = read_list()\nwrite_list(paths+new_paths, 'delete_file_pickle')\n\n\nlatest_del = paths+new_paths\n\n\ntry: \n    for idx in paths:\n        idx.unlink()\nexcept: pass\n\nwrite_list(paths, filename='delfile')\n\n\n!rm -rf data/4_tfds_dataset\n\n\nimport splitfolders\nsplitfolders.ratio(\n            \"data/2_processed\",\n            output=\"data/4_tfds_dataset\",\n            ratio=(0.70, 0.15, 0.15),\n            seed=42,\n            move=False,\n        )\n\nTraining after data cleaning\n\ndls = ImageDataLoaders.from_folder(path='data/4_tfds_dataset/',\n                                   train='train',\n                                   valid='val',\n                                   item_tfms=Resize(224))\ndls.valid_ds.items[:3]\n\n\nprint(f\"Number of images after deletetion:- \")\nprint(f'Training set samples:- {len(dls.train_ds)} images.\\nValidation set samples:- {len(dls.valid_ds)} images.')\n\n\ndls.show_batch()\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(5, cbs=ShowGraphCallback())"
  },
  {
    "objectID": "test_fastai.html#using-fastai-data-augmentation",
    "href": "test_fastai.html#using-fastai-data-augmentation",
    "title": "FastAI",
    "section": "Using fastai Data Augmentation",
    "text": "Using fastai Data Augmentation\n\ndls = ImageDataLoaders.from_folder(path='data/4_tfds_dataset/', train='train',\n                                   valid='val', item_tfms=Resize(460),\n                                   batch_tfms=aug_transforms(size=224))\ndls.valid_ds.items[:3]\n\n\ndls.show_batch()\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n\nlearn.lr_find()\n\nUsing optimal lr\n\nlearn.fine_tune(5, 3e-3)\n\n\nget_results(learn)"
  },
  {
    "objectID": "deploy_to_telegram.html",
    "href": "deploy_to_telegram.html",
    "title": "Deploy Telegram Bot",
    "section": "",
    "text": "pip install -r requirements.txt --quiet\n\n\n!python src/bot.py"
  },
  {
    "objectID": "b_data_utils.html",
    "href": "b_data_utils.html",
    "title": "Data utils",
    "section": "",
    "text": "move_and_rename\n\n move_and_rename (class_dir:str)\n\nMove files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, …), which contains image files.\n\n\n\n\nType\nDetails\n\n\n\n\nclass_dir\nstr\ndescription\n\n\n\n\n\n\nmove_files\n\n move_files (src_dir:str, dest_dir:str='data/2_processed/tmp')\n\nMove files to tmp directory in 2_processed.\nsrc_dir: directory of rock subclass with files [Basalt, Marble, Coal, …]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\ndescription\n\n\ndest_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\n\n\nrename_files\n\n rename_files (source_dir:str='data/2_processed/tmp')\n\nRename files in classes and moves to 2_processed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\n\n\nget_tfds_from_dir\n\n get_tfds_from_dir (cfg)\n\nConvert directory of images to tfds dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\nprepare\n\n prepare (ds, cfg, shuffle=False, augment=False)\n\nPrepare dataset using augment, preprocess, cache, shuffle and prefetch.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\ntype\n\ndescription\n\n\ncfg\ncfg (omegaconf.DictConfig):\n\nHydra Configuration\n\n\nshuffle\nbool\nFalse\ndescription, by default False\n\n\naugment\nbool\nFalse\ndescription, by default False\n\n\nReturns\ntype\n\ndescription\n\n\n\n\n\n\nget_preprocess\n\n get_preprocess (cfg)\n\nReturn preprocess function for particular model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\nscalar\n\n scalar (img:<module'PIL.Image'from'/Users/uday/miniforge3/envs/rocks/lib/\n         python3.10/site-packages/PIL/Image.py'>)\n\nScale pixel between -1 and +1.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimg\nPIL.Image\nPIL Image\n\n\nReturns\nPIL.Image\nimagew with pixel values scaled between -1 and 1\n\n\n\n\n\n\nget_value_counts\n\n get_value_counts (dataset_path:str)\n\nGet class counts of all classes in the dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ndataset_path\nstr\ndirectory with subclasses\n\n\nReturns\nNone\n\n\n\n\n\n\n\nget_df\n\n get_df (root:str='data/2_processed')\n\nReturn df with classes, image paths and file names.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\ndata/2_processed\ndirectory to scan for image files, by default “data/2_processed”\n\n\nReturns\nDataFrame\n\nwith columns file_name, class and file_path\n\n\n\n\n\n\nget_dims\n\n get_dims (file:str)\n\nReturn dimenstions for an RBG image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nstr\nfile path for image\n\n\nReturns\nOptional\nreturns a tuple of heights and width of image or None\n\n\n\n\n\n\ntimer_func..wrap_func\n\n timer_func.<locals>.wrap_func (*args, **kwargs)\n\n\n\n\nremove_unsupported_images\n\n remove_unsupported_images (root_folder:str)\n\nRemove unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\nRoot Folder.\n\n\n\n\n\n\nfind_filepaths\n\n find_filepaths (root_folder:str)\n\nRecursively finds all files.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\n\n\ntimer_func\n\n timer_func (func)\n\nShow the execution time of the function object passed.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\ntype\ndescription"
  },
  {
    "objectID": "fastai.html",
    "href": "fastai.html",
    "title": "FastAI",
    "section": "",
    "text": "!rm -rf /content/Whats-this-rock/\n# !git clone https://github.com/udaylunawat/Whats-this-rock.git\n!git clone -b nbdev https://github.com/udaylunawat/Whats-this-rock.git"
  },
  {
    "objectID": "fastai.html#downloading-data-and-moving-bad-corrupted-and-duplicate-images",
    "href": "fastai.html#downloading-data-and-moving-bad-corrupted-and-duplicate-images",
    "title": "FastAI",
    "section": "Downloading data and moving bad, corrupted and duplicate images",
    "text": "Downloading data and moving bad, corrupted and duplicate images\n\nDownloading data\n\n!rm -rf data/bad_images/\n\n\ndel get_data, process_data\n\n\nimport logging\n\n# STEP 1\n# create a logger object instance\nlogger = logging.getLogger()\n\n# STEP 2\n# specifies the lowest severity for logging\nlogger.setLevel(logging.NOTSET)\n\n# STEP 3\n# set a destination for your logs or a \"handler\"\n# here, we choose to print on console (a consoler handler)\nconsole_handler = logging.StreamHandler()\n\n# STEP 4\n# set the logging format for your handler\nlog_format = '%(asctime)s | %(levelname)s: %(message)s'\nconsole_handler.setFormatter(logging.Formatter(log_format))\n\n# finally, we add the handler to the logger\nlogger.addHandler(console_handler)\n\n# the second handler is a file handler\nfile_handler = logging.FileHandler('sample.log')\nfile_handler.setLevel(logging.INFO)\nfile_handler_format = '%(asctime)s | %(levelname)s | %(lineno)d: %(message)s'\nfile_handler.setFormatter(logging.Formatter(file_handler_format))\nlogger.addHandler(file_handler)\n\n\nimport omegaconf\nimport subprocess\nfrom src.data.download import get_data\nfrom src.data.preprocess import process_data\n\ncfg = omegaconf.OmegaConf.load('configs/config.yaml')\ncfg.dataset_id = [1,2,3,4]\n\nsubprocess.run([\"sh\", \"src/scripts/clean_dir.sh\"], stdout=subprocess.PIPE).stdout.decode(\"utf-8\")\nget_data()\nprocess_data(cfg)\n\n\n\nRemove duplicate images\n\n# https://github.com/saifjamsheer/google-dataset-creator/tree/c458ec1f2e676f474527cbf3ee2da3937d260d0c\nfrom imutils import paths\nfrom pathlib import Path\nimport os\nimport cv2\nimport argparse\nfrom src.data.utils import timer_func, find_filepaths\n\n\n\"\"\"\nScript that deletes duplicate images from a dataset. This\ncan be used in conjunction with 'create.py' to delete\nany duplicate images before manual pruning is done. \n\"\"\"\ndef dhash(image, hashSize=8):\n    # Convert the image to grayscale and resize it\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    resized = cv2.resize(gray, (hashSize + 1, hashSize))\n    # Compute the horizontal gradient between adjacent pixels\n    diff = resized[:, 1:] > resized[:, :-1]\n    # Converts the image to a hash and returns it\n    hash = sum([2**i for (i,j) in enumerate(diff.flatten()) if j])\n    return hash\n\n\n@timer_func\ndef remove_duplicates(dir):\n    # Path of the dataset of images\n    output_path = dir\n    hashes = dict()\n\n    # List of paths of the images in the datasets\n    image_paths = list(paths.list_images(output_path))\n\n    for image_path in image_paths:\n        # Load input image\n        \n        image = cv2.imread(image_path)\n        # Compute the hash of the image\n        try:\n            h = dhash(image)\n        except:\n            print(f'cv2.imread {image_path} Returns None!')\n        # Store all images in a dictionary of hashes\n        path = hashes.get(h, [])\n        path.append(image_path)\n        hashes[h] = path\n\n    # Iterate through the hashes\n    for h, path_value in hashes.items():\n        # Checking if a duplicate exists\n        if len(path_value) > 1:\n            # Deleting all duplicates\n            for path in path_value[1:]:\n                print(\"[INFO] Deleting: {}\".format(path))\n                # os.remove(path)\n    return list({k: v for (k, v) in hashes.items() if len(v) > 1}.values())\n\n\nduplicates = remove_duplicates('data/2_processed')\n\n\n!pip install imagededup\n\n\nfrom imagededup.methods import PHash\nimport random\n\ndef find_duplicates(path):\n    phasher = PHash()\n    encodings = phasher.encode_images(image_dir=path)\n    duplicates = phasher.find_duplicates(encoding_map=encodings)\n    duplicates = {k: v for (k, v) in duplicates.items() if len(v) > 0}\n    \n    from imagededup.utils import plot_duplicates\n    plot_duplicates(image_dir=path,\n                    duplicate_map=duplicates,\n                    filename=random.choice(list(duplicates.values()))[0]);\n\nfind_duplicates('data/2_processed/Coal')\n\n\n\nRemoving bad images\n\n# Python program to store list to file using pickle module\nimport pickle\n\n# write list to binary file\ndef write_list(names, filename='delfile'):\n    # store list in binary file so 'wb' mode\n    with open(filename, 'wb') as fp:\n        pickle.dump(names, fp)\n        print('Done writing list into a binary file')\n\n# Read list to memory\ndef read_list(filename='delfile'):\n    # for reading also binary mode is important\n    with open(filename, 'rb') as fp:\n        n_list = pickle.load(fp)\n        return n_list\n\n# del_list = read_list()\n# print('List is', del_list)\n\n\nbad_images = read_list('updated_delete_pickle')\n\n\nimport os\nimport shutil\nos.makedirs('data/bad_images', exist_ok=True)\n\nfor path in bad_images:\n    if os.path.exists(path):\n        shutil.move(str(path), 'data/bad_images')\n\n\n# assert len(os.listdir('data/bad_images')) == len(bad_images)"
  },
  {
    "objectID": "fastai.html#loading-data",
    "href": "fastai.html#loading-data",
    "title": "FastAI",
    "section": "Loading data",
    "text": "Loading data\n\nfrom fastai.vision.all import *\n\n\ndls = ImageDataLoaders.from_folder(path='data/2_processed/', valid_pct=0.2,\n                                   item_tfms=Resize(224))\ndls.valid_ds.items[:3]\n\n\nprint(f'Training set samples:- {len(dls.train_ds)} images.\\nValidation set samples:- {len(dls.valid_ds)} images.')\n\n\ndls.show_batch()\n\n\nlearn = vision_learner(dls, resnet34, metrics=[error_rate, F1Score(average='macro')])\n\n\nlearn.fine_tune(5, cbs=ShowGraphCallback())\n\nLowest error rate:- 0.2460, val_loss:- 0.93\n\n# save model\nlearn.export(file='model.pkl')\n\nlearn = load_learner('model.pkl')\n\n\nlearn.predict('data/4_tfds_dataset/test/Basalt/Basalt_104.jpg')\n\n\ndef get_results(learn):\n    print(f'\\nSample Results:')\n    learn.show_results()\n    plt.show()\n\n    interp = Interpretation.from_learner(learn)\n    print(f'\\nTop losses:-')\n    interp.plot_top_losses(9, figsize=(15,10))\n    plt.show()\n\n    interp = ClassificationInterpretation.from_learner(learn)\n    losses,idxs = interp.top_losses()\n    len(dls.valid_ds)==len(losses)==len(idxs)\n\n    print(f'\\nConfusion Matrix:-')\n    interp.plot_confusion_matrix(figsize=(7,7))\n    plt.show()\n\n    print(f'\\nNormalized Confusion Matrix:-')\n    interp.plot_confusion_matrix(figsize=(7,7), normalize=True)\n    plt.show()\n\n\nget_results(learn)"
  },
  {
    "objectID": "fastai.html#data-cleaning",
    "href": "fastai.html#data-cleaning",
    "title": "FastAI",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nRemove bad images\n\nfiles_to_delete = []\n\n\nfrom fastai.vision.widgets import *\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\nfor idx in cleaner.delete(): files_to_delete.append(cleaner.fns[idx])\n\n\ndef tfds_to_processed(filepath):\n    base = '/'.join(str(filepath).split('/')[-2:]).replace('\\')','')\n    processed = os.path.join('data/2_processed', base)\n    return Path(set(processed))\n\nnew_paths = list(map(lambda x:tfds_to_processed(x), files_to_delete))\n\n\n# update delfile\npaths = read_list()\nwrite_list(paths+new_paths, 'delete_file_pickle')\n\n\nlatest_del = paths+new_paths\n\n\ntry: \n    for idx in paths:\n        idx.unlink()\nexcept: pass\n\nwrite_list(paths, filename='delfile')\n\n\n!rm -rf data/4_tfds_dataset\n\n\nimport splitfolders\nsplitfolders.ratio(\n            \"data/2_processed\",\n            output=\"data/4_tfds_dataset\",\n            ratio=(0.70, 0.15, 0.15),\n            seed=42,\n            move=False,\n        )\n\nTraining after data cleaning\n\ndls = ImageDataLoaders.from_folder(path='data/4_tfds_dataset/',\n                                   train='train',\n                                   valid='val',\n                                   item_tfms=Resize(224))\ndls.valid_ds.items[:3]\n\n\nprint(f\"Number of images after deletetion:- \")\nprint(f'Training set samples:- {len(dls.train_ds)} images.\\nValidation set samples:- {len(dls.valid_ds)} images.')\n\n\ndls.show_batch()\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(5, cbs=ShowGraphCallback())"
  },
  {
    "objectID": "fastai.html#using-fastai-data-augmentation",
    "href": "fastai.html#using-fastai-data-augmentation",
    "title": "FastAI",
    "section": "Using fastai Data Augmentation",
    "text": "Using fastai Data Augmentation\n\ndls = ImageDataLoaders.from_folder(path='data/4_tfds_dataset/', train='train',\n                                   valid='val', item_tfms=Resize(460),\n                                   batch_tfms=aug_transforms(size=224))\ndls.valid_ds.items[:3]\n\n\ndls.show_batch()\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n\nlearn.lr_find()\n\nUsing optimal lr\n\nlearn.fine_tune(5, 3e-3)\n\n\nget_results(learn)"
  },
  {
    "objectID": "detailed_trainer.html",
    "href": "detailed_trainer.html",
    "title": "Trainer",
    "section": "",
    "text": "!rm -rf /content/Whats-this-rock/\n!git clone -b hydra https://github.com/udaylunawat/Whats-this-rock.git\n\n/content\nCloning into 'Whats-this-rock'...\nremote: Enumerating objects: 2093, done.\nremote: Counting objects: 100% (333/333), done.\nremote: Compressing objects: 100% (246/246), done.\nremote: Total 2093 (delta 152), reused 188 (delta 86), pack-reused 1760\nReceiving objects: 100% (2093/2093), 5.84 MiB | 2.85 MiB/s, done.\nResolving deltas: 100% (1355/1355), done.\n/content/Whats-this-rock\n\n\n\nimport os\nos.environ['WANDB_MODE'] = 'offline' # offline\n\nif 'WANDB_API_KEY' not in os.environ:\n    if os.environ['WANDB_MODE'] == 'online':\n        from getpass import getpass\n        secret = getpass('Enter WandB API Key: ')\n        os.environ['WANDB_API_KEY'] = secret\n    else:\n        print(\"WandB Offline!\")\n\nWandB Offline!\n\n\n\n!sh src/scripts/setup.sh\n!sh src/scripts/clean_dir.sh\n\nThe following package was automatically installed and is no longer required:\n  libnvidia-common-460\nUse 'apt autoremove' to remove it.\nThe following packages will be REMOVED:\n  libcudnn8-dev\nThe following held packages will be changed:\n  libcudnn8\nThe following packages will be upgraded:\n  libcudnn8\n1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\nNeed to get 430 MB of archives.\nAfter this operation, 3,139 MB disk space will be freed.\n(Reading database ... 155569 files and directories currently installed.)\nRemoving libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n(Reading database ... 155547 files and directories currently installed.)\nPreparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\nUnpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\nSetting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n     |████████████████████████████████| 1.8 MB 28.4 MB/s \n     |████████████████████████████████| 151 kB 74.1 MB/s \n     |████████████████████████████████| 158 kB 59.3 MB/s \n     |████████████████████████████████| 181 kB 72.3 MB/s \n     |████████████████████████████████| 63 kB 1.7 MB/s \n     |████████████████████████████████| 157 kB 72.6 MB/s \n     |████████████████████████████████| 157 kB 72.9 MB/s \n     |████████████████████████████████| 157 kB 75.3 MB/s \n     |████████████████████████████████| 157 kB 75.1 MB/s \n     |████████████████████████████████| 157 kB 84.0 MB/s \n     |████████████████████████████████| 157 kB 64.6 MB/s \n     |████████████████████████████████| 157 kB 78.5 MB/s \n     |████████████████████████████████| 156 kB 73.5 MB/s \n     |████████████████████████████████| 79 kB 8.3 MB/s \n     |████████████████████████████████| 117 kB 76.0 MB/s \n  Building wheel for antlr4-python3-runtime (setup.py) ... done\n  Building wheel for pathtools (setup.py) ... done\nrenamed 'kaggle.json' -> '/root/.kaggle/kaggle.json'\n\n\n\n#!/usr/bin/env python\n\"\"\"\nTrains a model on rocks dataset\n\"\"\"\n\nimport os\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\nimport gc\nimport subprocess\nimport random\nimport click\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import classification_report\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n# speed improvements\nfrom tensorflow.keras import mixed_precision\n\nmixed_precision.set_global_policy('mixed_float16')\n\nimport wandb\nfrom wandb.keras import WandbCallback\n\nfrom absl import app\n\nimport hydra\nfrom omegaconf import DictConfig\n\nfrom src.data.preprocess import process_data\nfrom src.models.models import get_model\nfrom src.data.utils import get_tfds_from_dir, prepare\nfrom src.models.utils import get_optimizer, get_model_weights_ds\nfrom src.data.download import get_data\nfrom src.callbacks.callbacks import get_earlystopper, get_reduce_lr_on_plateau\nfrom src.visualization import plot\n\n\ndef seed_everything(seed):\n    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nWARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\nYour GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n  METAL, no compute capability (probably not an Nvidia GPU)\nSee https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\nIf you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n\n\nImportError: cannot import name 'get_model_weights_ds' from 'src.models.utils' (/Users/uday/Downloads/Github/Whats-this-rock2/src/models/utils.py)\n\n\n\n# Get configs from the config file.\nconfig = get_config()\n\nconfig.seed = 42\nconfig.dataset_config.train_dataset = [1,2,3,4]\nconfig.dataset_config.sampling = None\n\nconfig.train_config.lr = 0.001\nconfig.train_config.epochs = 50\nconfig.train_config.loss = \"categorical_crossentropy\"\n\nconfig.model_config.preprocess = True\nconfig.model_config.backbone = 'resnet'\nconfig.model_config.regularize_more = False\n\nconfig.callback_config.save_model = False\nconfig.callback_config.rlrp_factor = 0.8\nconfig.callback_config.rlrp_patience = 1\nconfig.callback_config.early_patience = 10\n\n\nprint(config)\n\n\nfrom src.train import train\nfrom src.train import evaluate\n\nseed_everything(config.seed)\n\nrun = wandb.init(\n    project=config.wandb_config.project,\n    config=config.to_dict(),\n    allow_val_change=True,\n)\n\nartifact = wandb.Artifact('rocks', type='files')\nartifact.add_dir('src/')\nwandb.log_artifact(artifact)\n\nprint(f\"\\nDatasets used for Training:- {config.dataset_config.train_dataset}\")\n\nfor dataset_id in config.dataset_config.train_dataset:\n    get_data(dataset_id)\n\nif not os.path.exists('data/3_tfds_dataset/train'):\n    process_data(config)\n\ntrain_dataset, val_dataset, test_dataset = get_tfds_from_dir(config)\n\nlabels = [\n    \"Basalt\",\n    \"Coal\",\n    \"Granite\",\n    \"Limestone\",\n    \"Marble\",\n    \"Quartzite\",\n    \"Sandstone\",\n]\n## Update the `num_classes` and update wandb config\nconfig.dataset_config.num_classes = 7\nif wandb.run is not None:\n    wandb.config.update(\n        {\"dataset_config.num_classes\": config.dataset_config.num_classes})\n    \nmodel, history = train(config, train_dataset, val_dataset, labels)\nevaluate(config, model, history, test_dataset, labels)\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntest_ds = prepare(test_dataset, config)\n# Scores\nscores = model.evaluate(test_dataset, return_dict=True)\nprint('Scores: ', scores)\n\ny_true = tf.concat([y for x, y in test_ds], axis=0)\ntrue_categories = tf.argmax(y_true, axis=1)\n\n# Predict\ny_pred = model.predict(test_dataset, verbose=1)\npredicted_categories = tf.argmax(y_pred, axis=1)\n\n# Confusion Matrix\ncm = plot.plot_confusion_matrix(labels, true_categories, predicted_categories)\n\n# Classification Report\ncl_report = classification_report(true_categories,\n                                  predicted_categories,\n                                  labels=[0, 1, 2, 3, 4, 5, 6],\n                                  target_names=labels,\n                                  output_dict=True)\nprint(cl_report)\n\ncr = sns.heatmap(pd.DataFrame(cl_report).iloc[:-1, :].T, annot=True)\nplt.savefig('imgs/cr.png', dpi=400)\n\n6/6 [==============================] - 2s 302ms/step - loss: 4.8600 - accuracy: 0.3476 - f1_score: 0.3195\nScores:  {'loss': 4.859963417053223, 'accuracy': 0.34756097197532654, 'f1_score': 0.3195052146911621}\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n{'Basalt': {'precision': 0.05555555555555555, 'recall': 0.045454545454545456, 'f1-score': 0.049999999999999996, 'support': 22}, 'Coal': {'precision': 0.4727272727272727, 'recall': 0.4406779661016949, 'f1-score': 0.45614035087719296, 'support': 59}, 'Granite': {'precision': 0.3333333333333333, 'recall': 0.4, 'f1-score': 0.3636363636363636, 'support': 25}, 'Limestone': {'precision': 0.3888888888888889, 'recall': 0.375, 'f1-score': 0.3818181818181819, 'support': 56}, 'Marble': {'precision': 0.34210526315789475, 'recall': 0.24074074074074073, 'f1-score': 0.2826086956521739, 'support': 54}, 'Quartzite': {'precision': 0.3291139240506329, 'recall': 0.4, 'f1-score': 0.3611111111111111, 'support': 65}, 'Sandstone': {'precision': 0.3148148148148148, 'recall': 0.3617021276595745, 'f1-score': 0.33663366336633666, 'support': 47}, 'accuracy': 0.3475609756097561, 'macro avg': {'precision': 0.31950557893262754, 'recall': 0.32336791142236515, 'f1-score': 0.31884976663733716, 'support': 328}, 'weighted avg': {'precision': 0.34721532925108595, 'recall': 0.3475609756097561, 'f1-score': 0.34463378640286313, 'support': 328}}\n\n\n\n\n\n\ntest_ds = prepare(test_dataset, config)\n# Scores\nscores = model.evaluate(test_dataset, return_dict=True)\nprint(\"Scores: \", scores)\n\ny_true = tf.concat([y for x, y in test_ds], axis=0)\ntrue_categories = tf.argmax(y_true, axis=1)\n\n# Predict\ny_pred = model.predict(test_ds)\npredicted_categories = tf.argmax(y_pred, axis=1)\n\n# Confusion Matrix\ncm = plot.plot_confusion_matrix(labels, true_categories,\n                            predicted_categories)\n\n# Classification Report\ncl_report = classification_report(\n    true_categories,\n    predicted_categories,\n    labels=[0, 1, 2, 3, 4, 5, 6],\n    target_names=labels,\n    output_dict=True,\n)\nprint(cl_report)\n\ncr = sns.heatmap(pd.DataFrame(cl_report).iloc[:-1, :].T, annot=True)\nplt.savefig(\"cr.png\", dpi=400)\n\nwandb.log({\"Test Accuracy\": scores[\"accuracy\"]})\nwandb.log({\"Test F1 Score\": scores[\"f1_score\"]})\n\n# average of val and test f1 score\nwandb.log({\n    \"Avg VT F1 Score\":\n    (scores[\"f1_score\"] + max(history.history[\"val_f1_score\"])) / 2\n})\nwandb.log({\"Confusion Matrix\": cm})\nwandb.log({\n    \"Classification Report Image:\":\n    wandb.Image(\"cr.png\", caption=\"Classification Report\")\n})\n\nrun.finish()\n\n6/6 [==============================] - 2s 336ms/step - loss: 4.8600 - accuracy: 0.3476 - f1_score: 0.3195\nScores:  {'loss': 4.859963417053223, 'accuracy': 0.34756097197532654, 'f1_score': 0.3195052146911621}\n6/6 [==============================] - 1s 219ms/step\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n{'Basalt': {'precision': 0.05263157894736842, 'recall': 0.045454545454545456, 'f1-score': 0.04878048780487805, 'support': 22}, 'Coal': {'precision': 0.5370370370370371, 'recall': 0.4915254237288136, 'f1-score': 0.5132743362831859, 'support': 59}, 'Granite': {'precision': 0.36363636363636365, 'recall': 0.32, 'f1-score': 0.3404255319148936, 'support': 25}, 'Limestone': {'precision': 0.45614035087719296, 'recall': 0.4642857142857143, 'f1-score': 0.46017699115044247, 'support': 56}, 'Marble': {'precision': 0.4, 'recall': 0.37037037037037035, 'f1-score': 0.3846153846153846, 'support': 54}, 'Quartzite': {'precision': 0.3013698630136986, 'recall': 0.3384615384615385, 'f1-score': 0.31884057971014496, 'support': 65}, 'Sandstone': {'precision': 0.3584905660377358, 'recall': 0.40425531914893614, 'f1-score': 0.38, 'support': 47}, 'accuracy': 0.38109756097560976, 'macro avg': {'precision': 0.3527579656499138, 'recall': 0.3477647016357026, 'f1-score': 0.3494447587827042, 'support': 328}, 'weighted avg': {'precision': 0.38267056817598527, 'recall': 0.38109756097560976, 'f1-score': 0.381069435442386, 'support': 328}}\n\n\nError: ignored"
  },
  {
    "objectID": "training.html#login-to-wandb",
    "href": "training.html#login-to-wandb",
    "title": "Training Notebook",
    "section": "🪄 Login to WandB",
    "text": "🪄 Login to WandB\n\nimport os\nos.environ['WANDB_MODE'] = 'offline' # offline\n\nif 'WANDB_API_KEY' not in os.environ:\n    if os.environ['WANDB_MODE'] == 'online':\n        from getpass import getpass\n        secret = getpass('Enter WandB API Key: ')\n        os.environ['WANDB_API_KEY'] = secret\n    else:\n        print(\"WandB Offline!\")\n\nWandB Offline!"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Notebook",
    "section": "",
    "text": "from rocks_bot.data.download import get_data"
  },
  {
    "objectID": "eda.html#file-types",
    "href": "eda.html#file-types",
    "title": "Exploratory Notebook",
    "section": "File types",
    "text": "File types\n\n\nCode\ndf['file_name'].apply(lambda x: x.split('.')[-1]).value_counts()\n\n\njpg     2959\npng      398\njpeg      25\nJPEG       2\nName: file_name, dtype: int64"
  },
  {
    "objectID": "eda.html#corrupt-file-counts",
    "href": "eda.html#corrupt-file-counts",
    "title": "Exploratory Notebook",
    "section": "Corrupt file counts",
    "text": "Corrupt file counts\n\n\nCode\ndf['corrupt_status'] = df['file_path'].apply(lambda x: check_corrupted(x))\ndf.corrupt_status.value_counts()\n\n\nTrue    382\nName: corrupt_status, dtype: int64"
  },
  {
    "objectID": "eda.html#corrupted-file-list",
    "href": "eda.html#corrupted-file-list",
    "title": "Exploratory Notebook",
    "section": "Corrupted file list",
    "text": "Corrupted file list\n\n\nCode\ndf[df['corrupt_status']==True]\n\n\nNameError: name 'df' is not defined\n\n\n\n\nCode\nimport seaborn as sns\n\nclass_names = df['class'].value_counts().keys()\ncounts = df['class'].value_counts().values\n\ncount_df = pd.DataFrame(list(zip(class_names, counts)), columns=['class', 'count'])\n\nsns.set_theme(style=\"darkgrid\")\nax = sns.barplot(y='class', x='count', data=count_df)\n\n\n/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\nNameError: name 'df' is not defined\n\n\n\n\nCode\nwidth_list = df.image_width\nheight_list = df.image_height\naverage_width = sum(width_list)/len(width_list)\naverage_height = sum(height_list)/len(height_list)\n\nprint('average width: {} and height: {}'.format(average_width, average_height))\n\nfig, ax =plt.subplots(1,2, figsize=(15, 8))\n\nsns.distplot(width_list, ax=ax[0])\nax[0].set_title('Image width')\nsns.distplot(height_list, ax=ax[1])\nax[1].set_title('Image height')\nfig.show()\n\n\naverage width: nan and height: nan\n\n\n/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\nCode\n# plot histograms to show the distribution of width and height values\nfig, axs = plt.subplots(1,2, figsize=(15,7))\naxs[0].hist(df.image_width.values, bins=20, color = '#91bd3a')\naxs[0].set_title('Width distribution')\n# axs[0].set_xlim(1000, 3000)\n\naxs[1].hist(df.image_height.values, bins=20, color = '#91bd3a')\naxs[1].set_title('Height distribution')\n# axs[1].set_xlim(1000, 3000)\n\nplt.suptitle('Image Dimensions')\nplt.show()"
  },
  {
    "objectID": "eda.html#sample-counts",
    "href": "eda.html#sample-counts",
    "title": "Exploratory Notebook",
    "section": "Sample counts",
    "text": "Sample counts\n\nTraining set counts\n\n\nCode\nget_df(\"data/3_tfds_dataset/train\")['class'].value_counts()\n\n\nQuartzite    492\nLimestone    438\nMarble       431\nCoal         385\nSandstone    370\nGranite      254\nBasalt       211\nName: class, dtype: int64\n\n\n\n\nValidation set counts\n\n\nCode\nget_df(\"data/3_tfds_dataset/val\")['class'].value_counts()\n\n\nQuartzite    82\nLimestone    73\nMarble       71\nCoal         64\nSandstone    61\nGranite      42\nBasalt       35\nName: class, dtype: int64\n\n\n\n\nTest set counts\n\n\nCode\nget_df(\"data/3_tfds_dataset/test\")['class'].value_counts()\n\n\nQuartzite    82\nMarble       73\nLimestone    73\nCoal         65\nSandstone    63\nGranite      43\nBasalt       36\nName: class, dtype: int64"
  },
  {
    "objectID": "eda.html#sample-images",
    "href": "eda.html#sample-images",
    "title": "Exploratory Notebook",
    "section": "Sample Images",
    "text": "Sample Images\n\n\nCode\nds = builder.as_dataset(split='train', shuffle_files=True)\ntfds.show_examples(ds, builder.info)\n\n\nINFO:absl:Constructing tf.data.Dataset image_folder for split train, from data/3_tfds_dataset\n\n\n\n\n\n\n\n\n\nSamples before Augmentation\n\n\nCode\ntrain_dataset = load_dataset()\nvisualize_dataset(train_dataset, title=\"Before Augmentation\")\n\n\n\n\n\n\n\nSamples after RandAugment\n\n\nCode\ntrain_dataset = load_dataset().map(apply_rand_augment, num_parallel_calls=AUTOTUNE)\nvisualize_dataset(train_dataset, title=\"After RandAugment\")\n\n\n\n\n\n\n\nSamples after cutmix and mixup augmentation\n\n\nCode\ntrain_dataset = load_dataset().map(cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE)\nvisualize_dataset(train_dataset, title=\"After cut_mix and mix_up\")"
  },
  {
    "objectID": "a_datasets.html#downloading-datasets",
    "href": "a_datasets.html#downloading-datasets",
    "title": "Downloading Data",
    "section": "Downloading datasets",
    "text": "Downloading datasets\n\nDataset 1\nkaggle datasets download salmaneunus/rock-classification --path data/0_raw/\nunzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\nmv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n\n\nDataset 2\nkaggle datasets download mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals --path data/0_raw/\nunzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\nmv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n\nrm -rf data/1_extracted/dataset2/minerals\n\n\nDataset 3\nwget --quiet -O data/0_raw/dataset3.zip https://github.com/SmartPracticeschool/llSPS-INT-3797-Rock-identification-using-deep-convolution-neural-network/raw/master/dataset.zip\nunzip -qn data/0_raw/dataset3.zip -d data/1_extracted/\nmv data/1_extracted/dataset data/1_extracted/dataset3\n\n\nDataset 4\nkaggle datasets download neelgajare/rocks-dataset --path data/0_raw/\nunzip -qn data/0_raw/rocks-dataset.zip -d data/1_extracted/\nmkdir -p data/1_extracted/dataset4/rock_classes/\nmv -n data/1_extracted/Rocks/* data/1_extracted/dataset4/\nrm -rf data/1_extracted/Rocks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types."
  },
  {
    "objectID": "index.html#installation-training-steps",
    "href": "index.html#installation-training-steps",
    "title": "Whats-this-rock",
    "section": "Installation & Training Steps",
    "text": "Installation & Training Steps\n\nUse the Telegram Bot\nYou can try the bot here on Telegram.\nType /help to get instructions.\n\n\nDeploy Telegram Bot\npip install -r requirements-prod.txt\npython src/bot.py\n\n\nTrain Model\nPaste your kaggle.json file in the root directory\nRun these commands\npip install -r requirements-dev.txt\nsh src/scripts/setup.sh\npython src/models/train.py\nYou can try different models and parameters by editing config.json.\nBy using Hydra it’s now much more easier to override parameters like this\npython src/models/train.py  wandb.project=Whats-this-rockv \\\n                            dataset_id=[1,2,3,4] \\\n                            epochs=50 \\\n                            backbone=resnet\n\n\n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweeps.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid\nwandb agent udaylunawat/Whats-this-rock/$sweepid"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n\n\n\n\n<style=‘font-size:37px’>Features added\n\n\n<style=‘font-size:37px’>Features planned\n\n\n\n\n\nWandb\nDatasets\n\n4 Datasets\n\nAugmentation\n\nkeras-cv\nRegular Augmentation\n\nSampling\n\nOversampling\nUndersampling\nClass weights\n\nRemove Corrupted Images\nTry Multiple Optimizers (Adam, RMSProp, AdamW, SGD)\nGenerators\n\nTFDS datasets\nImageDataGenerator\n\nModels\n\nConvNextTiny\nBaselineCNN\nEfficientnet\nResnet101\nMobileNetv1\nMobileNetv2\nXception\n\nLRScheduleer, LRDecay\n\nBaseline without scheduler\nStep decay\nCosine annealing\nClassic cosine annealing with bathc steps w/o restart\n\nModel Checkpoint, Resume Training\nEvaluation\n\nConfusion Matrix\nClassification Report\n\nDeploy Telegram Bot\n\nHeroku - Deprecated\nRailway\nShow CM and CL in bot\n\nDocker\nGitHub Actions\n\nDeploy Bot when bot.py is updated.\nLint code using GitHub super-linter\n\nConfiguration Management\n\nml-collections\nHydra\n\nPerformance improvement\n\nConvert to tf.data.Dataset\n\nLinting & Formatting\n\nBlack\nFlake8\nisort\npydocstyle\n\n\n\nDeploy to Huggingface spaces\nAccessing the model through FastAPI (Backend)\nStreamlit (Frontend)\nconvert models.py to Classes and more OOP style\nnbdev\nGroup Runs\nkfold cross validation\n\nWandB Tables\nfind the long tail examples or hard examples,\nfind the classes that the model is performing terribly on,\nAdd Badges\nLinting\nRailway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n├── imgs                              <- Images for skill banner, project banner and other images\n│\n├── configs                           <- Configuration files\n│   ├── configs.yaml                  <- config for single run\n│   └── sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n│\n├── data\n│   ├── corrupted_images              <- corrupted images will be moved to this directory\n│   ├── sample_images                 <- Sample images for inference\n│   ├── 0_raw                         <- The original, immutable data dump.\n│   ├── 1_external                    <- Data from third party sources.\n│   ├── 2_interim                     <- Intermediate data that has been transformed.\n│   └── 3_processed                   <- The final, canonical data sets for modeling.\n│\n├── notebooks                         <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                                        the creator's initials, and a short `-` delimited description, e.g.\n│                                        1.0-jqp-initial-data-exploration`.\n│\n│\n├── src                               <- Source code for use in this project.\n│   │\n│   ├── data                          <- Scripts to download or generate data\n│   │   ├── download.py\n│   │   ├── preprocess.py\n│   │   └── utils.py\n│   │\n│   ├── callbacks                     <- functions that are executed during training at given stages of the training procedure\n│   │   ├── custom_callbacks.py\n│   │   └── callbacks.py\n│   │\n│   ├── models                        <- Scripts to train models and then use trained models to make\n│   │   │                                predictions\n│   │   ├── evaluate.py\n│   │   ├── models.py\n│   │   ├── predict.py\n│   │   ├── train.py\n│   │   └── utils.py\n│   │\n│   └── scripts                       <- Scripts to setup dir structure and download datasets\n│   │   ├── clean_dir.sh\n│   │   ├── dataset1.sh\n│   │   ├── dataset2.sh\n│   │   ├── dataset3.sh\n│   │   ├── dataset4.sh\n│   │   └── setup.sh\n│.  │\n│   └── visualization                 <- Scripts for visualizations\n│\n├── .dockerignore                     <- Docker ignore\n├── .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n├── LICENSE                           <- Your project's license.\n├── Makefile                          <- Makefile with commands like `make data` or `make train`\n├── README.md                         <- The top-level README for developers using this project.\n├── requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n│                                        generated with `pip freeze > requirements.txt`\n└── setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn’t handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you’d like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nLinkFree is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset - by Mahmoud Alforawi"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ⭐️ from you. Don’t forget to leave a star ⭐️\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  }
]