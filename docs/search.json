[
  {
    "objectID": "b_download.html",
    "href": "b_download.html",
    "title": "1) Download dataset",
    "section": "",
    "text": "We’ll create and use some bash scripts to create a directory structure for our project\nWe’ll be following the project template by cookiecutter - by datadriven"
  },
  {
    "objectID": "b_download.html#creating-project-directory-structure",
    "href": "b_download.html#creating-project-directory-structure",
    "title": "1) Download dataset",
    "section": "Creating project directory structure",
    "text": "Creating project directory structure\nLet’s create a script scripts/first_run.sh\n\nIt creates the directory structure, and clears existing data files.\n\nsh scripts/first_run.sh\n\n\nCode\n#!/bin/bash\n\n# setting up data dir\nrm -rf data/1_extracted/* data/2_processed/* data/3_tfds_dataset/*\nrm -rf data/corrupted_images/* data/duplicate_images/* data/bad_images/* data/misclassified_images/*\nmkdir -p data/0_raw data/1_extracted data/2_processed data/3_tfds_dataset\nmkdir -p data/corrupted_images data/duplicate_images data/bad_images data/misclassified_images/ checkpoints\n\nmkdir -p data/2_processed/Coal/\nmkdir -p data/2_processed/Basalt/\nmkdir -p data/2_processed/Granite/\nmkdir -p data/2_processed/Marble/\nmkdir -p data/2_processed/Quartzite/\nmkdir -p data/2_processed/Limestone/\nmkdir -p data/2_processed/Sandstone/"
  },
  {
    "objectID": "b_download.html#setting-up-kaggle-api-token",
    "href": "b_download.html#setting-up-kaggle-api-token",
    "title": "1) Download dataset",
    "section": "Setting up Kaggle API token",
    "text": "Setting up Kaggle API token\nDownload kaggle.json by going to Kaggle->Account->Create a new API token\nMove it to ~/$username/.kaggle/ directory."
  },
  {
    "objectID": "b_download.html#creating-scripts-to-download-and-setup-datasets",
    "href": "b_download.html#creating-scripts-to-download-and-setup-datasets",
    "title": "1) Download dataset",
    "section": "Creating scripts to download and setup datasets",
    "text": "Creating scripts to download and setup datasets\n\nDataset 1\nDownloads Dataset1 and moves the extracted files to data/1_extracted/dataset1.\nsh scripts/dataset1.sh\n\n\nCode\n#!/bin/bash\n\n# dataset 1 processing\nkaggle datasets download salmaneunus/rock-classification --path data/0_raw/\nunzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\nmv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n\nmv data/1_extracted/dataset1/Igneous/* data/1_extracted/dataset1/\nmv data/1_extracted/dataset1/Metamorphic/* data/1_extracted/dataset1/\nmv data/1_extracted/dataset1/Sedimentary/* data/1_extracted/dataset1/\n\nrm -rf data/1_extracted/dataset1/Igneous/\nrm -rf data/1_extracted/dataset1/Metamorphic/\nrm -rf data/1_extracted/dataset1/Sedimentary/\n\n\nrock-classification.zip: Skipping, found more recently modified local copy (use --force to force download)\ndata/1_extracted/Dataset -> data/1_extracted/dataset1\n\n\n\n\nDataset 2\nDownloads Dataset2 and moves the extracted files to data/1_extracted/dataset2.\nsh scripts/dataset2.sh\n\n\nCode\n#!/bin/bash\n\n# dataset 2 processing\nkaggle datasets download mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals --path data/0_raw/\nunzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\nmv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n\nrm -rf data/1_extracted/dataset2/minerals\n\nmv data/1_extracted/dataset2/igneous\\ rocks/Basalt data/1_extracted/dataset2/\nmv data/1_extracted/dataset2/igneous\\ rocks/granite data/1_extracted/dataset2/\nmv data/1_extracted/dataset2/metamorphic\\ rocks/marble data/1_extracted/dataset2/\nmv data/1_extracted/dataset2/metamorphic\\ rocks/quartzite data/1_extracted/dataset2/\nmv data/1_extracted/dataset2/sedimentary\\ rocks/Limestone data/1_extracted/dataset2/\nmv data/1_extracted/dataset2/sedimentary\\ rocks/Sandstone data/1_extracted/dataset2/\nmv data/1_extracted/dataset2/sedimentary\\ rocks/coal data/1_extracted/dataset2/\n\nmv data/1_extracted/dataset2/granite data/1_extracted/dataset2/Granite\nmv data/1_extracted/dataset2/marble data/1_extracted/dataset2/Marble\nmv data/1_extracted/dataset2/quartzite data/1_extracted/dataset2/Quartzite\nmv data/1_extracted/dataset2/coal data/1_extracted/dataset2/Coal\n\nrm -rf data/1_extracted/dataset2/igneous\\ rocks\nrm -rf data/1_extracted/dataset2/metamorphic\\ rocks\nrm -rf data/1_extracted/dataset2/sedimentary\\ rocks\n\n\nigneous-metamorphic-sedimentary-rocks-and-minerals.zip: Skipping, found more recently modified local copy (use --force to force download)"
  },
  {
    "objectID": "preprocess_data.html",
    "href": "preprocess_data.html",
    "title": "3) Preprocess Data",
    "section": "",
    "text": "This is the description & steps of the preprocess_data function which combines the following functions\n\n\nRename and move files to data/2_processed.\nList files other than jpg and png, to remove unsupported files.\nList file by types before cleaning.\nRemove\n\nBad Images\nDuplicate Images\nMisclassified Images\nUnsupported Images\nCorrupted Images\n\nList file by types after cleaning.\nGet count of files by class types.\nHandle Imbalance using Undersampling, Oversampling.\n\n\n\n\nsource\n\n\n\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\n\n\n\nprint(\"\\n\\nFiles other than jpg and png.\\n\")\nfiles, _ = find_filepaths('data/2_processed/')\nprint('\\n'.join(list(filter(lambda x: not x.endswith('jpg') and not x.endswith('png'), files))))"
  },
  {
    "objectID": "preprocess_data.html#functions",
    "href": "preprocess_data.html#functions",
    "title": "3) Preprocess Data",
    "section": "Functions",
    "text": "Functions"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types."
  },
  {
    "objectID": "index.html#installation-training-steps",
    "href": "index.html#installation-training-steps",
    "title": "Whats-this-rock",
    "section": "Installation & Training Steps",
    "text": "Installation & Training Steps\n\nUse the Telegram Bot\nYou can try the bot here on Telegram.\nType /help to get instructions.\n\n\nDeploy Telegram Bot\npip install -r requirements-prod.txt\npython src/bot.py\n\n\nTrain Model\nPaste your kaggle.json file in the root directory\nRun these commands\npip install -r requirements-dev.txt\nsh src/scripts/setup.sh\npython src/models/train.py\nYou can try different models and parameters by editing config.json.\nBy using Hydra it’s now much more easier to override parameters like this\npython src/models/train.py  wandb.project=Whats-this-rockv \\\n                            dataset_id=[1,2,3,4] \\\n                            epochs=50 \\\n                            backbone=resnet\n\n\n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweeps.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid\nwandb agent udaylunawat/Whats-this-rock/$sweepid"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n\n\n\n\n<style=‘font-size:37px’>Features added\n\n\n<style=‘font-size:37px’>Features planned\n\n\n\n\n\nWandb\nDatasets\n\n4 Datasets\n\nAugmentation\n\nkeras-cv\nRegular Augmentation\n\nSampling\n\nOversampling\nUndersampling\nClass weights\n\nRemove Corrupted Images\nTry Multiple Optimizers (Adam, RMSProp, AdamW, SGD)\nGenerators\n\nTFDS datasets\nImageDataGenerator\n\nModels\n\nConvNextTiny\nBaselineCNN\nEfficientnet\nResnet101\nMobileNetv1\nMobileNetv2\nXception\n\nLRScheduleer, LRDecay\n\nBaseline without scheduler\nStep decay\nCosine annealing\nClassic cosine annealing with bathc steps w/o restart\n\nModel Checkpoint, Resume Training\nEvaluation\n\nConfusion Matrix\nClassification Report\n\nDeploy Telegram Bot\n\nHeroku - Deprecated\nRailway\nShow CM and CL in bot\n\nDocker\nGitHub Actions\n\nDeploy Bot when bot.py is updated.\nLint code using GitHub super-linter\n\nConfiguration Management\n\nml-collections\nHydra\n\nPerformance improvement\n\nConvert to tf.data.Dataset\n\nLinting & Formatting\n\nBlack\nFlake8\nisort\npydocstyle\n\n\n\nDeploy to Huggingface spaces\nAccessing the model through FastAPI (Backend)\nStreamlit (Frontend)\nconvert models.py to Classes and more OOP style\nnbdev\nGroup Runs\nkfold cross validation\n\nWandB Tables\nfind the long tail examples or hard examples,\nfind the classes that the model is performing terribly on,\nAdd Badges\nLinting\nRailway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n├── imgs                              <- Images for skill banner, project banner and other images\n│\n├── configs                           <- Configuration files\n│   ├── configs.yaml                  <- config for single run\n│   └── sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n│\n├── data\n│   ├── corrupted_images              <- corrupted images will be moved to this directory\n│   ├── sample_images                 <- Sample images for inference\n│   ├── 0_raw                         <- The original, immutable data dump.\n│   ├── 1_external                    <- Data from third party sources.\n│   ├── 2_interim                     <- Intermediate data that has been transformed.\n│   └── 3_processed                   <- The final, canonical data sets for modeling.\n│\n├── notebooks                         <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                                        the creator's initials, and a short `-` delimited description, e.g.\n│                                        1.0-jqp-initial-data-exploration`.\n│\n│\n├── src                               <- Source code for use in this project.\n│   │\n│   ├── data                          <- Scripts to download or generate data\n│   │   ├── download.py\n│   │   ├── preprocess.py\n│   │   └── utils.py\n│   │\n│   ├── callbacks                     <- functions that are executed during training at given stages of the training procedure\n│   │   ├── custom_callbacks.py\n│   │   └── callbacks.py\n│   │\n│   ├── models                        <- Scripts to train models and then use trained models to make\n│   │   │                                predictions\n│   │   ├── evaluate.py\n│   │   ├── models.py\n│   │   ├── predict.py\n│   │   ├── train.py\n│   │   └── utils.py\n│   │\n│   └── scripts                       <- Scripts to setup dir structure and download datasets\n│   │   ├── clean_dir.sh\n│   │   ├── dataset1.sh\n│   │   ├── dataset2.sh\n│   │   ├── dataset3.sh\n│   │   ├── dataset4.sh\n│   │   └── setup.sh\n│.  │\n│   └── visualization                 <- Scripts for visualizations\n│\n├── .dockerignore                     <- Docker ignore\n├── .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n├── LICENSE                           <- Your project's license.\n├── Makefile                          <- Makefile with commands like `make data` or `make train`\n├── README.md                         <- The top-level README for developers using this project.\n├── requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n│                                        generated with `pip freeze > requirements.txt`\n└── setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn’t handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you’d like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nLinkFree is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset - by Mahmoud Alforawi"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ⭐️ from you. Don’t forget to leave a star ⭐️\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  },
  {
    "objectID": "a_download_utils.html",
    "href": "a_download_utils.html",
    "title": "2) Download utilities",
    "section": "",
    "text": "source\n\ntimer_func\n\n timer_func (func)\n\nShow the execution time of the function object passed.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\ntype\ndescription\n\n\n\nRecursively finding all files in folder\n\nsource\n\n\nfind_filepaths\n\n find_filepaths (root_folder:str)\n\nRecursively finds all files.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\nfolder path\n\n\nReturns\n(typing.List, <class ‘int’>)\nReturns a Tuple (Sorted filepaths, length of filepaths)\n\n\n\n\nsource\n\n\nget_new_name\n\n get_new_name (dir_list:list)\n\nReturn dict with old name and new name of files in multiple directories.\n{‘data/1_extracted/dataset1/Basalt/14.jpg’: ‘data/2_processed/Basalt/dataset1_01_Basalt_14.jpg’}\n\n\n\n\nType\nDetails\n\n\n\n\ndir_list\nlist\nlist of dir paths\n\n\nReturns\ndict\n{old_name: new_name}\n\n\n\n\nsource\n\n\nmove_to_processed\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed."
  }
]