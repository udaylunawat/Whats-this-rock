[
  {
    "objectID": "d_train.html",
    "href": "d_train.html",
    "title": "Training",
    "section": "",
    "text": "source\n\ntrain\n\n train (cfg:omegaconf.dictconfig.DictConfig,\n        train_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        val_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n        class_weights:dict)\n\nTrain the model and returns model and history.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration\n\n\ntrain_dataset\nDatasetV2\nTrain Dataset\n\n\nval_dataset\nDatasetV2\nValidation Dataset\n\n\nclass_weights\ndict\nClass weights dictionary\n\n\nReturns\nmodel and history\nmodel and history\n\n\n\n\nsource\n\n\nevaluate\n\n evaluate (cfg:omegaconf.dictconfig.DictConfig,\n           model:keras.engine.training.Model, history:dict,\n           test_dataset:tensorflow.python.data.ops.dataset_ops.DatasetV2,\n           labels:list)\n\nEvaluate the trained model on Test Dataset, log confusion matrix and classification report.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration.\n\n\nmodel\nModel\nTensorflow model.\n\n\nhistory\ndict\nHistory object.\n\n\ntest_dataset\nDatasetV2\nTest Dataset.\n\n\nlabels\nlist\nList of Labels.\n\n\n\n\nsource\n\n\nmain\n\n main (cfg)\n\nRun Main function.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\nDictConfig\nHydra Configuration\n\n\nReturns\nNone"
  },
  {
    "objectID": "b_train_models.html",
    "href": "b_train_models.html",
    "title": "Training models",
    "section": "",
    "text": "source\n\nget_model\n\n get_model (cfg)\n\nGet an image classifier with a CNN based backbone.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntensorflow.keras.Model\nModel object\n\n\n\n/Users/uday/miniforge3/envs/rocks/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n\nsource\n\n\nget_backbone\n\n get_backbone (cfg)\n\nGet backbone for the model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nkeras.api._v2.keras.models\nTensroflow Model"
  },
  {
    "objectID": "b_download.html",
    "href": "b_download.html",
    "title": "Download dataset",
    "section": "",
    "text": "Create directory structure\n\nand if data already exists, clean it!\n\n$ rocks_clean_data\n\n\nDownload and verify the data\n$ rocks_download_data\nDownloads Dataset1 and moves the extracted files to data/1_extracted/dataset1.\nDownloads Dataset2 and moves the extracted files to data/1_extracted/dataset2.\n\nsource\n\ndownload_and_move_datasets\n\n download_and_move_datasets ()\n\nRun the download and move datasets script."
  },
  {
    "objectID": "bot.html",
    "href": "bot.html",
    "title": "Telegram bot deployment",
    "section": "",
    "text": "source\n\nhandle_photo\n\n handle_photo (update, context)\n\n\nsource\n\n\nhandle_message\n\n handle_message (update, context)\n\n\nsource\n\n\nmodel_details\n\n model_details (update, context)\n\n\nsource\n\n\nhelp\n\n help (update, context)\n\n\nsource\n\n\nstart\n\n start (update, context)\n\n\nsource\n\n\ndeploy_bot\n\n deploy_bot ()"
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "source\n\nplotly_confusion_matrix\n\n plotly_confusion_matrix (labels, y, _y)\n\n\nsource\n\n\nget_classification_report\n\n get_classification_report (true_categories, predicted_categories)\n\n\n# #| export\n\n# def get_confusion_matrix(model, test_dataset, y_true, true_categories, y_pred, predicted_categories):\n#     # Confusion Matrix\n#     def get_cm(model, test_dataset, y_true):\n\n#         y_prediction = model.predict(test_dataset)\n#         y_prediction = np.argmax(y_prediction, axis=1)\n#         y_test = np.argmax(y_true, axis=1)\n#         # Create confusion matrix and normalizes it over predicted (columns)\n#         result = confusion_matrix(y_test, y_prediction, normalize=\"pred\")\n#         disp = ConfusionMatrixDisplay(confusion_matrix=result, display_labels=labels)\n#         disp.plot()\n#         plt.xticks(rotation=35)\n#         plt.savefig(\"confusion_matrix.png\")\n#         plt.close()\n\n#     cm_sklearn = get_cm(model, test_dataset, y_true)\n#     return cm_sklearn\n\n\nsource\n\n\nget_confusion_matrix\n\n get_confusion_matrix (true_categories, predicted_categories)"
  },
  {
    "objectID": "a_train_utils.html",
    "href": "a_train_utils.html",
    "title": "Training utilities",
    "section": "",
    "text": "source\n\nget_lr_scheduler\n\n get_lr_scheduler (cfg, lr)\n\nReturn A LearningRateSchedule.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\n\n\n\n\nReturns\nkeras.api._v2.keras.optimizers.schedules\nA LearningRateSchedule\n\n\n\n\nsource\n\n\nget_model_weights\n\n get_model_weights\n                    (train_ds:tensorflow.python.data.ops.dataset_ops.Datas\n                    etV2)\n\nReturn model weights dict.\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_ds\nDatasetV2\nTensorflow Dataset.\n\n\nReturns\ndict\nDictionary of class weights.\n\n\n\n/Users/uday/miniforge3/envs/rocks/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n\nsource\n\n\nget_optimizer\n\n get_optimizer (cfg, lr:str)\n\nGet optimizer set with an learning rate.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nlr\nstr\nlearning rate\n\n\nReturns\nkeras.api._v2.keras.optimizers\nTensorflow optimizer"
  },
  {
    "objectID": "03_e_predict.html",
    "href": "03_e_predict.html",
    "title": "rocks_classifier",
    "section": "",
    "text": "source\n\nmain\n\n main ()\n\n\nsource\n\n\nget_prediction\n\n get_prediction (file)\n\nGet prediction for image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nFile\nImage file\n\n\nReturns\nstr\nPrediction with class name and confidence %.\n\n\n\n\nsource\n\n\nload_model\n\n load_model ()\n\n\nsource\n\n\npreprocess_image\n\n preprocess_image (file, image_size)\n\nDecode and resize image.\n\n\n\n\nType\nDetails\n\n\n\n\nfile\ntype\ndescription\n\n\nimage_size\ntype\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nget_run_data\n\n get_run_data ()\n\nGet data for a wandb sweep."
  },
  {
    "objectID": "preprocess_data.html",
    "href": "preprocess_data.html",
    "title": "Preprocess Data",
    "section": "",
    "text": "This is the description & steps of the preprocess_data function which combines the following functions\n\n\nClear data directory, Download and move datasets.\nRename and move files to data/2_processed.\nList files other than jpg and png, to remove unsupported files.\nList file by types before cleaning.\nRemove\n\nBad Images\nDuplicate Images\nMisclassified Images\nUnsupported Images\nCorrupted Images\n\nList file by types after cleaning.\nGet count of files by class types.\nHandle Imbalance using Undersampling, Oversampling.\n\n\n\n\nAlso clears the data directory if files already exists.\n\n\nsource\n\n\n\n download_and_move_datasets ()\n\nRun the download and move datasets script.\n\n\n\n\n\nsource\n\n\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\n\n\n\n\n\nCode\nprint(\"\\nFiles other than jpg and png.\\n\")\nfiles, _ = find_filepaths('data/2_processed/')\nprint('\\n'.join(list(filter(lambda x: not x.endswith('jpg') and not x.endswith('png'), files))))\n\n\n\n\n\n\n\n\n- Bad Images\n- Duplicate Images\n- Misclassified Images\n- Unsupported Images\n- Corrupted Images\n\nsource\n\n\n\n clean_images (cfg)\n\nRemoves bad, misclassified, duplicate, corrupted and unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Undersampling, Oversampling and No Sampling.\n\nsource\n\n\n\n sampling (cfg)\n\nOversamples/Undersample/No Sampling data into train, val, test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\n\n\n\nprocess_data wraps all the above functions.\n\nsource\n\n\n\n process_data (cfg)\n\nDownload dataset, removes unsupported and corrupted images, and splits data into train, val and test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration"
  },
  {
    "objectID": "c_callbacks.html",
    "href": "c_callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "source\n\nget_callbacks\n\n get_callbacks (cfg)\n\nReturn a Callback List.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\nList\nCallbacks List\n\n\n\n\nsource\n\n\nCustomEarlyStopping\n\n CustomEarlyStopping (monitor='val_accuracy', value=0.6, min_epoch=15,\n                      verbose=0)\n\nStops training when epoch > min_epoch and monitor value < value.\n\nsource\n\n\nLRLogger\n\n LRLogger ()\n\nlog lr at the end of every epoch.\n\nsource\n\n\nget_reduce_lr_on_plateau\n\n get_reduce_lr_on_plateau (cfg)\n\nReturn tf.keras.callbacks.ReduceLROnPlateau.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntf.keras.callbacks.ReduceLROnPlateau\nReduce learning rate when a metric has stopped improving.\n\n\n\n\nsource\n\n\nget_earlystopper\n\n get_earlystopper (cfg)\n\nReturn tf.keras.callbacks.EarlyStopping.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntensorflow.keras.callbacks\nStop training when a monitored metric has stopped improving."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Whats-this-rock",
    "section": "",
    "text": "This project deploys a telegram bot that classifies rock images into 1 of 7 types.\nThis package uses tensorflow to accelerate deep learning experimentation.\nMLOps workflow like\nwas all done using Weights & Biases\nAdditionally, nbdev was used to"
  },
  {
    "objectID": "index.html#installation-training-steps",
    "href": "index.html#installation-training-steps",
    "title": "Whats-this-rock",
    "section": "Installation & Training Steps",
    "text": "Installation & Training Steps\n\nInstall\nTo install, use pip:\npip install git+https://github.com/udaylunawat/Whats-this-rock.git\n\n\nUse the Telegram Bot\nYou can try the bot here on Telegram.\n\nType /help to get instructions in chat.\n\n\n\nDeploy Telegram Bot\nrocks_deploy_bot\n\n\nDownload and process data\n%%bash\nrocks_process_data  --config-dir configs \\\n                    remove_bad= True \\\n                    remove_misclassified= True \\\n                    remove_duplicates= True \\\n                    remove_corrupted= True \\\n                    remove_unsupported= True \\\n                    sampling=None \\\n                    train_split=0.8 \\\n\n\nTrain Model\nRun these commands\nrocks_train_model   --config-dir configs \\\n                    wandb.project=Whats-this-rock \\\n                    wandb.mode=offline \\\n                    wandb.use=False \\\n                    dataset_id=[1,2] \\\n                    epochs=30 \\\n                    lr=0.005 \\\n                    augmentation=None \\\n                    monitor=val_loss \\\n                    loss=categorical_crossentropy \\\n                    backbone=resnet \\\n                    lr_schedule=cosine_decay_restarts \\\n                    lr_decay_steps=300 \\\n                    trainable=False \\\nYou can try different models and parameters by editing configs/config.yaml.\nBy using Hydra it’s now much more easier to override parameters like this\nrocks_train_model wandb.project=Whats-this-rock \\\n                  dataset_id=[1,2] \\\n                  epochs=50 \\\n                  backbone=resnet\n\n\n\n\n\nWandb Sweeps (Hyperparameter Tuning)\nEdit configs/sweeps.yaml\nwandb sweep \\\n--project Whats-this-rock \\\n--entity udaylunawat \\\nconfigs/sweep.yaml\nThis will return a command with $sweepid\nwandb agent udaylunawat/Whats-this-rock/$sweepid"
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Whats-this-rock",
    "section": "Demo",
    "text": "Demo\n\n\n\nRun in Colab\nView Source on GitHub\nDownload Notebook"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Whats-this-rock",
    "section": "Features",
    "text": "Features\n\n\n\n\n<style=‘font-size:37px’>Features added\n\n\n<style=‘font-size:37px’>Features planned\n\n\n\n\n\nWandb\nDatasets\n\n4 Datasets\n\nAugmentation\n\nkeras-cv\nRegular Augmentation\n\nSampling\n\nOversampling\nUndersampling\nClass weights\n\nRemove Corrupted Images\nTry Multiple Optimizers (Adam, RMSProp, AdamW, SGD)\nGenerators\n\nTFDS datasets\nImageDataGenerator\n\nModels\n\nConvNextTiny\nBaselineCNN\nEfficientnet\nResnet101\nMobileNetv1\nMobileNetv2\nXception\n\nLRScheduleer, LRDecay\n\nBaseline without scheduler\nStep decay\nCosine annealing\nClassic cosine annealing with bathc steps w/o restart\n\nModel Checkpoint, Resume Training\nEvaluation\n\nConfusion Matrix\nClassification Report\n\nDeploy Telegram Bot\n\nHeroku - Deprecated\nRailway\nShow CM and CL in bot\n\nDocker\nGitHub Actions\n\nDeploy Bot when bot.py is updated.\nLint code using GitHub super-linter\n\nConfiguration Management\n\nml-collections\nHydra\n\nPerformance improvement\n\nConvert to tf.data.Dataset\n\nLinting & Formatting\n\nBlack\nFlake8\nisort\npydocstyle\n\nAdd Badges\n\nLinting\n\nfound the classes that the model is performing terribly on\nnbdev\nCI\ndocumentation\n\n\nDeploy to Huggingface spaces\nAccessing the model through FastAPI (Backend)\nStreamlit (Frontend)\nconvert models.py to Classes and more OOP style\nGroup Runs\nkfold cross validation\n\nWandB Tables\nfind the long tail examples or hard examples,\nAdd Badges\nRailway"
  },
  {
    "objectID": "index.html#technologies-used",
    "href": "index.html#technologies-used",
    "title": "Whats-this-rock",
    "section": "Technologies Used",
    "text": "Technologies Used"
  },
  {
    "objectID": "index.html#directory-tree",
    "href": "index.html#directory-tree",
    "title": "Whats-this-rock",
    "section": "Directory Tree",
    "text": "Directory Tree\n├── imgs                              <- Images for skill banner, project banner and other images\n│\n├── configs                           <- Configuration files\n│   ├── configs.yaml                  <- config for single run\n│   └── sweeps.yaml                   <- confguration file for sweeps hyperparameter tuning\n│\n├── data\n│   ├── corrupted_images              <- corrupted images will be moved to this directory\n│   ├── misclassified_images          <- misclassified images will be moved to this directory\n│   ├── bad_images                    <- Bad images will be moved to this directory\n│   ├── duplicate_images              <- Duplicate images will be moved to this directory\n│   ├── sample_images                 <- Sample images for inference\n│   ├── 0_raw                         <- The original, immutable data dump.\n│   ├── 1_external                    <- Data from third party sources.\n│   ├── 2_interim                     <- Intermediate data that has been transformed.\n│   └── 3_processed                   <- The final, canonical data sets for modeling.\n│\n├── notebooks                         <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                                        the creator's initials, and a short `-` delimited description, e.g.\n│                                        1.0-jqp-initial-data-exploration`.\n│\n│\n├── rocks_classifier                  <- Source code for use in this project.\n│   │\n│   ├── data                          <- Scripts to download or generate data\n│   │   ├── download.py\n│   │   ├── preprocess.py\n│   │   └── utils.py\n│   │\n│   ├── callbacks                     <- functions that are executed during training at given stages of the training procedure\n│   │   └── callbacks.py\n│   │\n│   ├── models                        <- Scripts to train models and then use trained models to make\n│   │   │                                predictions\n│   │   ├── evaluate.py\n│   │   ├── models.py\n│   │   ├── predict.py\n│   │   ├── train.py\n│   │   └── utils.py\n│   │\n│   │\n│   └── visualization                 <- Scripts for visualizations\n│\n├── .dockerignore                     <- Docker ignore\n├── .gitignore                        <- GitHub's excellent Python .gitignore customized for this project\n├── LICENSE                           <- Your project's license.\n├── README.md                         <- The top-level README for developers using this project.\n├── CHANGELOG.md                      <- Release changes.\n├── CODE_OF_CONDUCT.md                <- Code of conduct.\n├── CONTRIBUTING.md                   <- Contributing Guidelines.\n├── settings.ini                      <- configuration.\n├── README.md                         <- The top-level README for developers using this project.\n├── requirements.txt                  <- The requirements file for reproducing the analysis environment, e.g.\n│                                        generated with `pip freeze > requirements.txt`\n└── setup.py                          <- makes project pip installable (pip install -e .) so src can be imported"
  },
  {
    "objectID": "index.html#bug-feature-request",
    "href": "index.html#bug-feature-request",
    "title": "Whats-this-rock",
    "section": "Bug / Feature Request",
    "text": "Bug / Feature Request\nIf you find a bug (the site couldn’t handle the query and / or gave undesired results), kindly open an issue here by including your search query and the expected result.\nIf you’d like to request a new function, feel free to do so by opening an issue here. Please include sample queries and their corresponding results."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Whats-this-rock",
    "section": "Contributing",
    "text": "Contributing\n\nContributions make the open source community such an amazing place to learn, inspire, and create.\nAny contributions you make are greatly appreciated.\nCheck out our contribution guidelines for more information."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Whats-this-rock",
    "section": "License",
    "text": "License\nLinkFree is licensed under the MIT License - see the LICENSE file for details."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Whats-this-rock",
    "section": "Credits",
    "text": "Credits\n\nDataset 1 - by Mahmoud Alforawi\nDataset 2 - by salmaneunus\nnbdev inspiration - tmabraham"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Whats-this-rock",
    "section": "Support",
    "text": "Support\nThis project needs a ⭐️ from you. Don’t forget to leave a star ⭐️\n\n\nWalt might be the one who knocks  but Hank is the one who rocks."
  },
  {
    "objectID": "a_download_utils.html",
    "href": "a_download_utils.html",
    "title": "Download utilities",
    "section": "",
    "text": "source\n\nclean_data_dir\n\n clean_data_dir ()\n\nClean all data directories except 0_raw.\n\nsource\n\n\ncopy_configs_tocwd\n\n copy_configs_tocwd ()\n\n\nsource\n\n\nget_new_name\n\n get_new_name (dir_list:list)\n\nReturn dict with old name and new name of files in multiple directories.\n{‘data/1_extracted/dataset1/Basalt/14.jpg’: ‘data/2_processed/Basalt/dataset1_01_Basalt_14.jpg’}\n\n\n\n\nType\nDetails\n\n\n\n\ndir_list\nlist\nlist of dir paths\n\n\nReturns\ndict\n{old_name: new_name}\n\n\n\n\nsource\n\n\nmove_to_processed\n\n move_to_processed ()\n\nCombines files with same subclass and moves them to the subclass under data/2_processed.\nUses get_new_name to create new names of files and then rename them and copy to data/2_processed.\n\nsource\n\n\nclean_images\n\n clean_images (cfg)\n\nRemoves bad, misclassified, duplicate, corrupted and unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\n\n\nsource\n\n\nmove_bad_files\n\n move_bad_files (txt_file, dest, text)\n\nMoves files in txt_file to dest.\n\n\n\n\nType\nDetails\n\n\n\n\ntxt_file\nfile\ntext file with path of bad images\n\n\ndest\ntype\ntarget destination\n\n\ntext\n\n\n\n\n\n\nsource\n\n\nmove_and_rename\n\n move_and_rename (class_dir:str)\n\nMove files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, …), which contains image files.\n\n\n\n\nType\nDetails\n\n\n\n\nclass_dir\nstr\ndescription\n\n\n\n\nsource\n\n\nmove_files\n\n move_files (src_dir:str, dest_dir:str='data/2_processed/tmp')\n\nMove files to tmp directory in 2_processed.\nsrc_dir: directory of rock subclass with files [Basalt, Marble, Coal, …]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\ndescription\n\n\ndest_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\nsource\n\n\nrename_files\n\n rename_files (source_dir:str='data/2_processed/tmp')\n\nRename files in classes and moves to 2_processed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_dir\nstr\ndata/2_processed/tmp\ndescription, by default “data/2_processed/tmp”\n\n\n\n\nsource\n\n\nget_tfds_from_dir\n\n get_tfds_from_dir (cfg)\n\nConvert directory of images to tfds dataset.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig):\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nprepare\n\n prepare (ds, cfg, shuffle=False, augment=False)\n\nPrepare dataset using augment, preprocess, cache, shuffle and prefetch.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\ntype\n\ndescription\n\n\ncfg\ncfg (omegaconf.DictConfig):\n\nHydra Configuration\n\n\nshuffle\nbool\nFalse\ndescription, by default False\n\n\naugment\nbool\nFalse\ndescription, by default False\n\n\nReturns\ntype\n\ndescription\n\n\n\n\nsource\n\n\nget_preprocess\n\n get_preprocess (cfg)\n\nReturn preprocess function for particular model.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\nget_value_counts\n\n get_value_counts (dataset_path:str, column:str='file_type')\n\nGet value counts of passed column.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\nstr\n\ndirectory with subclasses\n\n\ncolumn\nstr\nfile_type\ncolumn name\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nget_df\n\n get_df (root:str='data/2_processed')\n\nReturn df with classes, image paths and file names.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\ndata/2_processed\ndirectory to scan for image files, by default “data/2_processed”\n\n\nReturns\nDataFrame\n\nwith columns file_name, class and file_path\n\n\n\n\n\n\ntimer_func..wrap_func\n\n timer_func.<locals>.wrap_func (*args, **kwargs)\n\n\nsource\n\n\nremove_unsupported_images\n\n remove_unsupported_images (root_folder:str)\n\nRemove unsupported images.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\nRoot Folder.\n\n\n\n\nsource\n\n\nfind_filepaths\n\n find_filepaths (root_folder:str)\n\nRecursively finds all files.\n\n\n\n\nType\nDetails\n\n\n\n\nroot_folder\nstr\ndescription\n\n\nReturns\ntype\ndescription\n\n\n\n\nsource\n\n\ntimer_func\n\n timer_func (func)\n\nShow the execution time of the function object passed.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\ntype\ndescription\n\n\n\n\nsource\n\n\nsampling\n\n sampling (cfg)\n\nOversamples/Undersample/No Sampling data into train, val, test.\n\n\n\n\nType\nDetails\n\n\n\n\ncfg\ncfg (omegaconf.DictConfig)\nHydra Configuration"
  }
]