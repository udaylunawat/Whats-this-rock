{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71403000-8f21-4a8c-9960-058f9436f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e457d6-ff89-49d4-8318-144c6b6f53f7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79785345-67d3-4b59-ab95-b5d8cc76321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Trains a model on rocks dataset.\"\"\"\n",
    "\n",
    "import logging\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "import wandb\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# speed improvements\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, mixed_precision\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from rocks_classifier.callbacks.callbacks import get_callbacks\n",
    "from rocks_classifier.data.utils import get_tfds_from_dir, prepare\n",
    "from rocks_classifier.models.models import get_model\n",
    "from rocks_classifier.models.utils import get_lr_scheduler, get_model_weights, get_optimizer\n",
    "from rocks_classifier.visualization.plot import plotly_confusion_matrix, \\\n",
    "                                                get_classification_report, get_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab644f-a15e-4976-9b65-610e96e25de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    cfg: DictConfig,\n",
    "    train_dataset: tf.data.Dataset,\n",
    "    val_dataset: tf.data.Dataset,\n",
    "    class_weights: dict,\n",
    "):\n",
    "    \"\"\"Train the model and returns model and history.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : DictConfig\n",
    "        Hydra Configuration\n",
    "    train_dataset : tf.data.Dataset\n",
    "        Train Dataset\n",
    "    val_dataset : tf.data.Dataset\n",
    "        Validation Dataset\n",
    "    class_weights : dict\n",
    "        Class weights dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model and history\n",
    "        model and history\n",
    "    \"\"\"\n",
    "    # Initalize model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = get_model(cfg)\n",
    "    model.summary()\n",
    "\n",
    "    print(f\"\\nModel loaded: {cfg.backbone}.\\n\\n\")\n",
    "    lr_decayed_fn = get_lr_scheduler(cfg, cfg.lr)\n",
    "\n",
    "    optimizer = get_optimizer(cfg, lr=lr_decayed_fn)\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(optimizer)  # speed improvements\n",
    "\n",
    "    # f1_score_metrics = [tfa.metrics.F1Score(num_classes=cfg.num_classes, average=\"macro\", threshold=0.5)]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=cfg.loss,\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "        ],  # f1_score_metrics\n",
    "    )\n",
    "\n",
    "    callbacks, cfg = get_callbacks(cfg)\n",
    "    verbose = 1\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=cfg.epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        workers=-1,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if not cfg.trainable and history.history[\"val_accuracy\"][-1] > 0.75:\n",
    "        model.layers[0].trainable = False\n",
    "        # model.trainable = True\n",
    "        for layer in model.layers[0].layers[-cfg.last_layers :]:\n",
    "            layer.trainable = True\n",
    "        # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "        for layer in model.layers[0].layers:\n",
    "            if isinstance(layer, layers.BatchNormalization):\n",
    "                layer.trainable = False\n",
    "\n",
    "        print(\"\\nFinetuning model with BatchNorm layers freezed.\\n\")\n",
    "        # print(\"\\nBackbone layers\\n\\n\")\n",
    "        # for layer in model.layers[0].layers:\n",
    "        #     print(layer.name, layer.trainable)\n",
    "        #     lr_decayed_fn = (\n",
    "\n",
    "        # cfg.reduce_lr.min_lr = cfg.reduce_lr.min_lr * 0.7\n",
    "        lr_decayed_fn = get_lr_scheduler(cfg, cfg.lr / 10)\n",
    "        # lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        #     K.get_value(model.optimizer.learning_rate),\n",
    "        #     first_decay_steps=cfg.lr_decay_steps,\n",
    "        # )\n",
    "        optimizer = get_optimizer(cfg, lr=lr_decayed_fn)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=cfg.loss,\n",
    "            metrics=[\"accuracy\"],  # f1_score_metrics\n",
    "        )\n",
    "\n",
    "        epochs = cfg.epochs // 3\n",
    "\n",
    "        callbacks, cfg = get_callbacks(cfg)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            initial_epoch=len(history.history[\"loss\"]),\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620360b-8361-4c51-bb8a-aa2fb26b7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate(\n",
    "    cfg: DictConfig,\n",
    "    model: tf.keras.Model,\n",
    "    history: dict,\n",
    "    test_dataset: tf.data.Dataset,\n",
    "    labels: list,\n",
    "):\n",
    "    \"\"\"Evaluate the trained model on Test Dataset, log confusion matrix and classification report.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : DictConfig\n",
    "        Hydra Configuration.\n",
    "    model : tf.keras.Model\n",
    "        Tensorflow model.\n",
    "    history : dict\n",
    "        History object.\n",
    "    test_dataset : tf.data.Dataset\n",
    "        Test Dataset.\n",
    "    labels : list\n",
    "        List of Labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scores\n",
    "    test_dataset = prepare(test_dataset, cfg)\n",
    "    scores = model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "    print(\"\\n\\nTest Dataset Results!\")\n",
    "    print(\"Scores: \", scores)\n",
    "    \n",
    "    def predict(test_dataset):\n",
    "        y_true = tf.concat([y for x, y in test_dataset], axis=0)\n",
    "        true_categories = tf.argmax(y_true, axis=1)\n",
    "        y_pred = model.predict(test_dataset, verbose=1)\n",
    "        predicted_categories = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "        return true_categories, predicted_categories\n",
    "    \n",
    "    true_categories, predicted_categories = predict(test_dataset)\n",
    "    cl_report = get_classification_report(true_categories, predicted_categories)\n",
    "    cm_sklearn = get_confusion_matrix(true_categories, predicted_categories)\n",
    "    cm_plotly = plotly_confusion_matrix(labels, true_categories, predicted_categories)\n",
    "\n",
    "    wandb.log({\"Test Accuracy\": scores[\"accuracy\"]})\n",
    "    wandb.log({\"Confusion Matrix\": cm_plotly})\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"Classification Report Image:\": wandb.Image(\n",
    "                \"classification_report.png\", caption=\"Classification Report\"\n",
    "            )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee3342-8529-475f-847d-17b9f418e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@hydra.main(config_path=\".\", config_name=\"config\", version_base=\"1.2\")\n",
    "def main(cfg) -> None:\n",
    "    \"\"\"Run Main function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : DictConfig\n",
    "        Hydra Configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    tf.keras.utils.set_random_seed(cfg.seed) # Setting global seed\n",
    "\n",
    "    # WandB\n",
    "    run = wandb.init(\n",
    "        project=cfg.wandb.project,\n",
    "        notes=cfg.notes,\n",
    "        config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "    )\n",
    "\n",
    "    # artifact = wandb.Artifact(\"rocks\", type=\"files\")\n",
    "    # artifact.add_dir(\"rocks_classifier/\")\n",
    "    # wandb.log_artifact(artifact)\n",
    "\n",
    "    print(f\"\\nDatasets used for Training:- {cfg.dataset_id}\")\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = get_tfds_from_dir(cfg)\n",
    "    labels = train_dataset.class_names\n",
    "    cfg.num_classes = len(labels)\n",
    "\n",
    "    class_weights = get_model_weights(train_dataset) if cfg.class_weights else None\n",
    "\n",
    "    train_dataset = prepare(train_dataset, cfg, shuffle=True, augment=cfg.augmentation)\n",
    "    val_dataset = prepare(val_dataset, cfg)\n",
    "\n",
    "    model, history = train(cfg, train_dataset, val_dataset, class_weights)\n",
    "\n",
    "    evaluate(cfg, model, history, test_dataset, labels)\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd065a1e-e8dd-4da8-9a92-b30d7bab4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d55593-5ac7-4a33-8a3d-c6b73c569dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
