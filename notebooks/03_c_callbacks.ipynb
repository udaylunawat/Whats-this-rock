{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd5d06-dc87-4528-b533-e8d29603f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317cde24-455c-455f-94e4-a70404973992",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "- EarlyStopping\n",
    "- ReduceLROnPlateau\n",
    "- LRLogger\n",
    "- CustomEarlyStopping\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9a079-0491-4d7a-a766-0c66e387b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807cf60-5b34-49ff-b6ef-9d8ec70422fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_earlystopper(cfg) -> tf.keras.callbacks:\n",
    "    \"\"\"Return tf.keras.callbacks.EarlyStopping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.callbacks\n",
    "        Stop training when a monitored metric has stopped improving.\n",
    "    \"\"\"\n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=cfg.monitor,\n",
    "        patience=cfg.earlystopping.patience,\n",
    "        verbose=1,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    return earlystopper\n",
    "\n",
    "\n",
    "def get_reduce_lr_on_plateau(cfg):\n",
    "    \"\"\"Return tf.keras.callbacks.ReduceLROnPlateau.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.callbacks.ReduceLROnPlateau\n",
    "        Reduce learning rate when a metric has stopped improving.\n",
    "    \"\"\"\n",
    "    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=cfg.monitor,\n",
    "        factor=cfg.reduce_lr.factor,\n",
    "        patience=cfg.reduce_lr.patience,\n",
    "        min_lr=cfg.reduce_lr.min_lr,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return reduce_lr_on_plateau\n",
    "\n",
    "\n",
    "class LRLogger(tf.keras.callbacks.Callback):\n",
    "    \"\"\"log lr at the end of every epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tf : callbacks\n",
    "        callbacks\n",
    "    \"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Log lr on epoch end.\"\"\"\n",
    "        if callable(self.model.optimizer.learning_rate):\n",
    "            current_decayed_lr = self.model.optimizer.learning_rate(\n",
    "                self.model.optimizer.iterations\n",
    "            )\n",
    "        else:\n",
    "            current_decayed_lr = self.model.optimizer.learning_rate\n",
    "\n",
    "        # current_decayed_lr = self.model.optimizer._decayed_lr(tf.float32).numpy()\n",
    "        print(\" - LR: {:.8f}\".format(current_decayed_lr))\n",
    "        wandb.log({\"learning_rate\": current_decayed_lr}, commit=False)\n",
    "\n",
    "\n",
    "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Stops training when epoch > min_epoch and monitor value < default value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Callback : Tensorflow Callback\n",
    "        tensorflow Callback\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, monitor=\"val_accuracy\", value=0.60, min_epoch=15, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.min_epoch = min_epoch\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\n",
    "                f\"Early stopping requires {RuntimeWarning} available!\" % self.monitor\n",
    "            )\n",
    "\n",
    "        if current < self.value and epoch == self.min_epoch:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        # if current < self.value+0.08 and epoch == 25:\n",
    "        #     if self.verbose > 0:\n",
    "        #         print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "        #     self.model.stop_training = True\n",
    "\n",
    "\n",
    "def get_callbacks(cfg):\n",
    "    \"\"\"Return a Callback List.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        Callbacks List\n",
    "    \"\"\"\n",
    "    wandbcallback = WandbCallback(\n",
    "        monitor=cfg.monitor,\n",
    "        mode=\"auto\",\n",
    "        save_model=(cfg.save_model),\n",
    "    )\n",
    "\n",
    "    callbacks = [wandbcallback, LRLogger(), CustomEarlyStopping()]\n",
    "    if cfg.earlystopping.use:\n",
    "        earlystopper = get_earlystopper(cfg)\n",
    "        callbacks.append(earlystopper)\n",
    "    else:\n",
    "        cfg.earlystopping.patience = None\n",
    "    if cfg.reduce_lr.use:\n",
    "        reduce_lr = get_reduce_lr_on_plateau(cfg)\n",
    "        callbacks.append(reduce_lr)\n",
    "    else:\n",
    "        cfg.reduce_lr.factor = None\n",
    "        cfg.reduce_lr.min_lr = None\n",
    "        cfg.reduce_lr.patience = None\n",
    "\n",
    "    return callbacks, cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2ff6c-fd95-487b-940c-411d383735ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rocks')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
