{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "_doc_ = \"\"\"Dowload datasets and move them to the right folder.\n",
    "\n",
    "`run_scripts` is the main function for doing the whole process.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset\n",
    "\n",
    "> We'll create the project directory structure and download the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll create and use some bash scripts to create a directory structure for our project\n",
    "- We'll be following the project template by [cookiecutter - by datadriven](https://drivendata.github.io/cookiecutter-data-science/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from nbdev.config import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create directory structure\n",
    "> and if data already exists, clean it!\n",
    "\n",
    "```bash\n",
    "$ rocks_clean_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and verify the data \n",
    "\n",
    "```bash\n",
    "$ rocks_download_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset1](https://www.kaggle.com/datasets/salmaneunus/rock-classification) and moves the extracted files to `data/1_extracted/dataset1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset2](https://www.kaggle.com/datasets/mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals) and moves the extracted files to `data/1_extracted/dataset2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | hide\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "from rocks_classifier.data.utils import timer_func, find_filepaths\n",
    "from rocks_classifier.data.utils import copy_configs_tocwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | hide\n",
    "\n",
    "\n",
    "class download_and_move:\n",
    "    \"\"\"Downloads datasets(zip files), extracts them to the correct folders, and rearranges them.\"\"\"\n",
    "\n",
    "    data_dict = {\n",
    "        1: {\n",
    "            \"url\": \"https://huggingface.co/datasets/udayl/rocks/resolve/main/rock-classification.zip\",\n",
    "            \"file_name\": \"rock-classification.zip\",\n",
    "            \"folder_name\": \"Dataset\",\n",
    "            \"filecount\": 2083,\n",
    "        },\n",
    "        2: {\n",
    "            \"url\": \"https://huggingface.co/datasets/udayl/rocks/resolve/main/igneous-metamorphic-sedimentary-rocks-and-minerals.zip\",\n",
    "            \"file_name\": \"igneous-metamorphic-sedimentary-rocks-and-minerals.zip\",\n",
    "            \"folder_name\": \"Rock_Dataset\",\n",
    "            \"filecount\": 546,\n",
    "        },\n",
    "    }\n",
    "    classes = [\n",
    "        \"Coal\",\n",
    "        \"Basalt\",\n",
    "        \"Granite\",\n",
    "        \"Marble\",\n",
    "        \"Quartzite\",\n",
    "        \"Limestone\",\n",
    "        \"Sandstone\",\n",
    "    ]\n",
    "\n",
    "    @timer_func  # | hide_line\n",
    "    def run_scripts(self):\n",
    "        \"\"\"\n",
    "        Download the datasets using scripts.\n",
    "\n",
    "        Uses `find_filepaths` to recursively find paths for all files in a directory.\n",
    "        \"\"\"\n",
    "        copy_configs_tocwd()\n",
    "        self.clean_data_dir()\n",
    "        for dataset_id in self.data_dict:\n",
    "            if self.archives_exist(dataset_id) and self.files_exists(dataset_id):\n",
    "                # if both zip files exist and are extracted\n",
    "                print(f\"Dataset{dataset_id} already exists.\")\n",
    "                count = self.verify_files(dataset_id)\n",
    "                print(f\"Total Files in dataset{dataset_id}:- {count}.\\n\")\n",
    "            if not self.archives_exist(dataset_id):\n",
    "                # if zip files do not exist\n",
    "                print(f\"Downloading dataset {dataset_id}...\")\n",
    "                self.download_file(dataset_id)\n",
    "            if not self.files_exists(dataset_id):\n",
    "                # if zip files exists but they're not extracted\n",
    "                print(f\"Extracting dataset {dataset_id}...\")\n",
    "                self.extract_archive(f\"data/0_raw/dataset{dataset_id}.zip\")\n",
    "                shutil.move(\n",
    "                    f'data/1_extracted/{self.data_dict[dataset_id][\"folder_name\"]}',\n",
    "                    f\"data/1_extracted/dataset{dataset_id}\",\n",
    "                )\n",
    "            self.move_subclasses_to_root_dir(dataset_id)\n",
    "\n",
    "\n",
    "    def clean_data_dir(self):\n",
    "        \"\"\"Clean all data directories except 0_raw.\"\"\"\n",
    "        dir_0 = \"0_raw\"\n",
    "        dir_1 = \"1_extracted\"\n",
    "        dir_2 = \"2_processed\"\n",
    "        dir_3 = \"3_tfds_dataset\"\n",
    "\n",
    "        dir_list = [\n",
    "            dir_1,\n",
    "            dir_2,\n",
    "            dir_3,\n",
    "            \"corrupted_images\",\n",
    "            \"duplicate_images\",\n",
    "            \"misclassified_images\",\n",
    "            \"bad_images\",\n",
    "        ]\n",
    "\n",
    "        classes = [\n",
    "            \"Coal\",\n",
    "            \"Basalt\",\n",
    "            \"Granite\",\n",
    "            \"Marble\",\n",
    "            \"Quartzite\",\n",
    "            \"Limestone\",\n",
    "            \"Sandstone\",\n",
    "        ]\n",
    "        os.makedirs(os.path.join(\"data\", dir_0), exist_ok=True)\n",
    "        print(\"Cleaning data dir...\")\n",
    "        for dir_name in dir_list:\n",
    "            for root, dirs, files in os.walk(os.path.join(\"data\", dir_name)):\n",
    "                for f in files:\n",
    "                    os.unlink(os.path.join(root, f))\n",
    "                for d in dirs:\n",
    "                    shutil.rmtree(os.path.join(root, d), ignore_errors=True)\n",
    "            os.makedirs(os.path.join(\"data\", dir_name), exist_ok=True)\n",
    "\n",
    "        for class_name in classes:\n",
    "            os.makedirs(os.path.join(\"data\", dir_2, class_name))\n",
    "\n",
    "\n",
    "    def download_file(self, dataset_id, dest_dir=\"data/0_raw/\"):\n",
    "        \"\"\"Download and write file to destination directory.\"\"\"\n",
    "        r = requests.get(self.data_dict[dataset_id][\"url\"], allow_redirects=True)\n",
    "        open(os.path.join(dest_dir, f\"dataset{dataset_id}.zip\"), \"wb\").write(r.content)\n",
    "\n",
    "    def extract_archive(self, file_path, dest_dir=\"data/1_extracted\"):\n",
    "        \"\"\"Extract zip file to dest_dir.\"\"\"\n",
    "        shutil.unpack_archive(file_path, dest_dir, \"zip\")\n",
    "\n",
    "    def archives_exist(self, dataset_id) -> bool:\n",
    "        \"\"\"Check if zip files already exist.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_id : int\n",
    "            dataset number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        boolean\n",
    "            returns True if zip files exist\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(\"data/0_raw\", f\"dataset{dataset_id}.zip\")):\n",
    "            return True\n",
    "\n",
    "    def files_exists(self, dataset_id):\n",
    "        \"\"\"Check whether extracted files exist.\"\"\"\n",
    "        if os.path.exists(os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\")):\n",
    "            count = self.verify_files(dataset_id)\n",
    "            return count\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def move_subclasses_to_root_dir(self, dataset_id):\n",
    "        \"\"\"Move subclasses to data/2_processed.\"\"\"\n",
    "        root_path = f\"data/1_extracted/dataset{dataset_id}\"\n",
    "        for class_name in os.listdir(f\"data/1_extracted/dataset{dataset_id}\"):\n",
    "            class_path = os.path.join(root_path, class_name)\n",
    "            for subclass in os.listdir(class_path):\n",
    "                if subclass.capitalize() in self.classes:\n",
    "                    folder_path = os.path.join(class_path, subclass)\n",
    "                    shutil.move(folder_path, f\"data/1_extracted/dataset{dataset_id}/\")\n",
    "                    shutil.move(\n",
    "                        f\"{root_path}/{subclass}\",\n",
    "                        f\"{root_path}/{subclass.capitalize()}\",\n",
    "                    )\n",
    "            shutil.rmtree(class_path, ignore_errors=False)\n",
    "\n",
    "    def verify_files(self, dataset_id):\n",
    "        \"\"\"Verify the image counts.\"\"\"\n",
    "        _, count = find_filepaths(\n",
    "            os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\")\n",
    "        )\n",
    "        assert count == self.data_dict[dataset_id][\"filecount\"]\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_and_move_datasets():\n",
    "    \"\"\"Run the download and move datasets script.\"\"\"\n",
    "    download_and_move().run_scripts()\n",
    "    print(\"Download and move process finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('rocks')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
