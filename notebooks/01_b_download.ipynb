{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Download dataset\n",
    "\n",
    "> We'll create the project directory structure and download the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll create and use some bash scripts to create a directory structure for our project\n",
    "- We'll be following the project template by [cookiecutter - by datadriven](https://drivendata.github.io/cookiecutter-data-science/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.config import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "repo_name = get_config().lib_name\n",
    "!mkdir -p $repo_name\\/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import os\n",
    "from rocks_classifier.data.utils import timer_func, find_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating project directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a script `scripts/clean_dir.sh` \n",
    "\n",
    "> It creates the directory structure, and clears existing data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh scripts/clean_dir.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "with open ('rocks_classifier/scripts/clean_dir.sh', 'w') as rsh:\n",
    "    rsh.write('''\\\n",
    "#!/bin/bash\n",
    "\n",
    "# setting up data dir\n",
    "rm -rf data/1_extracted/* data/2_processed/* data/3_tfds_dataset/*\n",
    "rm -rf data/corrupted_images/* data/duplicate_images/* data/bad_images/* data/misclassified_images/*\n",
    "mkdir -p data/0_raw data/1_extracted data/2_processed data/3_tfds_dataset\n",
    "mkdir -p data/corrupted_images data/duplicate_images data/bad_images data/misclassified_images/ checkpoints\n",
    "\n",
    "mkdir -p data/2_processed/Coal/\n",
    "mkdir -p data/2_processed/Basalt/\n",
    "mkdir -p data/2_processed/Granite/\n",
    "mkdir -p data/2_processed/Marble/\n",
    "mkdir -p data/2_processed/Quartzite/\n",
    "mkdir -p data/2_processed/Limestone/\n",
    "mkdir -p data/2_processed/Sandstone/\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#|eval: false\n",
    "#|code-fold: true\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# setting up data dir\n",
    "rm -rf data/1_extracted/* data/2_processed/* data/3_tfds_dataset/*\n",
    "rm -rf data/corrupted_images/* data/duplicate_images/* data/bad_images/* data/misclassified_images/*\n",
    "mkdir -p data/0_raw data/1_extracted data/2_processed data/3_tfds_dataset\n",
    "mkdir -p data/corrupted_images data/duplicate_images data/bad_images data/misclassified_images/ checkpoints\n",
    "\n",
    "mkdir -p data/2_processed/Coal/\n",
    "mkdir -p data/2_processed/Basalt/\n",
    "mkdir -p data/2_processed/Granite/\n",
    "mkdir -p data/2_processed/Marble/\n",
    "mkdir -p data/2_processed/Quartzite/\n",
    "mkdir -p data/2_processed/Limestone/\n",
    "mkdir -p data/2_processed/Sandstone/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating scripts to download and setup datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset1](https://www.kaggle.com/datasets/salmaneunus/rock-classification) and moves the extracted files to `data/1_extracted/dataset1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh scripts/dataset1.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "with open ('rocks_classifier/scripts/dataset1.sh', 'w') as rsh:\n",
    "    rsh.write('''\\\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 1 processing\n",
    "wget --quiet -O data/0_raw/rock-classification.zip -nc https://huggingface.co/datasets/udayl/rocks/resolve/main/rock-classification.zip\n",
    "unzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\n",
    "mv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n",
    "\n",
    "mv data/1_extracted/dataset1/Igneous/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Metamorphic/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Sedimentary/* data/1_extracted/dataset1/\n",
    "\n",
    "rm -rf data/1_extracted/dataset1/Igneous/\n",
    "rm -rf data/1_extracted/dataset1/Metamorphic/\n",
    "rm -rf data/1_extracted/dataset1/Sedimentary/\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/1_extracted/Dataset -> data/1_extracted/dataset1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#|eval: false\n",
    "#|code-fold: true\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 1 processing\n",
    "wget --quiet -O data/0_raw/rock-classification.zip -nc https://huggingface.co/datasets/udayl/rocks/resolve/main/rock-classification.zip\n",
    "unzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\n",
    "mv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n",
    "\n",
    "mv data/1_extracted/dataset1/Igneous/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Metamorphic/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Sedimentary/* data/1_extracted/dataset1/\n",
    "\n",
    "rm -rf data/1_extracted/dataset1/Igneous/\n",
    "rm -rf data/1_extracted/dataset1/Metamorphic/\n",
    "rm -rf data/1_extracted/dataset1/Sedimentary/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset2](https://www.kaggle.com/datasets/mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals) and moves the extracted files to `data/1_extracted/dataset2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh scripts/dataset2.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "with open ('rocks_classifier/scripts/dataset2.sh', 'w') as rsh:\n",
    "    rsh.write('''\\\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 2 processing\n",
    "wget --quiet -O data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -nc https://huggingface.co/datasets/udayl/rocks/resolve/main/igneous-metamorphic-sedimentary-rocks-and-minerals.zip\n",
    "unzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\n",
    "mv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/minerals\n",
    "\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/Basalt data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/granite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/marble data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/quartzite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Limestone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Sandstone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/coal data/1_extracted/dataset2/\n",
    "\n",
    "mv data/1_extracted/dataset2/granite data/1_extracted/dataset2/Granite\n",
    "mv data/1_extracted/dataset2/marble data/1_extracted/dataset2/Marble\n",
    "mv data/1_extracted/dataset2/quartzite data/1_extracted/dataset2/Quartzite\n",
    "mv data/1_extracted/dataset2/coal data/1_extracted/dataset2/Coal\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/igneous\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/metamorphic\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/sedimentary\\ rocks\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#|eval: false\n",
    "#|code-fold: true\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 2 processing\n",
    "wget --quiet -O data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -nc https://huggingface.co/datasets/udayl/rocks/resolve/main/igneous-metamorphic-sedimentary-rocks-and-minerals.zip\n",
    "unzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\n",
    "mv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/minerals\n",
    "\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/Basalt data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/granite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/marble data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/quartzite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Limestone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Sandstone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/coal data/1_extracted/dataset2/\n",
    "\n",
    "mv data/1_extracted/dataset2/granite data/1_extracted/dataset2/Granite\n",
    "mv data/1_extracted/dataset2/marble data/1_extracted/dataset2/Marble\n",
    "mv data/1_extracted/dataset2/quartzite data/1_extracted/dataset2/Quartzite\n",
    "mv data/1_extracted/dataset2/coal data/1_extracted/dataset2/Coal\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/igneous\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/metamorphic\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/sedimentary\\ rocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and verify the data \n",
    "\n",
    "```bash\n",
    "python rock_classifier/data/download.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "#| code-fold: true\n",
    "\n",
    "\n",
    "class download_datasets:\n",
    "    data_dict = {\n",
    "        1: {\"script\": \"rocks_classifier/scripts/dataset1.sh\", \"filecount\": 2083},\n",
    "        2: {\"script\": \"rocks_classifier/scripts/dataset2.sh\", \"filecount\": 546},\n",
    "    }\n",
    "        \n",
    "    @timer_func  # | hide_line\n",
    "    def run_scripts(self):\n",
    "        \"\"\"\n",
    "        Download the datasets using scripts.\n",
    "\n",
    "        Uses `find_filepaths` to recursively find paths for all files in a directory.\n",
    "        \"\"\"\n",
    "\n",
    "        for dataset_id in self.data_dict:\n",
    "            if self.files_exists(dataset_id):\n",
    "                print(f\"Dataset{dataset_id} already exists.\")\n",
    "                self.verify_files(dataset_id)\n",
    "            else:\n",
    "                print(f\"Downloading dataset {dataset_id}...\")\n",
    "                os.system(f\"sh {self.data_dict[dataset_id]['script']}\")\n",
    "                \n",
    "                \n",
    "    def files_exists(self, dataset_id):\n",
    "        if os.path.exists(\n",
    "                os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\")):\n",
    "            self.verify_files(dataset_id)\n",
    "            return True\n",
    "    \n",
    "    def verify_files(self, dataset_id):\n",
    "        \"\"\"verifies the image counts\"\"\"\n",
    "        _, count = find_filepaths(\n",
    "            os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\"))\n",
    "        assert count == self.data_dict[dataset_id][\"filecount\"]\n",
    "        print(f\"Total Files in dataset{dataset_id}:- {count}.\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%%bash\n",
    "rocks_clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 1...\n",
      "data/1_extracted/Dataset -> data/1_extracted/dataset1\n",
      "Downloading dataset 2...\n",
      "Function 'run_scripts' executed in 7.6447s\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "download_datasets().run_scripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "assert all([dir in os.listdir() for dir in ['data', 'rocks_classifier']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
