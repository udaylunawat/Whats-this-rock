{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Download dataset\n",
    "\n",
    "> We'll create the project directory structure and download the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll create and use some bash scripts to create a directory structure for our project\n",
    "- We'll be following the project template by [cookiecutter - by datadriven](https://drivendata.github.io/cookiecutter-data-science/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.config import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timer_func, find_filepaths\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "from rocks_classifier.data.utils import timer_func, find_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create directory structure, and if data already exists, clean it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "rocks_clean_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and verify the data \n",
    "\n",
    "```bash\n",
    "$ rocks_clean_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset1](https://www.kaggle.com/datasets/salmaneunus/rock-classification) and moves the extracted files to `data/1_extracted/dataset1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset2](https://www.kaggle.com/datasets/mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals) and moves the extracted files to `data/1_extracted/dataset2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "from rocks_classifier.data.utils import clean_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "#| code-fold: true\n",
    "\n",
    "\n",
    "class download_and_move:\n",
    "    \"\"\"Downloads datasets(zip files), extracts them to the correct folders, and rearranges them.\n",
    "    \"\"\"\n",
    "    data_dict = {\n",
    "        1: {\"url\": \"https://huggingface.co/datasets/udayl/rocks/resolve/main/rock-classification.zip\",\n",
    "            \"file_name\": \"rock-classification.zip\",\n",
    "            \"folder_name\": \"Dataset\",\n",
    "            \"filecount\": 2083},\n",
    "        2: {\"url\": \"https://huggingface.co/datasets/udayl/rocks/resolve/main/igneous-metamorphic-sedimentary-rocks-and-minerals.zip\", \n",
    "            \"file_name\": \"igneous-metamorphic-sedimentary-rocks-and-minerals.zip\",\n",
    "            \"folder_name\": \"Rock_Dataset\",\n",
    "            \"filecount\": 546},\n",
    "    }\n",
    "    classes = ['Coal', 'Basalt', 'Granite', 'Marble', 'Quartzite', 'Limestone', 'Sandstone']\n",
    "        \n",
    "    @timer_func  # | hide_line\n",
    "    def run_scripts(self):\n",
    "        \"\"\"\n",
    "        Download the datasets using scripts.\n",
    "\n",
    "        Uses `find_filepaths` to recursively find paths for all files in a directory.\n",
    "        \"\"\"\n",
    "        clean_data_dir()\n",
    "        for dataset_id in self.data_dict:\n",
    "            if self.archives_exist(dataset_id) and self.files_exists(dataset_id):\n",
    "                # if both zip files exist and are extracted\n",
    "                print(f\"Dataset{dataset_id} already exists.\")\n",
    "                count = self.verify_files(dataset_id)\n",
    "                print(f\"Total Files in dataset{dataset_id}:- {count}.\\n\")\n",
    "            if not self.archives_exist(dataset_id):\n",
    "                # if zip files do not exist\n",
    "                print(f\"Extracting dataset {dataset_id}...\")\n",
    "                self.download_file(dataset_id)\n",
    "            if not self.files_exists(dataset_id):\n",
    "                # if zip files exists but they're not extracted\n",
    "                self.extract_archive(f'data/0_raw/dataset{dataset_id}.zip')\n",
    "                os.rename(f'data/1_extracted/{self.data_dict[dataset_id][\"folder_name\"]}', \n",
    "                          f'data/1_extracted/dataset{dataset_id}')\n",
    "            self.move_subclasses_to_root_dir(dataset_id)\n",
    "            \n",
    "                \n",
    "    def download_file(self, dataset_id, dest_dir='data/0_raw/'):\n",
    "        \"\"\"Download and write file to destination directory.\"\"\"\n",
    "        r = requests.get(self.data_dict[dataset_id]['url'], allow_redirects=True)\n",
    "        open(os.path.join(dest_dir, f'dataset{dataset_id}.zip'), 'wb').write(r.content)\n",
    "        \n",
    "    def extract_archive(self, file_path, dest_dir='data/1_extracted'):\n",
    "        \"\"\"Extract zip file to dest_dir.\"\"\"\n",
    "        shutil.unpack_archive(file_path, dest_dir, 'zip')\n",
    "    \n",
    "    def archives_exist(self, dataset_id):\n",
    "        if os.path.exists(os.path.join('data/0_raw', f'dataset{dataset_id}.zip')):\n",
    "            return True\n",
    "        \n",
    "    def files_exists(self, dataset_id):\n",
    "        \"\"\"check whether extracted files exist.\"\"\"\n",
    "        if os.path.exists(\n",
    "                os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\")):\n",
    "            count = self.verify_files(dataset_id)\n",
    "            return count\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def move_subclasses_to_root_dir(self, dataset_id):\n",
    "        \"\"\"Move subclasses to data/2_processed\"\"\"\n",
    "        root_path = f'data/1_extracted/dataset{dataset_id}'\n",
    "        for class_name in os.listdir(f'data/1_extracted/dataset{dataset_id}'):\n",
    "            class_path = os.path.join(root_path, class_name)\n",
    "            for subclass in os.listdir(class_path):\n",
    "                if subclass.capitalize() in self.classes:\n",
    "                    folder_path = os.path.join(class_path, subclass)\n",
    "                    shutil.move(folder_path, f'data/1_extracted/dataset{dataset_id}/')\n",
    "                    shutil.move(f'{root_path}/{subclass}', f'{root_path}/{subclass.capitalize()}')\n",
    "            shutil.rmtree(class_path)\n",
    "            \n",
    "    \n",
    "    def verify_files(self, dataset_id):\n",
    "        \"\"\"verifies the image counts\"\"\"\n",
    "        _, count = find_filepaths(\n",
    "            os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\"))\n",
    "        assert count == self.data_dict[dataset_id][\"filecount\"]\n",
    "        return count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'run_scripts' executed in 4.9303s\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "def download_and_move_datasets():\n",
    "    \"\"\"Run the download and move datasets script.\"\"\"\n",
    "    download_and_move().run_scripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "assert all([dir in os.listdir() for dir in ['data', 'rocks_classifier']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
