{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: false\n",
    "skip_showdoc: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Download dataset\n",
    "\n",
    "> We'll create the project directory structure and download the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll create and use some bash scripts to create a directory structure for our project\n",
    "- We'll be following the project template by [cookiecutter - by datadriven](https://drivendata.github.io/cookiecutter-data-science/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "assert all([dir in os.listdir() for dir in ['data', 'rocks_classifier']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.config import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "repo_name = get_config().repo\n",
    "!mkdir -p $repo_name\\/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating project directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a script `scripts/first_run.sh` \n",
    "\n",
    "> It creates the directory structure, and clears existing data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh scripts/first_run.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "with open ('rocks_classifier/scripts/first_run.sh', 'w') as rsh:\n",
    "    rsh.write('''\\\n",
    "#!/bin/bash\n",
    "\n",
    "# setting up data dir\n",
    "rm -rf data/1_extracted/* data/2_processed/* data/3_tfds_dataset/*\n",
    "rm -rf data/corrupted_images/* data/duplicate_images/* data/bad_images/* data/misclassified_images/*\n",
    "mkdir -p data/0_raw data/1_extracted data/2_processed data/3_tfds_dataset\n",
    "mkdir -p data/corrupted_images data/duplicate_images data/bad_images data/misclassified_images/ checkpoints\n",
    "\n",
    "mkdir -p data/2_processed/Coal/\n",
    "mkdir -p data/2_processed/Basalt/\n",
    "mkdir -p data/2_processed/Granite/\n",
    "mkdir -p data/2_processed/Marble/\n",
    "mkdir -p data/2_processed/Quartzite/\n",
    "mkdir -p data/2_processed/Limestone/\n",
    "mkdir -p data/2_processed/Sandstone/\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#|eval: false\n",
    "#|code-fold: true\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# setting up data dir\n",
    "rm -rf data/1_extracted/* data/2_processed/* data/3_tfds_dataset/*\n",
    "rm -rf data/corrupted_images/* data/duplicate_images/* data/bad_images/* data/misclassified_images/*\n",
    "mkdir -p data/0_raw data/1_extracted data/2_processed data/3_tfds_dataset\n",
    "mkdir -p data/corrupted_images data/duplicate_images data/bad_images data/misclassified_images/ checkpoints\n",
    "\n",
    "mkdir -p data/2_processed/Coal/\n",
    "mkdir -p data/2_processed/Basalt/\n",
    "mkdir -p data/2_processed/Granite/\n",
    "mkdir -p data/2_processed/Marble/\n",
    "mkdir -p data/2_processed/Quartzite/\n",
    "mkdir -p data/2_processed/Limestone/\n",
    "mkdir -p data/2_processed/Sandstone/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Kaggle API token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `kaggle.json` by going to [Kaggle](https://www.kaggle.com/)->Account->`Create a new API token`\n",
    "\n",
    "Move it to `~/$username/.kaggle/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating scripts to download and setup datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset1](https://www.kaggle.com/datasets/salmaneunus/rock-classification) and moves the extracted files to `data/1_extracted/dataset1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh scripts/dataset1.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "with open ('rocks_classifier/scripts/dataset1.sh', 'w') as rsh:\n",
    "    rsh.write('''\\\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 1 processing\n",
    "kaggle datasets download salmaneunus/rock-classification --path data/0_raw/\n",
    "unzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\n",
    "mv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n",
    "\n",
    "mv data/1_extracted/dataset1/Igneous/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Metamorphic/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Sedimentary/* data/1_extracted/dataset1/\n",
    "\n",
    "rm -rf data/1_extracted/dataset1/Igneous/\n",
    "rm -rf data/1_extracted/dataset1/Metamorphic/\n",
    "rm -rf data/1_extracted/dataset1/Sedimentary/\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock-classification.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "data/1_extracted/Dataset -> data/1_extracted/dataset1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#|eval: false\n",
    "#|code-fold: true\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 1 processing\n",
    "kaggle datasets download salmaneunus/rock-classification --path data/0_raw/\n",
    "unzip -qn data/0_raw/rock-classification.zip -d data/1_extracted/\n",
    "mv -vn data/1_extracted/Dataset data/1_extracted/dataset1\n",
    "\n",
    "mv data/1_extracted/dataset1/Igneous/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Metamorphic/* data/1_extracted/dataset1/\n",
    "mv data/1_extracted/dataset1/Sedimentary/* data/1_extracted/dataset1/\n",
    "\n",
    "rm -rf data/1_extracted/dataset1/Igneous/\n",
    "rm -rf data/1_extracted/dataset1/Metamorphic/\n",
    "rm -rf data/1_extracted/dataset1/Sedimentary/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads [Dataset2](https://www.kaggle.com/datasets/mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals) and moves the extracted files to `data/1_extracted/dataset2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh scripts/dataset2.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "with open ('rocks_classifier/scripts/dataset2.sh', 'w') as rsh:\n",
    "    rsh.write('''\\\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 2 processing\n",
    "kaggle datasets download mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals --path data/0_raw/\n",
    "unzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\n",
    "mv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/minerals\n",
    "\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/Basalt data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/granite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/marble data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/quartzite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Limestone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Sandstone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/coal data/1_extracted/dataset2/\n",
    "\n",
    "mv data/1_extracted/dataset2/granite data/1_extracted/dataset2/Granite\n",
    "mv data/1_extracted/dataset2/marble data/1_extracted/dataset2/Marble\n",
    "mv data/1_extracted/dataset2/quartzite data/1_extracted/dataset2/Quartzite\n",
    "mv data/1_extracted/dataset2/coal data/1_extracted/dataset2/Coal\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/igneous\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/metamorphic\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/sedimentary\\ rocks\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igneous-metamorphic-sedimentary-rocks-and-minerals.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#|eval: false\n",
    "#|code-fold: true\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# dataset 2 processing\n",
    "kaggle datasets download mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals --path data/0_raw/\n",
    "unzip -qn data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\n",
    "mv data/1_extracted/Rock_Dataset data/1_extracted/dataset2\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/minerals\n",
    "\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/Basalt data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/igneous\\ rocks/granite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/marble data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/metamorphic\\ rocks/quartzite data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Limestone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/Sandstone data/1_extracted/dataset2/\n",
    "mv data/1_extracted/dataset2/sedimentary\\ rocks/coal data/1_extracted/dataset2/\n",
    "\n",
    "mv data/1_extracted/dataset2/granite data/1_extracted/dataset2/Granite\n",
    "mv data/1_extracted/dataset2/marble data/1_extracted/dataset2/Marble\n",
    "mv data/1_extracted/dataset2/quartzite data/1_extracted/dataset2/Quartzite\n",
    "mv data/1_extracted/dataset2/coal data/1_extracted/dataset2/Coal\n",
    "\n",
    "rm -rf data/1_extracted/dataset2/igneous\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/metamorphic\\ rocks\n",
    "rm -rf data/1_extracted/dataset2/sedimentary\\ rocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and verify the data \n",
    "\n",
    "```bash\n",
    "python src/data/download.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "#| code-fold: true\n",
    "\n",
    "# | code-fold: true\n",
    "import os\n",
    "from rocks_classifier.data.utils import timer_func, find_filepaths\n",
    "\n",
    "\n",
    "@timer_func  # | hide_line\n",
    "def download_datasets():\n",
    "    \"\"\"\n",
    "    Download the datasets using scripts and verifies the image counts.\n",
    "\n",
    "    Uses `find_filepaths` to recursively find paths for all files in a directory.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dict = {\n",
    "        1: {\"script\": \"rocks_classifier/scripts/dataset1.sh\", \"filecount\": 2083},\n",
    "        2: {\"script\": \"rocks_classifier/scripts/dataset2.sh\", \"filecount\": 4553},\n",
    "    }\n",
    "    for dataset_id in data_dict:\n",
    "        if not os.path.exists(\n",
    "            os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\")\n",
    "        ):\n",
    "            print(f\"Downloading dataset {dataset_id}...\")\n",
    "            os.system(f\"sh {data_dict[dataset_id]['script']}\")\n",
    "        else:\n",
    "            _, count = find_filepaths(\n",
    "                os.path.join(\"data\", \"1_extracted\", f\"dataset{dataset_id}\")\n",
    "            )\n",
    "            # assert count == data_dict[dataset_id][\"filecount\"]\n",
    "            print(f\"dataset{dataset_id} already exists.\")\n",
    "            print(f\"Total Files in dataset{dataset_id}:- {count}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "\n",
    "# | eval: false\n",
    "if __name__ == \"__main__\":\n",
    "    download_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
