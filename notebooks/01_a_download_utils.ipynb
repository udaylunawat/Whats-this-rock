{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da3016-8c83-40f4-9cb6-3b9e5697046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303a7af-0fc3-48f0-ac29-040daa5f491d",
   "metadata": {},
   "source": [
    "# Download utilities\n",
    "> Utils for downloading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da0382-f63b-439b-b26d-7ffe67994e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ee873-fb26-46e2-ae0f-41aeaa61c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f78344-8738-403d-8113-39e33c7d9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import imghdr\n",
    "import os\n",
    "import shutil\n",
    "import splitfolders\n",
    "from time import time\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import logging\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import applications, layers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748da08-1029-4264-a241-298c514b2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def clean_data_dir():\n",
    "    \"\"\"Clean all data directories except 0_raw.\n",
    "    \"\"\"\n",
    "    dir_0 = '0_raw'\n",
    "    dir_1 = '1_extracted'\n",
    "    dir_2 = '2_processed'\n",
    "    dir_3 = '3_tfds_dataset'\n",
    "    \n",
    "    dir_list = [dir_1, dir_2, dir_3,\n",
    "                'corrupted_images', \n",
    "                'duplicate_images', \n",
    "                'misclassified_images']\n",
    "    \n",
    "    classes = ['Coal', 'Basalt', 'Granite', 'Marble', 'Quartzite', 'Limestone', 'Sandstone']\n",
    "    os.makedirs(os.path.join('data', dir_0), exist_ok=True)\n",
    "    for dir_name in dir_list:\n",
    "        for root, dirs, files in os.walk(os.path.join('data', dir_name)):\n",
    "            for f in files:\n",
    "                os.unlink(os.path.join(root, f))\n",
    "            for d in dirs:\n",
    "                shutil.rmtree(os.path.join(root, d), ignore_errors = True)\n",
    "        os.makedirs(os.path.join('data', dir_name), exist_ok=True)\n",
    "    \n",
    "    for class_name in classes:\n",
    "        os.makedirs(os.path.join('data', dir_2, class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ffe80-1a21-4bff-836e-f66c130871df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def download_configs():\n",
    "    config_files = {\n",
    "        1:{'url':'https://raw.githubusercontent.com/udaylunawat/Whats-this-rock/nbdev/configs/bad_images%20selected%20by%20gemini.txt','file_name':'bad_images selected by gemini.txt'},\n",
    "        2:{'url':'https://raw.githubusercontent.com/udaylunawat/Whats-this-rock/nbdev/configs/config.yaml','file_name':'config.yaml'},\n",
    "        3:{'url':'https://raw.githubusercontent.com/udaylunawat/Whats-this-rock/nbdev/configs/duplicates%20selected%20by%20gemini.txt','file_name':'duplicates selected by gemini.txt'},\n",
    "        4:{'url':'https://raw.githubusercontent.com/udaylunawat/Whats-this-rock/nbdev/configs/misclassified%20selected%20by%20gemini.txt','file_name':'misclassified selected by gemini.txt'},\n",
    "        5:{'url':'https://raw.githubusercontent.com/udaylunawat/Whats-this-rock/nbdev/configs/sweep.yaml','file_name':'sweep.yaml'}\n",
    "    }\n",
    "    def download_file(url, file_name, dest_dir='configs'):\n",
    "        \"\"\"Download and write file to destination directory.\"\"\"\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(os.path.join(dest_dir, file_name), 'wb').write(r.content)\n",
    "    \n",
    "    os.makedirs('configs', exist_ok=True)\n",
    "    for index, file in config_files.items():\n",
    "        file_url = file['url']\n",
    "        file_name = file['file_name']\n",
    "        download_file(file_url, file_name)\n",
    "        \n",
    "def get_new_name(dir_list: list) -> dict:\n",
    "    \"\"\"Return dict with old name and new name of files in multiple directories.\n",
    "\n",
    "    {'data/1_extracted/dataset1/Basalt/14.jpg': 'data/2_processed/Basalt/dataset1_01_Basalt_14.jpg'}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_list : list\n",
    "        list of dir paths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {old_name: new_name}\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for dir in dir_list:\n",
    "        paths, _ = find_filepaths(dir)\n",
    "        file_list.extend(paths)\n",
    "\n",
    "    count = 1\n",
    "    file_dict = {}\n",
    "    for file_path in file_list:\n",
    "        dataset = file_path.split(\"/\")[-3]\n",
    "        class_name = file_path.split(\"/\")[-2]\n",
    "        basename = os.path.basename(file_path)\n",
    "        file_name = os.path.splitext(basename)[0]\n",
    "        extension = os.path.splitext(basename)[1]\n",
    "        new_file_name = os.path.join(\n",
    "            \"data\",\n",
    "            \"2_processed\",\n",
    "            class_name,\n",
    "            f\"{dataset}_{class_name}_{str(count).zfill(3)}_{file_name}{extension}\",\n",
    "        )\n",
    "        file_dict[file_path] = new_file_name\n",
    "        count += 1\n",
    "\n",
    "    return file_dict\n",
    "\n",
    "\n",
    "def move_to_processed():\n",
    "    \"\"\"\n",
    "    Combines files with same subclass and moves them to the subclass under data/2_processed.\n",
    "\n",
    "    Uses `get_new_name` to create new names of files and then rename them and copy to data/2_processed.\n",
    "    \"\"\"\n",
    "    dir1 = \"data/1_extracted/dataset1\"\n",
    "    dir2 = \"data/1_extracted/dataset2\"\n",
    "    for d1, d2 in zip(sorted(os.listdir(dir1)), sorted(os.listdir(dir2))):\n",
    "        path_dict = get_new_name([os.path.join(dir1, d1), os.path.join(dir2, d2)])\n",
    "        print(f\"Moving files from dataset1/{d1} and dataset2/{d2} to data/2_processed/{d1} ...\")\n",
    "        for old_path, new_path in path_dict.items():\n",
    "            shutil.copy(old_path, new_path)\n",
    "\n",
    "\n",
    "def clean_images(cfg):\n",
    "    \"\"\"Removes bad, misclassified, duplicate, corrupted and unsupported images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig)\n",
    "        Hydra Configuration\n",
    "    \"\"\"\n",
    "    if cfg.remove_bad: move_bad_files(\n",
    "        \"configs/bad_images selected by gemini.txt\",\n",
    "        \"data/bad_images\",\n",
    "        \"Moving bad images...\",\n",
    "    )\n",
    "    if cfg.remove_misclassified: move_bad_files(\n",
    "        \"configs/misclassified selected by gemini.txt\",\n",
    "        \"data/misclassified_images\",\n",
    "        \"Moving misclassified images...\",\n",
    "    )\n",
    "    if cfg.remove_duplicates: move_bad_files(\n",
    "        \"configs/duplicates selected by gemini.txt\",\n",
    "        \"data/duplicate_images\",\n",
    "        \"Moving duplicate images...\",\n",
    "    )\n",
    "\n",
    "    if cfg.remove_unsupported: remove_unsupported_images(\"data/2_processed\")\n",
    "    if cfg.remove_corrupted: remove_corrupted_images(\"data/2_processed\")\n",
    "\n",
    "\n",
    "def move_bad_files(txt_file, dest, text):\n",
    "    \"\"\"Moves files in txt_file to dest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt_file : file\n",
    "        text file with path of bad images\n",
    "    dest : _type_\n",
    "        target destination\n",
    "    \"\"\"\n",
    "    print(text)\n",
    "    f = open(txt_file, \"r\")\n",
    "    cleaned = list(map(lambda x:x.replace('\\n', ''), f.readlines()))\n",
    "    assert len(list(set([x for i,x in enumerate(cleaned) if cleaned.count(x) > 1]))) == 0\n",
    "\n",
    "    count = 0\n",
    "    for line in cleaned:\n",
    "        if len(line) > 0 and not line.startswith('#') and not line == \"\":\n",
    "            basename = os.path.basename(line)\n",
    "            file_name = os.path.splitext(basename)[0]\n",
    "            try:\n",
    "                shutil.move(line.strip(), os.path.join(dest, file_name))\n",
    "                count +=1\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "    print(f\"\\nMoved {count} images to {dest}.\")\n",
    "\n",
    "\n",
    "def sampling(cfg):\n",
    "    \"\"\"Oversamples/Undersample/No Sampling data into train, val, test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig)\n",
    "        Hydra Configuration\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"\\nSplitting files in Train, Validation and Test and saving to data/3_tfds_dataset/\"\n",
    "    )\n",
    "    scc = min(get_df()[\"class\"].value_counts())\n",
    "    val_split = test_split = (1 - cfg.train_split) / 2\n",
    "    print(\n",
    "        f\"Data Split:- Training {cfg.train_split:.2f}, Validation {val_split:.2f}, Test {test_split:.2f}\"\n",
    "    )\n",
    "    if cfg.sampling == \"oversample\":\n",
    "        print(\"\\nSampling type:- Oversampling...\")\n",
    "        # If your datasets is balanced (each class has the same number of samples), choose ratio otherwise fixed.\n",
    "        print(\"Finding smallest class for oversampling fixed parameter.\")\n",
    "        print(f\"Smallest class count is {scc}\\n\")\n",
    "        splitfolders.fixed(\n",
    "            \"data/2_processed\",\n",
    "            output=\"data/3_tfds_dataset\",\n",
    "            oversample=True,\n",
    "            fixed=(((scc // 2) - 1, (scc // 2) - 1)),\n",
    "            seed=cfg.seed,\n",
    "            move=False,\n",
    "        )\n",
    "    elif cfg.sampling == \"undersample\":\n",
    "        print(f\"Sampling type:- Undersampling to {cfg.sampling} samples.\")\n",
    "        splitfolders.fixed(\n",
    "            \"data/2_processed\",\n",
    "            output=\"data/3_tfds_dataset\",\n",
    "            fixed=(\n",
    "                int(scc * cfg.train_split),\n",
    "                int(scc * val_split),\n",
    "                int(scc * test_split),\n",
    "            ),\n",
    "            oversample=False,\n",
    "            seed=cfg.seed,\n",
    "            move=False,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Sampling type:- No Sampling.\")\n",
    "        splitfolders.ratio(\n",
    "            \"data/2_processed\",\n",
    "            output=\"data/3_tfds_dataset\",\n",
    "            ratio=(cfg.train_split, val_split, test_split),\n",
    "            seed=cfg.seed,\n",
    "            move=False,\n",
    "        )\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def timer_func(func):\n",
    "    \"\"\"Show the execution time of the function object passed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f\"Function {func.__name__!r} executed in {(t2-t1):.4f}s\")\n",
    "        return result\n",
    "\n",
    "    return wrap_func\n",
    "\n",
    "\n",
    "def find_filepaths(root_folder: str):\n",
    "    \"\"\"Recursively finds all files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder : str\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    for dirname, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            filepaths.append(os.path.join(dirname, filename))\n",
    "    return sorted(filepaths), len(filepaths)\n",
    "\n",
    "\n",
    "def remove_unsupported_images(root_folder: str):\n",
    "    \"\"\"Remove unsupported images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder : str\n",
    "        Root Folder.\n",
    "    \"\"\"\n",
    "    print(\"\\nRemoving unsupported images...\")\n",
    "    count = 1\n",
    "    filepaths, _ = find_filepaths(root_folder)\n",
    "    for filepath in filepaths:\n",
    "        if filepath.endswith((\"JFIF\", \"webp\", \"jfif\")):\n",
    "            shutil.move(\n",
    "                filepath,\n",
    "                os.path.join(\"data\", \"corrupted_images\", os.path.basename(filepath)),\n",
    "            )\n",
    "            count += 1\n",
    "    print(f\"Removed {count} unsupported files.\")\n",
    "\n",
    "\n",
    "@timer_func\n",
    "def remove_corrupted_images(\n",
    "    s_dir: str, ext_list: list = [\"jpg\", \"png\", \"jpeg\", \"gif\", \"bmp\", \"JPEG\"]\n",
    "):\n",
    "    \"\"\"Remove corrupted images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s_dir : str\n",
    "        Source directory.\n",
    "    ext_list : list, optional\n",
    "        Extensions list, by default [\"jpg\", \"png\", \"jpeg\", \"gif\", \"bmp\", \"JPEG\"]\n",
    "    \"\"\"\n",
    "    print(\"\\nRemoving corrupted images...\")\n",
    "    classes = os.listdir(s_dir)\n",
    "\n",
    "    def remove_corrupted_from_dir(rock_class):\n",
    "        # remove corrupted images from single directory\n",
    "        bad_images = []\n",
    "        class_path = os.path.join(s_dir, rock_class)\n",
    "        if os.path.isdir(class_path):\n",
    "            file_list = os.listdir(class_path)\n",
    "            for f in file_list:\n",
    "                f_path = os.path.join(class_path, f)\n",
    "                tip = imghdr.what(f_path)\n",
    "                if ext_list.count(tip) == 0:\n",
    "                    bad_images.append(f_path)\n",
    "                if os.path.isfile(f_path):\n",
    "                    try:\n",
    "                        cv2.imread(f_path)\n",
    "                        # shape = img.shape\n",
    "                    except Exception:\n",
    "                        print(\"file \", f_path, \" is not a valid image file\")\n",
    "                        bad_images.append(f_path)\n",
    "                else:\n",
    "                    print(\n",
    "                        \"*** fatal error, you a sub directory \",\n",
    "                        f,\n",
    "                        \" in class directory \",\n",
    "                        rock_class,\n",
    "                    )\n",
    "        else:\n",
    "            print(\n",
    "                \"*** WARNING*** you have files in \",\n",
    "                s_dir,\n",
    "                \" it should only contain sub directories\",\n",
    "            )\n",
    "\n",
    "        for f_path in bad_images:\n",
    "            shutil.move(\n",
    "                f_path,\n",
    "                os.path.join(\"data\", \"corrupted_images\", os.path.basename(f_path)),\n",
    "            )\n",
    "        print(f\"Removed {len(bad_images)} bad images from {rock_class}.\")\n",
    "\n",
    "    def remove_corrupted_images_multicore():\n",
    "        # Multiprocessing\n",
    "        # create all tasks\n",
    "        from multiprocessing import Process\n",
    "\n",
    "        processes = [\n",
    "            Process(target=remove_corrupted_from_dir, args=(i,)) for i in classes\n",
    "        ]\n",
    "        # start all processes\n",
    "        for process in processes:\n",
    "            process.start()\n",
    "        # wait for all processes to complete\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "        # report that all tasks are completed\n",
    "        print(\"Removed all corrupted images.\", flush=True)\n",
    "\n",
    "    for rock_class in classes:\n",
    "        remove_corrupted_from_dir(rock_class)\n",
    "\n",
    "\n",
    "def get_dims(file: str) -> Optional[tuple]:\n",
    "    \"\"\"Return dimenstions for an RBG image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        file path for image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optional[tuple, None]\n",
    "        returns a tuple of heights and width of image or None\n",
    "    \"\"\"\n",
    "    im = cv2.imread(file)\n",
    "    if im is not None:\n",
    "        arr = np.array(im)\n",
    "        h, w = arr.shape[0], arr.shape[1]\n",
    "        return h, w\n",
    "    elif im is None:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_df(root: str = \"data/2_processed\") -> pd.DataFrame:\n",
    "    \"\"\"Return df with classes, image paths and file names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str, optional\n",
    "        directory to scan for image files, by default \"data/2_processed\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        with columns file_name, class and file_path\n",
    "    \"\"\"\n",
    "    classes = os.listdir(root)\n",
    "\n",
    "    class_names = []\n",
    "    images_paths = []\n",
    "    file_names = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        for dirname, _, filenames in os.walk(os.path.join(root, class_name)):\n",
    "            for file_name in filenames:\n",
    "                images_paths.append(os.path.join(root, class_name, file_name))\n",
    "                class_names.append(class_name)\n",
    "                file_names.append(file_name)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        list(zip(file_names, class_names, images_paths)),\n",
    "        columns=[\"file_name\", \"class\", \"file_path\"],\n",
    "    )\n",
    "    df['file_type'] = df[\"file_name\"].apply(lambda x: os.path.splitext(x)[1])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_value_counts(dataset_path: str, column: str='file_type') -> None:\n",
    "    \"\"\"Get value counts of passed column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_path : str\n",
    "        directory with subclasses\n",
    "    column : str\n",
    "        column name\n",
    "    \"\"\"\n",
    "    data = get_df(dataset_path)\n",
    "    print(data[column].value_counts())\n",
    "\n",
    "\n",
    "####################################### tf.data Utilities ###################################\n",
    "\n",
    "\n",
    "def scalar(img: Image) -> Image:\n",
    "    \"\"\"Scale pixel between -1 and +1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : Image\n",
    "        PIL Image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Image\n",
    "        imagew with pixel values scaled between -1 and 1\n",
    "    \"\"\"\n",
    "    return img / 127.5 - 1\n",
    "\n",
    "\n",
    "def get_preprocess(cfg):\n",
    "    \"\"\"Return preprocess function for particular model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig)\n",
    "        Hydra Configuration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    preprocess_dict = {\n",
    "        # \"convnexttiny\":applications.convnext,\n",
    "        \"vgg16\": applications.vgg16,\n",
    "        \"resnet\": applications.resnet,\n",
    "        \"inceptionresnetv2\": applications.inception_resnet_v2,\n",
    "        \"mobilenetv2\": applications.mobilenet_v2,\n",
    "        \"efficientnetv2\": applications.efficientnet_v2,\n",
    "        \"efficientnetv2m\": applications.efficientnet_v2,\n",
    "        \"xception\": applications.xception,\n",
    "    }\n",
    "\n",
    "    return preprocess_dict[cfg.backbone].preprocess_input\n",
    "\n",
    "\n",
    "def prepare(ds, cfg, shuffle=False, augment=False):\n",
    "    \"\"\"Prepare dataset using augment, preprocess, cache, shuffle and prefetch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : _type_\n",
    "        _description_\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "    shuffle : bool, optional\n",
    "        _description_, by default False\n",
    "    augment : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    # keras_cv\n",
    "    def to_dict(image, label):\n",
    "        image = tf.image.resize(image, (cfg.image_size, cfg.image_size))\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        # label = tf.one_hot(label, cfg.num_classes)\n",
    "        return {\"images\": image, \"labels\": label}\n",
    "\n",
    "    def preprocess_for_model(inputs):\n",
    "        images, labels = inputs[\"images\"], inputs[\"labels\"]\n",
    "        images = tf.cast(images, tf.float32)\n",
    "        return images, labels\n",
    "\n",
    "    def cut_mix_and_mix_up(samples):\n",
    "        samples = cut_mix(samples, training=True)\n",
    "        samples = mix_up(samples, training=True)\n",
    "        return samples\n",
    "\n",
    "    def apply_rand_augment(inputs):\n",
    "        inputs[\"images\"] = rand_augment(inputs[\"images\"])\n",
    "        return inputs\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    if cfg.preprocess:\n",
    "        preprocess_input = get_preprocess(cfg)\n",
    "        ds = ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if augment:\n",
    "        # normal augmentation\n",
    "        data_augmentation = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.RandomFlip(\n",
    "                    \"horizontal\",\n",
    "                    input_shape=(cfg.image_size, cfg.image_size, cfg.image_channels),\n",
    "                ),\n",
    "                layers.RandomRotation(0.1),\n",
    "                layers.RandomZoom(0.1),\n",
    "            ]\n",
    "        )\n",
    "        # Use data augmentation only on the training set.\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "    elif augment == \"kerascv\":\n",
    "        # using keras_cv\n",
    "        ds = ds.map(to_dict, num_parallel_calls=AUTOTUNE)\n",
    "        rand_augment = keras_cv.layers.RandAugment(\n",
    "            value_range=(0, 255),\n",
    "            augmentations_per_image=3,\n",
    "            magnitude=0.3,\n",
    "            magnitude_stddev=0.2,\n",
    "            rate=0.5,\n",
    "        )\n",
    "        cut_mix = keras_cv.layers.CutMix()\n",
    "        mix_up = keras_cv.layers.MixUp()\n",
    "\n",
    "        ds = ds.map(apply_rand_augment, num_parallel_calls=AUTOTUNE).map(\n",
    "            cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "        ds = ds.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = ds.cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "\n",
    "    # # Batch all datasets.\n",
    "    # ds = ds.batch(cfg.batch_size)\n",
    "\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "def get_tfds_from_dir(cfg):\n",
    "    \"\"\"Convert directory of images to tfds dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    IMAGE_SIZE = (cfg.image_size, cfg.image_size)\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/3_tfds_dataset/train\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=cfg.batch_size,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=cfg.seed,\n",
    "        # subset='training'\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/3_tfds_dataset/val\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=cfg.batch_size,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=cfg.seed,\n",
    "        # subset='validation'\n",
    "    )\n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/3_tfds_dataset/test\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=cfg.batch_size,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        shuffle=False,\n",
    "        seed=cfg.seed,\n",
    "        # subset='validation'\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def rename_files(source_dir: str = \"data/2_processed/tmp\"):\n",
    "    \"\"\"Rename files in classes and moves to 2_processed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_dir : str, optional\n",
    "        _description_, by default \"data/2_processed/tmp\"\n",
    "    \"\"\"\n",
    "    class_names = os.listdir(source_dir)\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(source_dir, class_name)\n",
    "        dest_path = os.path.join(\"data/2_processed\", class_name)\n",
    "        count = len(os.listdir(dest_path)) + 1\n",
    "        for filename in os.listdir(class_path):\n",
    "            old_path = os.path.join(source_dir, class_name, filename)\n",
    "            _, extension = os.path.splitext(filename)\n",
    "            new_name = f\"{class_name}_{count}{extension}\"\n",
    "            new_path = os.path.join(dest_path, new_name)\n",
    "            shutil.copy(old_path, new_path)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "def move_files(src_dir: str, dest_dir: str = \"data/2_processed/tmp\"):\n",
    "    \"\"\"Move files to tmp directory in 2_processed.\n",
    "\n",
    "    src_dir: directory of rock subclass with files [Basalt, Marble, Coal, ...]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_dir : str\n",
    "        _description_\n",
    "    dest_dir : str, optional\n",
    "        _description_, by default \"data/2_processed/tmp\"\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    src_dir_name = os.path.basename(src_dir)\n",
    "    dest_dir_path = os.path.join(dest_dir, src_dir_name.capitalize())\n",
    "    os.makedirs(dest_dir_path, exist_ok=True)\n",
    "\n",
    "    files = os.listdir(src_dir)\n",
    "    total = len(files)\n",
    "    for index, filename in tqdm(\n",
    "        enumerate(files), desc=f\"Moving {src_dir_name}\", total=total\n",
    "    ):\n",
    "        src_path = os.path.join(src_dir, filename)\n",
    "        dest_path = os.path.join(dest_dir_path, filename)\n",
    "        # print(\"Copying\", src_path, dest_path)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "        # print(f\"Moved {index+1} files from {src_dir} to {dest_dir_path}\")\n",
    "\n",
    "\n",
    "def move_and_rename(class_dir: str):\n",
    "    \"\"\"Move files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, ...), which contains image files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    class_dir : str\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    target_classes = os.listdir(\"data/2_processed/\")\n",
    "    if \"tmp\" in target_classes:\n",
    "        target_classes.remove(\"tmp\")\n",
    "    target_classes_lower = list(map(lambda x: x.lower(), target_classes))\n",
    "\n",
    "    for subclass_dir in os.listdir(class_dir):\n",
    "        if subclass_dir.lower() in target_classes_lower:\n",
    "            subclass_dir_path = os.path.join(class_dir, subclass_dir)\n",
    "            move_files(subclass_dir_path)\n",
    "            rename_files()\n",
    "            shutil.rmtree(\"data/2_processed/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf0268-dce1-4fe6-8902-e934116c1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e91ab-2adc-43eb-8d0f-0706bc43a681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
