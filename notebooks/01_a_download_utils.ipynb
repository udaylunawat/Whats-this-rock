{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5064750b",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: false\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da3016-8c83-40f4-9cb6-3b9e5697046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea64b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "_doc_ = \"\"\"Utility function for downloading data.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303a7af-0fc3-48f0-ac29-040daa5f491d",
   "metadata": {},
   "source": [
    "# Download utilities\n",
    "> Utils for downloading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da0382-f63b-439b-b26d-7ffe67994e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f78344-8738-403d-8113-39e33c7d9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | hide\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import imghdr\n",
    "import requests\n",
    "import shutil\n",
    "import splitfolders\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import logging\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import applications, layers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d089f-4bd0-4683-a110-019383709089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def copy_configs_tocwd():\n",
    "    \"\"\"Copy configs directory from package to current directory.\"\"\"\n",
    "    if os.path.exists(\"configs\"):\n",
    "        shutil.rmtree(\"configs\")\n",
    "    pkgdir = sys.modules[\"rocks_classifier\"].__path__[0]\n",
    "    fullpath = os.path.join(pkgdir, \"configs\")\n",
    "    os.makedirs(\"configs\", exist_ok=True)\n",
    "    for path, directories, files in os.walk(fullpath):\n",
    "        for file_name in files:\n",
    "            if any([file_name.endswith(ext) for ext in [\"py\", \"yaml\", \"txt\"]]):\n",
    "                shutil.copy(\n",
    "                    os.path.join(fullpath, file_name),\n",
    "                    os.path.join(\"configs\", file_name),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def find_filepaths(root_folder: str):\n",
    "    \"\"\"Recursively finds all files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder : str\n",
    "        directory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        sorted filepaths and length of filepaths\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    for dirname, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            filepaths.append(os.path.join(dirname, filename))\n",
    "    return sorted(filepaths), len(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e0efa-4314-4ffd-aa17-548de2cc836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_new_name(dir_list: list) -> dict:\n",
    "    \"\"\"Return dict with old name and new name of files in multiple directories.\n",
    "\n",
    "    {'data/1_extracted/dataset1/Basalt/14.jpg': 'data/2_processed/Basalt/dataset1_01_Basalt_14.jpg'}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_list : list\n",
    "        list of dir paths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {old_name: new_name}\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for dir in dir_list:\n",
    "        paths, _ = find_filepaths(dir)\n",
    "        file_list.extend(paths)\n",
    "\n",
    "    count = 1\n",
    "    file_dict = {}\n",
    "    for file_path in file_list:\n",
    "        dataset = file_path.split(\"/\")[-3]\n",
    "        class_name = file_path.split(\"/\")[-2]\n",
    "        basename = os.path.basename(file_path)\n",
    "        file_name = os.path.splitext(basename)[0]\n",
    "        extension = os.path.splitext(basename)[1]\n",
    "        new_file_name = os.path.join(\n",
    "            \"data\",\n",
    "            \"2_processed\",\n",
    "            class_name,\n",
    "            f\"{dataset}_{class_name}_{str(count).zfill(3)}_{file_name}{extension}\",\n",
    "        )\n",
    "        file_dict[file_path] = new_file_name\n",
    "        count += 1\n",
    "\n",
    "    return file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8709d2-52dd-4b5d-9d82-17102dbcb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def move_to_processed():\n",
    "    \"\"\"\n",
    "    Combine files with same subclass and moves them to the subclass under data/2_processed.\n",
    "\n",
    "    Uses `get_new_name` to create new names of files and then rename them and copy to data/2_processed.\n",
    "    \"\"\"\n",
    "    print(f\"CWD {os.getcwd()}\")\n",
    "    print(f\"listdir:- {os.listdir('.')}\")\n",
    "    dir1 = 'data/1_extracted/dataset1'\n",
    "    dir2 = 'data/1_extracted/dataset2'\n",
    "    for d1, d2 in zip(sorted(os.listdir(dir1)), sorted(os.listdir(dir2))):\n",
    "        path_dict = get_new_name([os.path.join(dir1, d1), os.path.join(dir2, d2)])\n",
    "        print(\n",
    "            f\"Moving files from dataset1/{d1} and dataset2/{d2} to data/2_processed/{d1} ...\"\n",
    "        )\n",
    "        for old_path, new_path in path_dict.items():\n",
    "            print(f\"paths: old: {old_path}, new: {new_path}.\")\n",
    "            shutil.copy(old_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ce3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def move_bad_files(txt_file, dest, text):\n",
    "    \"\"\"Move files in txt_file to dest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt_file : file\n",
    "        text file with path of bad images\n",
    "    dest : _type_\n",
    "        target destination\n",
    "    \"\"\"\n",
    "    print(f\"\\n{text}\")\n",
    "    f = open(txt_file, \"r\")\n",
    "    cleaned = list(map(lambda x: x.replace(\"\\n\", \"\"), f.readlines()))\n",
    "    if len(list(set([x for i, x in enumerate(cleaned) if cleaned.count(x) > 1]))) > 0:\n",
    "        print(f\"Duplicate found in {txt_file}.\")\n",
    "    assert (\n",
    "        len(list(set([x for i, x in enumerate(cleaned) if cleaned.count(x) > 1]))) == 0\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    for line in cleaned:\n",
    "        if len(line) > 0 and not line.startswith(\"#\") and not line == \"\":\n",
    "            basename = os.path.basename(line)\n",
    "            file_name = \".\".join(os.path.splitext(basename))\n",
    "            try:\n",
    "                shutil.move(line.strip(), os.path.join(dest, file_name))\n",
    "                count += 1\n",
    "            except FileNotFoundError:\n",
    "                print(f\"{file_name} not found!\")\n",
    "                continue\n",
    "    print(f\"Moved {count} images to {dest}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def timer_func(func):\n",
    "    \"\"\"Show the execution time of the function object passed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : function\n",
    "        function\n",
    "    \"\"\"\n",
    "\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(f\"Function {func.__name__!r} executed in {(t2-t1):.4f}s\")\n",
    "        return result\n",
    "\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc39126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@timer_func\n",
    "def remove_corrupted_images(\n",
    "    s_dir: str, ext_list: list = [\"jpg\", \"png\", \"jpeg\", \"gif\", \"bmp\", \"JPEG\"]\n",
    "):\n",
    "    \"\"\"Remove corrupted images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s_dir : str\n",
    "        Source directory.\n",
    "    ext_list : list, optional\n",
    "        Extensions list, by default [\"jpg\", \"png\", \"jpeg\", \"gif\", \"bmp\", \"JPEG\"]\n",
    "    \"\"\"\n",
    "    print(\"\\nRemoving corrupted images...\")\n",
    "    classes = os.listdir(s_dir)\n",
    "\n",
    "    def remove_corrupted_from_dir(rock_class):\n",
    "        \"\"\"Remove corrupted images from single directory.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rock_class : str\n",
    "            folder name that contains images\n",
    "        \"\"\"\n",
    "        bad_images = []\n",
    "        class_path = os.path.join(s_dir, rock_class)\n",
    "        if os.path.isdir(class_path):\n",
    "            file_list = os.listdir(class_path)\n",
    "            for f in file_list:\n",
    "                f_path = os.path.join(class_path, f)\n",
    "                tip = imghdr.what(f_path)\n",
    "                if ext_list.count(tip) == 0:\n",
    "                    bad_images.append(f_path)\n",
    "                if os.path.isfile(f_path):\n",
    "                    try:\n",
    "                        cv2.imread(f_path)\n",
    "                        # shape = img.shape\n",
    "                    except Exception:\n",
    "                        print(\"file \", f_path, \" is not a valid image file\")\n",
    "                        bad_images.append(f_path)\n",
    "                else:\n",
    "                    print(\n",
    "                        \"*** fatal error, you a sub directory \",\n",
    "                        f,\n",
    "                        \" in class directory \",\n",
    "                        rock_class,\n",
    "                    )\n",
    "        else:\n",
    "            print(\n",
    "                \"*** WARNING*** you have files in \",\n",
    "                s_dir,\n",
    "                \" it should only contain sub directories\",\n",
    "            )\n",
    "\n",
    "        for f_path in bad_images:\n",
    "            shutil.move(\n",
    "                f_path,\n",
    "                os.path.join(\"data\", \"corrupted_images\", os.path.basename(f_path)),\n",
    "            )\n",
    "        print(f\"Removed {len(bad_images)} corrupted images from {rock_class}.\")\n",
    "\n",
    "    def remove_corrupted_images_multicore():\n",
    "        \"\"\"Multiprocessing enabled, remove corrupted images (Disabled as not supported on M1 macs - my local machine).\"\"\"\n",
    "        # Multiprocessing\n",
    "        # create all tasks\n",
    "        from multiprocessing import Process\n",
    "\n",
    "        processes = [\n",
    "            Process(target=remove_corrupted_from_dir, args=(i,)) for i in classes\n",
    "        ]\n",
    "        # start all processes\n",
    "        for process in processes:\n",
    "            process.start()\n",
    "        # wait for all processes to complete\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "        # report that all tasks are completed\n",
    "        print(\"Removed all corrupted images.\", flush=True)\n",
    "\n",
    "    for rock_class in classes:\n",
    "        remove_corrupted_from_dir(rock_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def remove_unsupported_images(root_folder: str):\n",
    "    \"\"\"Remove unsupported images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder : str\n",
    "        Root Folder.\n",
    "    \"\"\"\n",
    "    print(\"\\nRemoving unsupported images...\")\n",
    "    count = 1\n",
    "    filepaths, _ = find_filepaths(root_folder)\n",
    "    for filepath in filepaths:\n",
    "        if filepath.endswith((\"JFIF\", \"webp\", \"jfif\")):\n",
    "            shutil.move(\n",
    "                filepath,\n",
    "                os.path.join(\"data\", \"corrupted_images\", os.path.basename(filepath)),\n",
    "            )\n",
    "            count += 1\n",
    "    print(f\"Removed {count} unsupported files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551875b4-0e2e-4823-9bd0-6d58652cea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def clean_images(cfg):\n",
    "    \"\"\"Remove bad, misclassified, duplicate, corrupted and unsupported images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig)\n",
    "        Hydra Configuration\n",
    "    \"\"\"\n",
    "    if cfg.remove_bad:\n",
    "        move_bad_files(\n",
    "            \"configs/bad_images selected by gemini.txt\",\n",
    "            \"data/bad_images\",\n",
    "            \"Moving bad images...\",\n",
    "        )\n",
    "    if cfg.remove_misclassified:\n",
    "        move_bad_files(\n",
    "            \"configs/misclassified selected by gemini.txt\",\n",
    "            \"data/misclassified_images\",\n",
    "            \"Moving misclassified images...\",\n",
    "        )\n",
    "    if cfg.remove_duplicates:\n",
    "        move_bad_files(\n",
    "            \"configs/duplicates selected by gemini.txt\",\n",
    "            \"data/duplicate_images\",\n",
    "            \"Moving duplicate images...\",\n",
    "        )\n",
    "\n",
    "    if cfg.remove_unsupported:\n",
    "        remove_unsupported_images(\"data/2_processed\")\n",
    "    if cfg.remove_corrupted:\n",
    "        remove_corrupted_images(\"data/2_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61009f27-c232-4c2b-9cd1-4221f344b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def sampling(cfg):\n",
    "    \"\"\"Oversamples/Undersample/No Sampling data into train, val, test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig)\n",
    "        Hydra Configuration\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"\\nSplitting files in Train, Validation and Test and saving to data/3_tfds_dataset/\"\n",
    "    )\n",
    "    scc = min(get_df()[\"class\"].value_counts())\n",
    "    val_split = test_split = (1 - cfg.train_split) / 2\n",
    "    print(\n",
    "        f\"Data Split:- Training {cfg.train_split:.2f}, Validation {val_split:.2f}, Test {test_split:.2f}\"\n",
    "    )\n",
    "    if cfg.sampling == \"oversample\":\n",
    "        print(\"\\nSampling type:- Oversampling...\")\n",
    "        # If your datasets is balanced (each class has the same number of samples), choose ratio otherwise fixed.\n",
    "        print(\"Finding smallest class for oversampling fixed parameter.\")\n",
    "        print(f\"Smallest class count is {scc}\\n\")\n",
    "        splitfolders.fixed(\n",
    "            \"data/2_processed\",\n",
    "            output=\"data/3_tfds_dataset\",\n",
    "            oversample=True,\n",
    "            fixed=(((scc // 2) - 1, (scc // 2) - 1)),\n",
    "            seed=cfg.seed,\n",
    "            move=False,\n",
    "        )\n",
    "    elif cfg.sampling == \"undersample\":\n",
    "        print(f\"Sampling type:- Undersampling to {cfg.sampling} samples.\")\n",
    "        splitfolders.fixed(\n",
    "            \"data/2_processed\",\n",
    "            output=\"data/3_tfds_dataset\",\n",
    "            fixed=(\n",
    "                int(scc * cfg.train_split),\n",
    "                int(scc * val_split),\n",
    "                int(scc * test_split),\n",
    "            ),\n",
    "            oversample=False,\n",
    "            seed=cfg.seed,\n",
    "            move=False,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Sampling type:- No Sampling.\")\n",
    "        splitfolders.ratio(\n",
    "            \"data/2_processed\",\n",
    "            output=\"data/3_tfds_dataset\",\n",
    "            ratio=(cfg.train_split, val_split, test_split),\n",
    "            seed=cfg.seed,\n",
    "            move=False,\n",
    "        )\n",
    "    # assert len(os.listdir(\"data/3_tfds_dataset\")) == 3\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def get_df(root: str = \"data/2_processed\") -> pd.DataFrame:\n",
    "    \"\"\"Return df with classes, image paths and file names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str, optional\n",
    "        directory to scan for image files, by default \"data/2_processed\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        with columns file_name, class and file_path\n",
    "    \"\"\"\n",
    "    classes = os.listdir(root)\n",
    "\n",
    "    class_names = []\n",
    "    images_paths = []\n",
    "    file_names = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        for dirname, _, filenames in os.walk(os.path.join(root, class_name)):\n",
    "            for file_name in filenames:\n",
    "                images_paths.append(os.path.join(root, class_name, file_name))\n",
    "                class_names.append(class_name)\n",
    "                file_names.append(file_name)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        list(zip(file_names, class_names, images_paths)),\n",
    "        columns=[\"file_name\", \"class\", \"file_path\"],\n",
    "    )\n",
    "    df[\"file_type\"] = df[\"file_name\"].apply(lambda x: os.path.splitext(x)[1])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_value_counts(dataset_path: str, column: str = \"file_type\") -> None:\n",
    "    \"\"\"Get value counts of passed column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_path : str\n",
    "        directory with subclasses\n",
    "    column : str\n",
    "        column name\n",
    "    \"\"\"\n",
    "    data = get_df(dataset_path)\n",
    "    print(data[column].value_counts())\n",
    "\n",
    "\n",
    "####################################### tf.data Utilities ###################################\n",
    "\n",
    "\n",
    "def get_preprocess(cfg):\n",
    "    \"\"\"Return preprocess function for particular model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig)\n",
    "        Hydra Configuration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    preprocess_dict = {\n",
    "        # \"convnexttiny\":applications.convnext,\n",
    "        \"vgg16\": applications.vgg16,\n",
    "        \"resnet\": applications.resnet,\n",
    "        \"inceptionresnetv2\": applications.inception_resnet_v2,\n",
    "        \"mobilenetv2\": applications.mobilenet_v2,\n",
    "        \"efficientnetv2\": applications.efficientnet_v2,\n",
    "        \"efficientnetv2m\": applications.efficientnet_v2,\n",
    "        \"xception\": applications.xception,\n",
    "    }\n",
    "\n",
    "    return preprocess_dict[cfg.backbone].preprocess_input\n",
    "\n",
    "\n",
    "def prepare(ds, cfg, shuffle=False, augment=False):\n",
    "    \"\"\"Prepare dataset using augment, preprocess, cache, shuffle and prefetch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : Dataset\n",
    "        Tensorflow Dataset\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "    shuffle : bool, optional\n",
    "        shuffle parameter, by default False\n",
    "    augment : bool, optional\n",
    "        augment parameter, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataset\n",
    "        Tensorflow Dataset preprocessed, shuffled, augmented and batched\n",
    "    \"\"\"\n",
    "    # keras_cv\n",
    "    def to_dict(image, label):\n",
    "        image = tf.image.resize(image, (cfg.image_size, cfg.image_size))\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        # label = tf.one_hot(label, cfg.num_classes)\n",
    "        return {\"images\": image, \"labels\": label}\n",
    "\n",
    "    def preprocess_for_model(inputs):\n",
    "        images, labels = inputs[\"images\"], inputs[\"labels\"]\n",
    "        images = tf.cast(images, tf.float32)\n",
    "        return images, labels\n",
    "\n",
    "    def cut_mix_and_mix_up(samples):\n",
    "        samples = cut_mix(samples, training=True)\n",
    "        samples = mix_up(samples, training=True)\n",
    "        return samples\n",
    "\n",
    "    def apply_rand_augment(inputs):\n",
    "        inputs[\"images\"] = rand_augment(inputs[\"images\"])\n",
    "        return inputs\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    if cfg.preprocess:\n",
    "        preprocess_input = get_preprocess(cfg)\n",
    "        ds = ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if augment:\n",
    "        # normal augmentation\n",
    "        data_augmentation = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.RandomFlip(\n",
    "                    \"horizontal\",\n",
    "                    input_shape=(cfg.image_size, cfg.image_size, cfg.image_channels),\n",
    "                ),\n",
    "                layers.RandomRotation(0.1),\n",
    "                layers.RandomZoom(0.1),\n",
    "            ]\n",
    "        )\n",
    "        # Use data augmentation only on the training set.\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "    elif augment == \"kerascv\":\n",
    "        # using keras_cv\n",
    "        ds = ds.map(to_dict, num_parallel_calls=AUTOTUNE)\n",
    "        rand_augment = keras_cv.layers.RandAugment(\n",
    "            value_range=(0, 255),\n",
    "            augmentations_per_image=3,\n",
    "            magnitude=0.3,\n",
    "            magnitude_stddev=0.2,\n",
    "            rate=0.5,\n",
    "        )\n",
    "        cut_mix = keras_cv.layers.CutMix()\n",
    "        mix_up = keras_cv.layers.MixUp()\n",
    "\n",
    "        ds = ds.map(apply_rand_augment, num_parallel_calls=AUTOTUNE).map(\n",
    "            cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "        ds = ds.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = ds.cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "\n",
    "    # # Batch all datasets.\n",
    "    # ds = ds.batch(cfg.batch_size)\n",
    "\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "def get_tfds_from_dir(cfg):\n",
    "    \"\"\"Convert directory of images to tfds dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        Tuple containing 3 Tensorflow Datasets\n",
    "    \"\"\"\n",
    "    IMAGE_SIZE = (cfg.image_size, cfg.image_size)\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/3_tfds_dataset/train\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=cfg.batch_size,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=cfg.seed,\n",
    "        # subset='training'\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/3_tfds_dataset/val\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=cfg.batch_size,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=cfg.seed,\n",
    "        # subset='validation'\n",
    "    )\n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/3_tfds_dataset/test\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=cfg.batch_size,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        shuffle=False,\n",
    "        seed=cfg.seed,\n",
    "        # subset='validation'\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def rename_files(source_dir: str = \"data/2_processed/tmp\"):\n",
    "    \"\"\"Rename files in classes and moves to 2_processed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_dir : str, optional\n",
    "        Directory, by default \"data/2_processed/tmp\"\n",
    "    \"\"\"\n",
    "    class_names = os.listdir(source_dir)\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(source_dir, class_name)\n",
    "        dest_path = os.path.join(\"data/2_processed\", class_name)\n",
    "        count = len(os.listdir(dest_path)) + 1\n",
    "        for filename in os.listdir(class_path):\n",
    "            old_path = os.path.join(source_dir, class_name, filename)\n",
    "            _, extension = os.path.splitext(filename)\n",
    "            new_name = f\"{class_name}_{count}{extension}\"\n",
    "            new_path = os.path.join(dest_path, new_name)\n",
    "            shutil.copy(old_path, new_path)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "def move_files(src_dir: str, dest_dir: str = \"data/2_processed/tmp\"):\n",
    "    \"\"\"Move files to tmp directory in 2_processed.\n",
    "\n",
    "    src_dir: directory of rock subclass with files [Basalt, Marble, Coal, ...]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_dir : str\n",
    "        Source Directory path\n",
    "    dest_dir : str, optional\n",
    "        Destination Directory path, by default \"data/2_processed/tmp\"\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    src_dir_name = os.path.basename(src_dir)\n",
    "    dest_dir_path = os.path.join(dest_dir, src_dir_name.capitalize())\n",
    "    os.makedirs(dest_dir_path, exist_ok=True)\n",
    "\n",
    "    files = os.listdir(src_dir)\n",
    "    total = len(files)\n",
    "    for index, filename in tqdm(\n",
    "        enumerate(files), desc=f\"Moving {src_dir_name}\", total=total\n",
    "    ):\n",
    "        src_path = os.path.join(src_dir, filename)\n",
    "        dest_path = os.path.join(dest_dir_path, filename)\n",
    "        # print(\"Copying\", src_path, dest_path)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "        # print(f\"Moved {index+1} files from {src_dir} to {dest_dir_path}\")\n",
    "\n",
    "\n",
    "def move_and_rename(class_dir: str):\n",
    "    \"\"\"Move files from class_dir to tmp, renames them there based on count, and moves back to 2_processed class_dir: A class dir of supporting classes (Marble, Coal, ...), which contains image files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    class_dir : str\n",
    "        Class directory that contains folders of classes containing images\n",
    "    \"\"\"\n",
    "    target_classes = os.listdir(\"data/2_processed/\")\n",
    "    if \"tmp\" in target_classes:\n",
    "        target_classes.remove(\"tmp\")\n",
    "    target_classes_lower = list(map(lambda x: x.lower(), target_classes))\n",
    "\n",
    "    for subclass_dir in os.listdir(class_dir):\n",
    "        if subclass_dir.lower() in target_classes_lower:\n",
    "            subclass_dir_path = os.path.join(class_dir, subclass_dir)\n",
    "            move_files(subclass_dir_path)\n",
    "            rename_files()\n",
    "            shutil.rmtree(\"data/2_processed/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf0268-dce1-4fe6-8902-e934116c1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464795a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
