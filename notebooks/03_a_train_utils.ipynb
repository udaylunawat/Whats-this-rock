{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c05d4b-12b5-4e9a-b6f5-36035e32bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cfc13-5291-48b6-a566-61ff78694ed2",
   "metadata": {},
   "source": [
    "# Training utilities\n",
    "> Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dca604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "_doc_ = \"\"\"Utility functions for training data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783065a1-d60d-4b52-bc6e-c92ca7a9bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | hide\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e61f62-9288-48e9-83cf-e5ead05b30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_optimizer(cfg, lr: str) -> optimizers:\n",
    "    \"\"\"Get optimizer set with an learning rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "    lr : str\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.optimizers\n",
    "        Tensorflow optimizer\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Raise error if cfg.optimizer not implemented.\n",
    "    \"\"\"\n",
    "    optimizer_dict = {\n",
    "        \"adam\": optimizers.Adam,\n",
    "        \"rms\": optimizers.RMSprop,\n",
    "        \"sgd\": optimizers.SGD,\n",
    "        \"adamax\": optimizers.Adamax,\n",
    "    }\n",
    "    try:\n",
    "        opt = optimizer_dict[cfg.optimizer](learning_rate=lr)\n",
    "    except NotImplementedError:\n",
    "        raise NotImplementedError(\"Not implemented.\")\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def get_model_weights(train_ds: tf.data.Dataset):\n",
    "    \"\"\"Return model weights dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_ds : tf.data.Dataset\n",
    "        Tensorflow Dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of class weights.\n",
    "    \"\"\"\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_ds.class_names),\n",
    "        y=train_ds.class_names,\n",
    "    )\n",
    "\n",
    "    train_class_weights = dict(enumerate(class_weights))\n",
    "    return train_class_weights\n",
    "\n",
    "\n",
    "def get_lr_scheduler(cfg, lr) -> optimizers.schedules:\n",
    "    \"\"\"Return A LearningRateSchedule.\n",
    "\n",
    "    Supports [cosine_decay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay), [exponentialdecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) and [cosine_decay_restarts](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecayRestarts).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : cfg (omegaconf.DictConfig):\n",
    "        Hydra Configuration\n",
    "    lr : str\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.optimizers.schedules\n",
    "        A LearningRateSchedule\n",
    "    \"\"\"\n",
    "    scheduler = {\n",
    "        \"cosine_decay\": optimizers.schedules.CosineDecay(\n",
    "            lr, decay_steps=cfg.lr_decay_steps\n",
    "        ),\n",
    "        \"exponentialdecay\": optimizers.schedules.ExponentialDecay(\n",
    "            lr,\n",
    "            decay_steps=cfg.lr_decay_steps,\n",
    "            decay_rate=cfg.reduce_lr.factor,\n",
    "            staircase=True,\n",
    "        ),\n",
    "        \"cosine_decay_restarts\": optimizers.schedules.CosineDecayRestarts(\n",
    "            lr, first_decay_steps=cfg.lr_decay_steps\n",
    "        ),\n",
    "    }\n",
    "    return scheduler[cfg.lr_schedule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651fa1a-7fd4-4afd-8fef-d9e65949b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('rocks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "215557856cb2427135ee1299fe98091812eefded806e23b37d735190fbb4601b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
