{
 "cells": [
  {
   "cell_type": "raw",
   "id": "73667223-8026-446b-9ff4-7b3c2135fd87",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAI\n",
    "Using FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!rm -rf /content/Whats-this-rock/\n",
    "# !git clone https://github.com/udaylunawat/Whats-this-rock.git\n",
    "!git clone -b nbdev https://github.com/udaylunawat/Whats-this-rock.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/Whats-this-rock/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh src/scripts/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data and moving bad, corrupted and duplicate images\n",
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/bad_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del get_data, process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# STEP 1\n",
    "# create a logger object instance\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# STEP 2\n",
    "# specifies the lowest severity for logging\n",
    "logger.setLevel(logging.NOTSET)\n",
    "\n",
    "# STEP 3\n",
    "# set a destination for your logs or a \"handler\"\n",
    "# here, we choose to print on console (a consoler handler)\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# STEP 4\n",
    "# set the logging format for your handler\n",
    "log_format = '%(asctime)s | %(levelname)s: %(message)s'\n",
    "console_handler.setFormatter(logging.Formatter(log_format))\n",
    "\n",
    "# finally, we add the handler to the logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# the second handler is a file handler\n",
    "file_handler = logging.FileHandler('sample.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler_format = '%(asctime)s | %(levelname)s | %(lineno)d: %(message)s'\n",
    "file_handler.setFormatter(logging.Formatter(file_handler_format))\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "import subprocess\n",
    "from src.data.download import get_data\n",
    "from src.data.preprocess import process_data\n",
    "\n",
    "cfg = omegaconf.OmegaConf.load('configs/config.yaml')\n",
    "cfg.dataset_id = [1,2,3,4]\n",
    "\n",
    "subprocess.run([\"sh\", \"src/scripts/clean_dir.sh\"], stdout=subprocess.PIPE).stdout.decode(\"utf-8\")\n",
    "get_data()\n",
    "process_data(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/saifjamsheer/google-dataset-creator/tree/c458ec1f2e676f474527cbf3ee2da3937d260d0c\n",
    "from imutils import paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "from src.data.utils import timer_func, find_filepaths\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Script that deletes duplicate images from a dataset. This\n",
    "can be used in conjunction with 'create.py' to delete\n",
    "any duplicate images before manual pruning is done. \n",
    "\"\"\"\n",
    "def dhash(image, hashSize=8):\n",
    "    # Convert the image to grayscale and resize it\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (hashSize + 1, hashSize))\n",
    "    # Compute the horizontal gradient between adjacent pixels\n",
    "    diff = resized[:, 1:] > resized[:, :-1]\n",
    "    # Converts the image to a hash and returns it\n",
    "    hash = sum([2**i for (i,j) in enumerate(diff.flatten()) if j])\n",
    "    return hash\n",
    "\n",
    "\n",
    "@timer_func\n",
    "def remove_duplicates(dir):\n",
    "    # Path of the dataset of images\n",
    "    output_path = dir\n",
    "    hashes = dict()\n",
    "\n",
    "    # List of paths of the images in the datasets\n",
    "    image_paths = list(paths.list_images(output_path))\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        # Load input image\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        # Compute the hash of the image\n",
    "        try:\n",
    "            h = dhash(image)\n",
    "        except:\n",
    "            print(f'cv2.imread {image_path} Returns None!')\n",
    "        # Store all images in a dictionary of hashes\n",
    "        path = hashes.get(h, [])\n",
    "        path.append(image_path)\n",
    "        hashes[h] = path\n",
    "\n",
    "    # Iterate through the hashes\n",
    "    for h, path_value in hashes.items():\n",
    "        # Checking if a duplicate exists\n",
    "        if len(path_value) > 1:\n",
    "            # Deleting all duplicates\n",
    "            for path in path_value[1:]:\n",
    "                print(\"[INFO] Deleting: {}\".format(path))\n",
    "                # os.remove(path)\n",
    "    return list({k: v for (k, v) in hashes.items() if len(v) > 1}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = remove_duplicates('data/2_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imagededup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagededup.methods import PHash\n",
    "import random\n",
    "\n",
    "def find_duplicates(path):\n",
    "    phasher = PHash()\n",
    "    encodings = phasher.encode_images(image_dir=path)\n",
    "    duplicates = phasher.find_duplicates(encoding_map=encodings)\n",
    "    duplicates = {k: v for (k, v) in duplicates.items() if len(v) > 0}\n",
    "    \n",
    "    from imagededup.utils import plot_duplicates\n",
    "    plot_duplicates(image_dir=path,\n",
    "                    duplicate_map=duplicates,\n",
    "                    filename=random.choice(list(duplicates.values()))[0]);\n",
    "\n",
    "find_duplicates('data/2_processed/Coal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing bad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to store list to file using pickle module\n",
    "import pickle\n",
    "\n",
    "# write list to binary file\n",
    "def write_list(names, filename='delfile'):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(names, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "# Read list to memory\n",
    "def read_list(filename='delfile'):\n",
    "    # for reading also binary mode is important\n",
    "    with open(filename, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "# del_list = read_list()\n",
    "# print('List is', del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images = read_list('updated_delete_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "os.makedirs('data/bad_images', exist_ok=True)\n",
    "\n",
    "for path in bad_images:\n",
    "    if os.path.exists(path):\n",
    "        shutil.move(str(path), 'data/bad_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(os.listdir('data/bad_images')) == len(bad_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(path='data/2_processed/', valid_pct=0.2,\n",
    "                                   item_tfms=Resize(224))\n",
    "dls.valid_ds.items[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training set samples:- {len(dls.train_ds)} images.\\nValidation set samples:- {len(dls.valid_ds)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=[error_rate, F1Score(average='macro')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(5, cbs=ShowGraphCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest error rate:- 0.2460, val_loss:- 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "learn.export(file='model.pkl')\n",
    "\n",
    "learn = load_learner('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict('data/4_tfds_dataset/test/Basalt/Basalt_104.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(learn):\n",
    "    print(f'\\nSample Results:')\n",
    "    learn.show_results()\n",
    "    plt.show()\n",
    "\n",
    "    interp = Interpretation.from_learner(learn)\n",
    "    print(f'\\nTop losses:-')\n",
    "    interp.plot_top_losses(9, figsize=(15,10))\n",
    "    plt.show()\n",
    "\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    losses,idxs = interp.top_losses()\n",
    "    len(dls.valid_ds)==len(losses)==len(idxs)\n",
    "\n",
    "    print(f'\\nConfusion Matrix:-')\n",
    "    interp.plot_confusion_matrix(figsize=(7,7))\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nNormalized Confusion Matrix:-')\n",
    "    interp.plot_confusion_matrix(figsize=(7,7), normalize=True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_delete = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.widgets import *\n",
    "cleaner = ImageClassifierCleaner(learn)\n",
    "cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in cleaner.delete(): files_to_delete.append(cleaner.fns[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfds_to_processed(filepath):\n",
    "    base = '/'.join(str(filepath).split('/')[-2:]).replace('\\')','')\n",
    "    processed = os.path.join('data/2_processed', base)\n",
    "    return Path(set(processed))\n",
    "\n",
    "new_paths = list(map(lambda x:tfds_to_processed(x), files_to_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update delfile\n",
    "paths = read_list()\n",
    "write_list(paths+new_paths, 'delete_file_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_del = paths+new_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    for idx in paths:\n",
    "        idx.unlink()\n",
    "except: pass\n",
    "\n",
    "write_list(paths, filename='delfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/4_tfds_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\n",
    "            \"data/2_processed\",\n",
    "            output=\"data/4_tfds_dataset\",\n",
    "            ratio=(0.70, 0.15, 0.15),\n",
    "            seed=42,\n",
    "            move=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training after data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(path='data/4_tfds_dataset/',\n",
    "                                   train='train',\n",
    "                                   valid='val',\n",
    "                                   item_tfms=Resize(224))\n",
    "dls.valid_ds.items[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of images after deletetion:- \")\n",
    "print(f'Training set samples:- {len(dls.train_ds)} images.\\nValidation set samples:- {len(dls.valid_ds)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(5, cbs=ShowGraphCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using fastai Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(path='data/4_tfds_dataset/', train='train',\n",
    "                                   valid='val', item_tfms=Resize(460),\n",
    "                                   batch_tfms=aug_transforms(size=224))\n",
    "dls.valid_ds.items[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using optimal lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(5, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rocks')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
