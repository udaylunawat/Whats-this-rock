{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHpZwQ6o3r1q"
      },
      "source": [
        "# Notes\n",
        "\n",
        "- improve using these\n",
        "    - [Finetuning Efficientnet](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/image_classification_efficientnet_fine_tuning.ipynb#scrollTo=1KvkMvLI0iCh)\n",
        "    - [Wandb Article](https://wandb.ai/stacey/mendeleev/reports/Tables-Tutorial-Visualize-Data-for-Image-Classification--VmlldzozNjE3NjA)\n",
        "    - [bloodMNIST](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/Keras_pipeline_with_Weights_and_Biases.ipynb#scrollTo=-303Shsata7Z)\n",
        "\n",
        "    - [Simpsons classification](https://colab.research.google.com/drive/181GCGp36_75C2zm7WLxr9U2QjMXXoibt#scrollTo=xftEwKyuaJ5q)\n",
        "\n",
        "## Efficientnet Finetuning Notes\n",
        "\n",
        "- too low- [0.000001, 0.000005, 0.00005]\n",
        "- too high [0.002, 0.005]\n",
        "- 0.001, 0.0005 works\n",
        "    - why?\n",
        "    - isn't it too high for finetuning?\n",
        "\n",
        "## Efficientnet to do\n",
        "- should lr be min max or a list [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
        "- freeze parameter - [true. false]\n",
        "    - so 2 best models, one with freeze true and another with freeze false\n",
        "    - [freeze: true best model](https://wandb.ai/rock-classifiers/Whats-this-rockv2/runs/9fgg9eua?workspace=user-udaylunawat)\n",
        "        \n",
        "    val_accuracy - 63.9\n",
        "\n",
        "    val_f1_score - 63.4\n",
        "            earlystopping_patience 3\n",
        "            freeze true\n",
        "            lr 0.00007\n",
        "            lr_reduce_factor 0.9\n",
        "            lr_reduce_patience 1\n",
        "    - [freeze: false best model](https://wandb.ai/rock-classifiers/Whats-this-rockv2/runs/4qmss0xh/overview?workspace=user-udaylunawat)\n",
        "    \n",
        "    val_accuracy:-0.74\n",
        "    \n",
        "    val_f1_score:-0.73\n",
        "\n",
        "            lr:- 0.0003\n",
        "            reduce_factor: 0.3\n",
        "            lr_reduce_patience:- 3\n",
        "            earlystopping_patience:- 10\n",
        "\n",
        "    - finetune both models in another sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4NbVJC5wc8m"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     import keras_cv\n",
        "# except:\n",
        "#     print(\"Install Tensorflow >= 2.9 and Restart runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUSuNjlQZSi-"
      },
      "source": [
        "# Clone Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3CTmNjsIAIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418005b9-66f7-4e8f-ea24-2546c22fae0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Whats-this-rock'...\n",
            "remote: Enumerating objects: 1330, done.\u001b[K\n",
            "remote: Counting objects: 100% (278/278), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 1330 (delta 180), reused 181 (delta 88), pack-reused 1052\u001b[K\n",
            "Receiving objects: 100% (1330/1330), 3.27 MiB | 23.91 MiB/s, done.\n",
            "Resolving deltas: 100% (848/848), done.\n",
            "/content/Whats-this-rock\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/Whats-this-rock/\n",
        "!git clone https://github.com/udaylunawat/Whats-this-rock.git\n",
        "%cd /content/Whats-this-rock/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if 'WANDB_API_KEY' not in os.environ:\n",
        "  from getpass import getpass\n",
        "  secret = getpass('Enter WandB API Key: ')\n",
        "  os.environ['WANDB_API_KEY'] = secret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr2rIQzrr58M",
        "outputId": "4ad526e3-fbad-4a6f-e753-def25031274e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter WandB API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jMcGudGAUoe"
      },
      "source": [
        "# Deploy Telegram Bot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaYiDe5Me-kc"
      },
      "source": [
        "Uploading secrets with telegram key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "telegram_token = getpass('Enter Telegram API Key: ')\n",
        "os.environ['TOKEN'] = telegram_token"
      ],
      "metadata": {
        "id": "FBgQIx4d5Xng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76219591-0637-40af-98a4-f277dabcd9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Telegram API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR3udTPnSOmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da145120-a521-4a28-d3f8-9f927ee30fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires keras<2.9,>=2.8.0rc0, but you have keras 2.9.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.9.1 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires tensorflow-estimator<2.9,>=2.8, but you have tensorflow-estimator 2.9.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-esB-z6SZP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8244d215-90ec-48b2-e03a-a02e4fe616c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model...\n",
            "Downloaded Trained model: rock-classifiers/Whats-this-rockv2/cvzc7hq0.\n",
            "Model loaded!\n",
            "Bot started!\n",
            "Please visit t.me/test7385_bot to start using me!\n",
            "Telegram Bot Deployed!\n",
            "Exiting immediately!\n"
          ]
        }
      ],
      "source": [
        "!python src/bot.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "api = wandb.Api()\n",
        "\n",
        "run = api.run(\"rock-classifiers/Whats-this-rockv2/cvzc7hq0\")\n",
        "run.file('model-best.h5').download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLCaj2oewwPV",
        "outputId": "1c6ab82b-5f9f-48de-9c60-b5735b8a44d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='./model-best.h5' mode='r' encoding='UTF-8'>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in run.files():\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSU5iWmcw631",
        "outputId": "10908a88-56c2-447b-b255-9c44d1f50737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<File artifact/174765146/wandb_manifest.json (application/json) 899.0B>\n",
            "<File code/src/train.py (text/x-python; charset=utf-8) 5.7KiB>\n",
            "<File config.yaml () 1.4KiB>\n",
            "<File diff.patch (text/plain) 2.4KiB>\n",
            "<File media/images/Classification Report Image:_49_4e09aa405c52575608c9.png (image/png) 202.5KiB>\n",
            "<File media/plotly/Confusion Matrix_48_c48bc37a6a931cf7ec4e.plotly.json (application/json) 16.7KiB>\n",
            "<File model-best.h5 (application/keras) 86.5MiB>\n",
            "<File output.log () 4.4KiB>\n",
            "<File requirements.txt (text/plain; charset=utf-8) 7.1KiB>\n",
            "<File wandb-metadata.json (application/json) 1.3KiB>\n",
            "<File wandb-summary.json (application/json) 1.0KiB>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtEPDzGW04cQ",
        "outputId": "9f7a1dce-f136-49ec-dc55-8d4e768901c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available objects for config:\n",
            "     AliasManager\n",
            "     ColabHistoryManager\n",
            "     DisplayFormatter\n",
            "     IPCompleter\n",
            "     IPKernelApp\n",
            "     InlineBackend\n",
            "     LoggingMagics\n",
            "     MagicsManager\n",
            "     OSMagics\n",
            "     PrefilterManager\n",
            "     ScriptMagics\n",
            "     Shell\n",
            "     StoreMagics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluating Model"
      ],
      "metadata": {
        "id": "RSk5wNo6SAS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "1A308ttOR-Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile setup.sh\n",
        "# #!/bin/bash\n",
        "\n",
        "# # setting up kaggle\n",
        "# wget https://www.dropbox.com/s/ltp4ly8ilvxlgas/kaggle.json\n",
        "# mkdir -p ~/.kaggle\n",
        "# mv kaggle.json ~/.kaggle\n",
        "# chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# pip install -r requirements-dev.txt\n",
        "\n",
        "# # setting up data dir\n",
        "# rm -rf data/1_extracted data/2_processed data/3_consume data/4_tfds_dataset data/corrupted_images\n",
        "# mkdir -p data/0_raw data/1_extracted data/2_processed data/3_consume data/4_tfds_dataset data/corrupted_images checkpoints\n",
        "# mkdir -p data/2_processed/Basalt/\n",
        "# mkdir -p data/2_processed/Granite/\n",
        "# mkdir -p data/2_processed/Marble/\n",
        "# mkdir -p data/2_processed/Quartzite/\n",
        "# mkdir -p data/2_processed/Limestone/\n",
        "# mkdir -p data/2_processed/Sandstone/\n",
        "\n",
        "# # dataset 1 processing\n",
        "# kaggle datasets download salmaneunus/rock-classification --path data/0_raw/\n",
        "# unzip -q data/0_raw/rock-classification.zip -d data/1_extracted/\n",
        "# mv data/1_extracted/Dataset/Igneous/* data/2_processed\n",
        "# mv data/1_extracted/Dataset/Metamorphic/* data/2_processed\n",
        "# mv data/1_extracted/Dataset/Sedimentary/* data/2_processed\n",
        "\n",
        "# # # dataset 2 processing\n",
        "# # kaggle datasets download mahmoudalforawi/igneous-metamorphic-sedimentary-rocks-and-minerals --path data/0_raw/\n",
        "# # unzip -q data/0_raw/igneous-metamorphic-sedimentary-rocks-and-minerals.zip -d data/1_extracted/\n",
        "# # # https://serverfault.com/a/267266/979238\n",
        "# # mv --backup=t data/1_extracted/Rock_Dataset/igneous\\ rocks/Basalt/* data/2_processed/Basalt/\n",
        "# # mv --backup=t data/1_extracted/Rock_Dataset/igneous\\ rocks/granite/* data/2_processed/Granite/\n",
        "# # mv --backup=t data/1_extracted/Rock_Dataset/metamorphic\\ rocks/marble/* data/2_processed/Marble/\n",
        "# # mv --backup=t data/1_extracted/Rock_Dataset/metamorphic\\ rocks/quartzite/* data/2_processed/Quartzite/\n",
        "# # mv --backup=t data/1_extracted/Rock_Dataset/sedimentary\\ rocks/coal/* data/2_processed/Coal/\n",
        "# # mv --backup=t data/1_extracted/Rock_Dataset/sedimentary\\ rocks/Limestone/* data/2_processed/Limestone/\n",
        "# # mv --backup=t data/1_extracted/Rock_Dataset/sedimentary\\ rocks/Sandstone/* data/2_processed/Sandstone/\n",
        "# # rm -rf data/1_extracted/Rock_Dataset/minerals\n",
        "\n",
        "# # # dataset 3 processing\n",
        "# # wget --quiet -O data/0_raw/dataset3.zip https://github.com/SmartPracticeschool/llSPS-INT-3797-Rock-identification-using-deep-convolution-neural-network/raw/master/dataset.zip\n",
        "# # unzip -q data/0_raw/dataset3.zip -d data/1_extracted/\n",
        "# # mv --backup=t data/1_extracted/dataset/*/Basalt/* data/2_processed/Basalt/\n",
        "# # mv --backup=t data/1_extracted/dataset/*/Granite/* data/2_processed/Granite/\n",
        "# # mv --backup=t data/1_extracted/dataset/*/Marble/* data/2_processed/Marble/\n",
        "# # mv --backup=t data/1_extracted/dataset/*/Quartzite/* data/2_processed/Quartzite/\n",
        "# # mv --backup=t data/1_extracted/dataset/*/Limestone/* data/2_processed/Limestone/\n",
        "# # mv --backup=t data/1_extracted/dataset/*/Sandstone/* data/2_processed/Sandstone/\n",
        "\n",
        "# # # dataset 4 processing\n",
        "# # kaggle datasets download neelgajare/rocks-dataset --path data/0_raw/\n",
        "# # unzip -q data/0_raw/rocks-dataset.zip -d data/1_extracted/\n",
        "# # cp -r --backup=t data/1_extracted/Rocks/Basalt/* data/2_processed/Basalt/\n",
        "# # cp -r --backup=t data/1_extracted/Rocks/Granite/* data/2_processed/Granite/\n",
        "# # cp -r --backup=t data/1_extracted/Rocks/Marble/* data/2_processed/Marble/\n",
        "# # cp -r --backup=t data/1_extracted/Rocks/Quartzite/* data/2_processed/Quartzite/\n",
        "# # cp -r --backup=t data/1_extracted/Rocks/Coal/* data/2_processed/Coal/\n",
        "# # cp -r --backup=t data/1_extracted/Rocks/Limestone/* data/2_processed/Limestone/\n",
        "# # cp -r --backup=t data/1_extracted/Rocks/Sandstone/* data/2_processed/Sandstone/\n"
      ],
      "metadata": {
        "id": "Rs1Qp8Yu6_U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YhGXETTXICk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14bd15c8-2376-4b77-a0d7-b939d6e9a607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 9s (49.3 MB/s)\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155654 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 2)) (4.6.0.66)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 3)) (1.5.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 4)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 8)) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 9)) (0.17.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 10)) (0.13.2)\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (6.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (2022.6.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (4.64.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements-dev.txt (line 4)) (2022.2.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (1.1.2)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (14.0.6)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (1.12)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (0.26.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (1.47.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements-dev.txt (line 8)) (57.4.0)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements-dev.txt (line 8)) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements-dev.txt (line 8)) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->-r requirements-dev.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->-r requirements-dev.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->-r requirements-dev.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->-r requirements-dev.txt (line 9)) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->-r requirements-dev.txt (line 9)) (2.7.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (2.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (1.9.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (1.0.9)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (3.1.27)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 10)) (7.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb->-r requirements-dev.txt (line 10)) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements-dev.txt (line 10)) (5.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons->-r requirements-dev.txt (line 9)) (3.0.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->-r requirements-dev.txt (line 3)) (1.3)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, split-folders\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-cpu 2.9.1 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.8.0 which is incompatible.\n",
            "tensorflow-cpu 2.9.1 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.8.0 which is incompatible.\n",
            "tensorflow-cpu 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.8.0 split-folders-0.5.1 tensorboard-2.8.0 tensorflow-estimator-2.8.0\n",
            "--2022-08-30 21:46:17--  https://www.dropbox.com/s/ltp4ly8ilvxlgas/kaggle.json\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/ltp4ly8ilvxlgas/kaggle.json [following]\n",
            "--2022-08-30 21:46:17--  https://www.dropbox.com/s/raw/ltp4ly8ilvxlgas/kaggle.json\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8a57b7d6164d94e00bd7e46d33.dl.dropboxusercontent.com/cd/0/inline/BsAjI_QwaccQaN4YpbddBfJZ_ePZEzTVEYMW06W5ciISFeCRW9AC9k7AIHKwcBYXKYxpY_SAtxfi2pBaDxZPFN-J1tVuilcRFfN0JVslT-vMIm_DJnNJ7E5-fHevO4sFzes1EXSMf5tiw_FW7ecI0E8mHln01ll6u8pWcrM8uB-8Zw/file# [following]\n",
            "--2022-08-30 21:46:17--  https://uc8a57b7d6164d94e00bd7e46d33.dl.dropboxusercontent.com/cd/0/inline/BsAjI_QwaccQaN4YpbddBfJZ_ePZEzTVEYMW06W5ciISFeCRW9AC9k7AIHKwcBYXKYxpY_SAtxfi2pBaDxZPFN-J1tVuilcRFfN0JVslT-vMIm_DJnNJ7E5-fHevO4sFzes1EXSMf5tiw_FW7ecI0E8mHln01ll6u8pWcrM8uB-8Zw/file\n",
            "Resolving uc8a57b7d6164d94e00bd7e46d33.dl.dropboxusercontent.com (uc8a57b7d6164d94e00bd7e46d33.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc8a57b7d6164d94e00bd7e46d33.dl.dropboxusercontent.com (uc8a57b7d6164d94e00bd7e46d33.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67 [text/plain]\n",
            "Saving to: ‘kaggle.json’\n",
            "\n",
            "kaggle.json         100%[===================>]      67  --.-KB/s    in 0s      \n",
            "\n",
            "2022-08-30 21:46:18 (9.52 MB/s) - ‘kaggle.json’ saved [67/67]\n",
            "\n",
            "Downloading rock-classification.zip to data/0_raw\n",
            " 95% 152M/159M [00:01<00:00, 140MB/s]\n",
            "100% 159M/159M [00:01<00:00, 128MB/s]\n",
            "Downloading igneous-metamorphic-sedimentary-rocks-and-minerals.zip to data/0_raw\n",
            " 98% 343M/349M [00:02<00:00, 131MB/s]\n",
            "100% 349M/349M [00:02<00:00, 126MB/s]\n",
            "Downloading rocks-dataset.zip to data/0_raw\n",
            "100% 593M/595M [00:04<00:00, 124MB/s]\n",
            "100% 595M/595M [00:04<00:00, 149MB/s]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3444 entries, 0 to 3443\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   file_name  3444 non-null   object\n",
            " 1   class      3444 non-null   object\n",
            " 2   file_path  3444 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 80.8+ KB\n",
            "None\n",
            "Quartzite    656\n",
            "Limestone    584\n",
            "Marble       575\n",
            "Coal         514\n",
            "Sandstone    494\n",
            "Granite      339\n",
            "Basalt       282\n",
            "Name: class, dtype: int64\n",
            "Splitting files in Train, Validation and Test and saving to data/4_tfds_dataset/\n",
            "Copying files: 3444 files [00:01, 3310.58 files/s]\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!sh setup.sh\n",
        "!python src/preprocess.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXu-3mstXJ0k"
      },
      "source": [
        "# Load Data and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LisTfP8OmsD9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "default = dict(\n",
        "    notes='''\n",
        "    \n",
        "    Datasets used:[1, 2, 3, 4], sampling: None, class-weights:True, \n",
        "    preprocessing:rescale=1./255, horizontal_flip=True, vertical_flip=True\n",
        "    \n",
        "    ''',\n",
        "    model_name=\"inceptionresnetv2\",\n",
        "    augment=True,\n",
        "    freeze=False,\n",
        "    finetune=True,\n",
        "    pretrained_model_link= \"rock-classifiers/Whats-this-rockv2/cvzc7hq0\",\n",
        "    optimizer=\"adam\",\n",
        "    lr=.0005,\n",
        "    batch_size=32,\n",
        "    max_epochs=30,\n",
        "    image_size=224,\n",
        "    loss_fn='categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        "    earlystopping_patience=10,\n",
        "    earlystopping_min_delta = 0.02,\n",
        "    lr_reduce_patience=3,\n",
        "    lr_reduce_factor=.5,\n",
        "    threshold=.7,\n",
        ")\n",
        "\n",
        "# save dictionary to config.json file\n",
        "with open('config.json', 'w') as f:\n",
        "    json.dump(default, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default['pretrained_model_link'].split('/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asTLphTt1wVy",
        "outputId": "b5409bd8-e57b-4350-f9e9-03b52d61b75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rock-classifiers', 'Whats-this-rockv2', 'cvzc7hq0']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "H5UlUGLqCDgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['WANDB_MODE'] = 'offline' # offline"
      ],
      "metadata": {
        "id": "4fjPGHxCvjQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKG91Awf_DVH"
      },
      "outputs": [],
      "source": [
        "!python src/train.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python src/evaluate.py"
      ],
      "metadata": {
        "id": "KIbRmgJ62ADB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa9mUak6P7bm"
      },
      "source": [
        "# Weights and Biases Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldXE23A3xKGP",
        "outputId": "8a279da9-cd54-4959-bc20-332208736a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating 004570f..8240747\n",
            "Fast-forward\n",
            " src/src.data.utils.py | 12 \u001b[32m+++++++++\u001b[m\u001b[31m---\u001b[m\n",
            " src/models.py         |  1 \u001b[32m+\u001b[m\n",
            " sweep.yaml            | 27 \u001b[32m+++++++++++\u001b[m\u001b[31m----------------\u001b[m\n",
            " 3 files changed, 21 insertions(+), 19 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  lr:- 0.0003\n",
        "  reduce_factor: 0.3\n",
        "  lr_reduce_patience:- 3\n",
        "  earlystopping_patience:- 10"
      ],
      "metadata": {
        "id": "V66F01wXi_j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rozTKJ0T9EMz",
        "outputId": "9ef41385-38e0-49f9-d007-5f93f8bc3239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sweep.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile sweep.yaml\n",
        "method: bayes\n",
        "metric:\n",
        "  goal: maximize\n",
        "  name: val_f1_score\n",
        "parameters:\n",
        "  notes:\n",
        "    value: \"Datasets:[1,2,3,4], train shuffle :True, Augment: 9 augmentations, \n",
        "            sampling: None, removing_corrupt: False\"\n",
        "  augment:\n",
        "    values: [true]\n",
        "  finetune:\n",
        "    values: [false]\n",
        "  freeze:\n",
        "    values: [false]\n",
        "  batch_size:\n",
        "    values: [32]\n",
        "  metrics:\n",
        "    value: [\"accuracy\"]\n",
        "  max_epochs:\n",
        "    value: 100\n",
        "  loss_fn:\n",
        "    value: \"categorical_crossentropy\"\n",
        "  lr:\n",
        "    distribution: uniform\n",
        "    max: 0.001\n",
        "    min: 0.00005\n",
        "  model_name:\n",
        "    values: [\"efficientnet\"]\n",
        "  optimizer:\n",
        "    values: [\"adam\"]\n",
        "  image_size:\n",
        "    value: 224\n",
        "  earlystopping_patience:\n",
        "    value: 10\n",
        "  earlystopping_min_delta:\n",
        "    value: 0.02\n",
        "  lr_reduce_patience:\n",
        "    values: [1, 2, 3, 4]\n",
        "  lr_reduce_factor:\n",
        "    values: [.9, .7, .3, .1],\n",
        "threshold:\n",
        "    value: 0.7\n",
        "early_terminate:\n",
        "  type: hyperband\n",
        "  s: 2\n",
        "  eta: 3\n",
        "  max_iter: 100\n",
        "program: src/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADsiqB08Mthg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc029f0e-bed6-48a1-c9a5-24f7f0b9d178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: sweep.yaml\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Created sweep with ID: \u001b[33m81jd5gu7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/rock-classifiers/Whats-this-rockv2/sweeps/81jd5gu7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent rock-classifiers/Whats-this-rockv2/81jd5gu7\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!wandb sweep \\\n",
        "    --project Whats-this-rockv2 \\\n",
        "    --entity rock-classifiers \\\n",
        "    sweep.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcHI98wnM5H4"
      },
      "outputs": [],
      "source": [
        "!wandb agent rock-classifiers/Whats-this-rockv2/f9rl3654"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BrmYJMgw8oxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Whats-this-rock.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}